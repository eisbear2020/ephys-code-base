########################################################################################################################
#
#   MULITPLE_PHASES
#
#   Description: contains classes that are used to perform analysis on several phases (e.g. sleep and exploration)
#
#   Author: Lars Bollmann
#
#   Created: 26/06/2020
#
#   Structure:
#
########################################################################################################################

from collections import OrderedDict
from functools import partial
from function_files.support_functions import upper_tri_without_diag, find_hse, compute_sparsity, moving_average, \
    cross_correlate, multi_dim_scaling, perform_TSNE, perform_isomap, perform_PCA, angle_between_col_vectors, \
    evaluate_clustering_fit, down_sample_array_sum, down_sample_array_mean, transition_matrix, bayes_likelihood, \
    nr_goals_coded_per_cell, goal_coding_per_cell, nr_goals_coded_subset_of_cells, distance_peak_firing_to_closest_goal, \
    decode_using_phmm_modes, read_integers, read_arrays, cross_correlate_matrices
from function_files.single_phase import Sleep, TwoPopSleep, Exploration, TwoPopExploration, Cheeseboard
from function_files.ml_methods import MlMethodsOnePopulation, MlMethodsTwoPopulations, PoissonHMM
from function_files.plotting_functions import plot_3D_scatter, plot_2D_scatter
import matplotlib.pyplot as plt
import numpy as np
import pickle
import multiprocessing as mp
from matplotlib.ticker import MaxNLocator
import itertools
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import StratifiedShuffleSplit
import copy
import seaborn as sns
import matplotlib.patches as patches
from matplotlib import collections as matcoll
from scipy.stats import ks_2samp
from scipy.spatial.distance import pdist, squareform, cdist
import matplotlib.cm as cm
from statsmodels.tsa import stattools
from scipy.spatial import distance
from scipy import stats
from scipy import optimize
from scipy import fft
from sklearn.manifold import MDS, TSNE
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from scipy.stats import pearsonr, entropy, spearmanr, sem, mannwhitneyu, wilcoxon, ks_2samp, multivariate_normal, \
      zscore, f_oneway, ttest_ind, binom_test
from sklearn.metrics import pairwise_distances
from sklearn import svm
from scipy.stats import mode as mode_of_dist
from scipy.stats import kstest
from scipy.signal import correlate
from skimage.measure import block_reduce
import pandas as pd
import os
import scipy.cluster.hierarchy as sch
from scipy.spatial.distance import squareform
from scipy.signal import correlate2d
from scipy.stats import gaussian_kde
from sklearn import preprocessing
from sklearn.metrics import pairwise_distances
from numpy.linalg import norm
from scipy.ndimage.filters import uniform_filter1d
import matplotlib

matplotlib.rcParams['pdf.fonttype'] = 42
matplotlib.rcParams['ps.fonttype'] = 42

"""#####################################################################################################################
#   LONG SLEEP
#####################################################################################################################"""


class LongSleep:
    """Class for long sleep"""

    def __init__(self, sleep_data_obj, params, session_params):
        self.params = params
        self.session_params = session_params
        self.cell_type = sleep_data_obj.get_cell_type()
        self.long_sleep = []
        self.session_name = session_params.session_name

        # initialize each phase
        for phase_id, phase in enumerate(sleep_data_obj.data_description):
            # check if extended data (incl. LFP)  needs to be loaded
            if params.data_to_use == "std":
                data_dic = sleep_data_obj.get_standard_data()[phase_id]
            elif params.data_to_use == "ext":
                data_dic = sleep_data_obj.get_extended_data()[phase_id]

            self.long_sleep.append(Sleep(data_dic=[data_dic], cell_type=sleep_data_obj.get_cell_type(),
                                         params=self.params, session_params=self.session_params,
                                         experiment_phase=phase))

    def check_sleep(self):
        speed = np.zeros(0)
        for sleep in self.long_sleep:
            sleep.get_speed()
            np.hstack(speed, sleep.get_speed())
        plt.plot(speed)
        plt.ylabel("SPEED")
        plt.xlabel("TIME")
        plt.title("SPEED DURING LONG SLEEP")
        plt.show()

    def get_sleep_raster(self):
        raster = []
        for sleep in self.long_sleep:
            r = sleep.get_raster()
            raster.append(r)
        raster = np.hstack(raster)
        return raster

    def get_sleep_phase_raster(self, sleep_phase):
        if self.session_name == "mjc169R1R_0114" and sleep_phase == "nrem":
            sleep_phase = "all_swr"

        raster_list = []
        for sleep in self.long_sleep:
            r, _, _ = sleep.get_event_time_bin_rasters(sleep_phase=sleep_phase)
            raster_list.append(r)
        # TODO: check why simple np.hstack() does not work!
        raster = []
        for raster_entry in raster_list:
            raster.append(np.hstack(raster_entry))

        raster = np.hstack(raster)
        return raster

    def get_constant_spike_bin_length(self, plotting=False, return_median=False):

        rem_bin_length = []
        for sleep in self.long_sleep:
            _, event_spike_window_lengths = sleep.get_event_spike_rasters(part_to_analyze="rem")
            rem_bin_length.append(np.hstack(event_spike_window_lengths))
        rem_bin_length = np.hstack(rem_bin_length)


        nrem_bin_length = []
        # mjc169R1R doesnt have NREM labels
        if self.session_name == "mjc169R1R_0114":
            for sleep in self.long_sleep:
                _, event_spike_window_lengths = sleep.get_event_spike_rasters(part_to_analyze="all_swr")
                if len(event_spike_window_lengths) > 0:
                    nrem_bin_length.append(np.hstack(event_spike_window_lengths))
        else:
            for sleep in self.long_sleep:
                _, event_spike_window_lengths = sleep.get_event_spike_rasters(part_to_analyze="nrem")
                if len(event_spike_window_lengths) > 0:
                    nrem_bin_length.append(np.hstack(event_spike_window_lengths))

        nrem_bin_length = np.hstack(nrem_bin_length)

        if plotting:
            p_rem = 1. * np.arange(rem_bin_length.shape[0]) / (rem_bin_length.shape[0] - 1)
            p_nrem = 1. * np.arange(nrem_bin_length.shape[0]) / (nrem_bin_length.shape[0] - 1)
            plt.plot(np.sort(nrem_bin_length), p_nrem, color="blue", label="NREM")
            plt.plot(np.sort(rem_bin_length), p_rem, color="red", label="REM")
            plt.legend()
            plt.xscale("log")
            plt.ylabel("cdf")
            plt.xlabel("12-spike-bin duration (s)")
            plt.show()
        else:
            if return_median:
                return np.median(nrem_bin_length), np.median(rem_bin_length)
            else:
                return nrem_bin_length, rem_bin_length

    def classify_cells_firing_rate_polyfit(self):
        raster = []
        for l_s in self.long_sleep:
            raster.append(l_s.get_raster())

        raster = np.hstack(raster)

        m = []

        for cell_id, cell_firing in enumerate(raster):
            smooth_firing = moving_average(a=cell_firing, n=1000)
            smooth_firing /= np.max(smooth_firing)
            # compute regression
            coef = np.polyfit(np.arange(smooth_firing.shape[0])*self.params.time_bin_size, smooth_firing, 1)
            m.append(coef[0])
            # if abs(coef[0]) < 5e-6:
            #     stable_cells_k_means.append(cell_id)
            # # if abs(coef[0]) < 2e-6:
            #     poly1d_fn = np.poly1d(coef)
            #     plt.plot(smooth_firing)
            #     plt.plot(np.arange(smooth_firing.shape[0]), poly1d_fn(np.arange(smooth_firing.shape[0])), '--w')
            #     plt.title(coef)
            #     plt.show()

        m = np.array(m)
        # find stable cells: < median --> not a good measure, but for the time being
        stable_cells = np.squeeze(np.argwhere(np.abs(m) < np.median(np.abs(m))))

        plt.hist(np.abs(np.array(m)), bins=80)
        plt.xlabel("M")
        plt.ylabel("DENSITY")
        print(np.median(np.array(np.abs(m))))
        plt.show()

        np.save(self.params.pre_proc_dir+"stable_cells_k_means/"+self.params.session_name+"_regression", np.array(stable_cells))

    def classifiy_cells_firing_rate_distribution(self, alpha=0.01, test="ks"):

        raster = []
        for l_s in self.long_sleep:
            raster.append(l_s.get_raster())

        raster = np.hstack(raster)

        raster_1 = raster[:, :int(raster.shape[1]/10)]
        raster_2 = raster[:, -int(raster.shape[1]/10):]

        stable_cell_ids = []
        increase_cell_ids = []
        decrease_cell_ids = []
        for cell_id, (cell_fir_bef, cell_fir_aft) in enumerate(zip(raster_1, raster_2)):
            if test == "ks":
                if ks_2samp(data1=cell_fir_aft, data2=cell_fir_bef, alternative="less")[1] < alpha:
                    increase_cell_ids.append(cell_id)
                elif ks_2samp(data1=cell_fir_aft, data2=cell_fir_bef, alternative="greater")[1] < alpha:
                    decrease_cell_ids.append(cell_id)
                else:
                    stable_cell_ids.append(cell_id)
            elif test == "mwu":
                if mannwhitneyu(x=cell_fir_aft, y=cell_fir_bef, alternative="less")[1] < alpha:
                    decrease_cell_ids.append(cell_id)
                elif mannwhitneyu(x=cell_fir_aft, y=cell_fir_bef, alternative="greater")[1] < alpha:
                    increase_cell_ids.append(cell_id)
                else:
                    stable_cell_ids.append(cell_id)

        print("#stable: "+str(len(stable_cell_ids))+" ,#inc: "+
              str(len(increase_cell_ids))+" ,#dec:"+str(len(decrease_cell_ids)))

        cell_class_dic = {
            "stable_cell_ids": np.array(stable_cell_ids),
            "decrease_cell_ids": np.array(decrease_cell_ids),
            "increase_cell_ids": np.array(increase_cell_ids)
        }

        with open(self.params.pre_proc_dir+"cell_classification/"+self.params.session_name+"_"+test+
                  "_sleep.pickle", "wb") as f:
            pickle.dump(cell_class_dic, f, pickle.HIGHEST_PROTOCOL)

    def classify_cells_firing_rate_mean(self):

        raster = []
        for l_s in self.long_sleep:
            raster.append(l_s.get_raster())

        raster = np.hstack(raster)

        raster_1 = raster[:, :int(raster.shape[1]/10)]
        raster_2 = raster[:, -int(raster.shape[1]/10):]

        fir_diff = []
        for cell_id, (cell_fir_bef, cell_fir_aft) in enumerate(zip(raster_1, raster_2)):
            fir_diff.append(np.mean(cell_fir_aft)-np.mean(cell_fir_bef))

        fir_diff = np.abs(np.array(fir_diff))
        stable_cells = np.squeeze(np.argwhere(fir_diff < 0.025))
        plt.hist(fir_diff, bins=80)
        plt.show()

        np.save(self.params.pre_proc_dir+"stable_cells_k_means/"+self.params.session_name+"_sleep_fir_diff", np.array(stable_cells))

    def classify_cells_firing_rate_k_means(self, nr_clusters=3):

        # get rasters
        raster = []
        first = 0
        for l_s in self.long_sleep:
            duration = l_s.get_duration_sec()
            r, t = l_s.get_spike_binned_raster(return_estimated_times=True)
            raster.append(r)
            first += duration
        raster = np.hstack(raster)

        # down sample raster
        raster_ds = down_sample_array_mean(x=raster, chunk_size=5000)
        raster_ds = raster_ds / np.max(raster_ds, axis=1, keepdims=True)

        # smooth down sampled rasters
        raster_ds_smoothed = []

        for cell_arr in raster_ds:
            s = moving_average(a=cell_arr, n=50)
            s = s/np.max(s)
            raster_ds_smoothed.append(s)

        raster_ds_smoothed = np.array(raster_ds_smoothed)

        # k-means clustering
        kmeans = KMeans(n_clusters=nr_clusters).fit(X=raster_ds_smoothed)
        k_labels = kmeans.labels_

        stable = []
        increase = []
        decrease = []
        # find clusters with constant/decreasing/increasing firing rates
        # compare firing during first 20% with firing during last 20%
        for cl_id in np.unique(k_labels):
            cl_data = raster_ds_smoothed[k_labels == cl_id, :]
            diff = np.mean(cl_data[:, -int(0.2 * cl_data.shape[1]):].flatten()) - np.mean(
                cl_data[:, :int(0.2 * cl_data.shape[1])].flatten())
            if diff < -0.09:
                decrease.append(cl_id)
            elif diff > 0.09:
                increase.append(cl_id)
            else:
                stable.append(cl_id)

        decrease = np.array(decrease)
        increase = np.array(increase)
        stable = np.array(stable)

        print("STABLE CLUSTER IDS: "+str(stable))
        print("INCREASING CLUSTER IDS: " + str(increase))
        print("DECREASING CLUSTER IDS: " + str(decrease))

        stable_cell_ids = []
        decrease_cell_ids = []
        increase_cell_ids = []
        for stable_cluster_id in stable:
            stable_cell_ids.append(np.where(k_labels==stable_cluster_id)[0])
        for dec_cluster_id in decrease:
            decrease_cell_ids.append(np.where(k_labels==dec_cluster_id)[0])
        for inc_cluster_id in increase:
            increase_cell_ids.append(np.where(k_labels==inc_cluster_id)[0])

        stable_cell_ids = np.hstack(stable_cell_ids)
        decrease_cell_ids = np.hstack(decrease_cell_ids)
        increase_cell_ids = np.hstack(increase_cell_ids)

        # create dictionary with labels

        cell_class_dic = {
            "stable_cell_ids": stable_cell_ids,
            "decrease_cell_ids": decrease_cell_ids,
            "increase_cell_ids": increase_cell_ids
        }

        with open(self.params.pre_proc_dir+"cell_classification/"+self.params.session_name+"_k_means.pickle", "wb") as f:
            pickle.dump(cell_class_dic, f, pickle.HIGHEST_PROTOCOL)

        k_labels_sorted = k_labels.argsort()

        fig = plt.figure(figsize=(8,6))
        gs = fig.add_gridspec(6, 20)

        ax1 = fig.add_subplot(gs[:, 0])
        ax1.set_title("CLUSTER \n IDs")
        ax2 = fig.add_subplot(gs[:, 3:-2])
        ax3 = fig.add_subplot(gs[:, -1:])

        # plotting

        ax1.imshow(np.expand_dims(k_labels[k_labels_sorted], 1), aspect="auto", cmap="tab10")
        ax1.axis("off")
        rate_map = ax2.imshow(raster_ds_smoothed[k_labels_sorted,:], interpolation='nearest', aspect='auto')
        ax2.set_xlabel("BIN ID")
        ax2.set_ylabel("CELLS SORTED")
        a = plt.colorbar(rate_map, cax=ax3)
        a.set_label("#SPIKES / NORMALIZED")
        plt.show()


        exit()

        # new_ml = MlMethodsOnePopulation()
        # weights = new_ml.ridge_time_bin_progress(x=raster, y=times, new_time_bin_size=str(self.params.spikes_per_bin)+" SPIKE")
        # sort according to weights
        # weights_sorted = weights.argsort()

        # new labels to sort according to increase/decrease
        # decreasing labels: 30+
        # stable labels: 50+
        # increasing labels: 80+

        new_labels_1 = np.zeros(raster_ds_smoothed.shape[0])

        for cl_id in decrease:
            new_labels_1[k_labels == cl_id] = 30+cl_id

        for cl_id in increase:
            new_labels_1[k_labels == cl_id] = 80+cl_id

        for cl_id in stable:
            new_labels_1[k_labels == cl_id] = 50+cl_id

        # have smaller numbers again
        combined_k_labels_sorted = new_labels_1.argsort()

        a = np.unique(new_labels_1)
        a = np.sort(a)
        for c_id, n_id in zip(a, np.arange(a.shape[0])):
            new_labels_1[new_labels_1 == c_id] = n_id

        # new_labels = np.zeros(raster_ds_smoothed.shape[0])
        # for cl_id in decrease:
        #     new_labels[k_labels == cl_id] = 1
        #
        # for cl_id in increase:
        #     new_labels[k_labels == cl_id] = 2

        combined_k_labels_sorted = new_labels_1.argsort()


        fig = plt.figure(figsize=(8,6))
        gs = fig.add_gridspec(6, 20)

        ax1 = fig.add_subplot(gs[:, 0])
        ax1.set_title("CLUSTERS")
        ax2 = fig.add_subplot(gs[:, 3:-2])
        ax3 = fig.add_subplot(gs[:, -1:])

        # plotting

        ax1.imshow(np.expand_dims(new_labels_1[combined_k_labels_sorted], 1), aspect="auto", cmap="tab10")
        ax1.axis("off")
        rate_map = ax2.imshow(raster_ds_smoothed[combined_k_labels_sorted,:], interpolation='nearest', aspect='auto')
        ax2.set_xlabel("BIN ID")
        ax2.set_ylabel("CELLS SORTED")
        a = plt.colorbar(rate_map, cax=ax3)
        a.set_label("#SPIKES / NORMALIZED")
        ax2.set_title("COMBINED CLUSTERS")
        plt.savefig("ex1.pdf")
        plt.show()

    def firing_rate_changes(self, smoothing=200, plotting=False, pop_vec_threshold_rem=10,
                            pop_vec_threshold_nrem=2, stats_test="mwu", use_only_non_stationary_periods=True,
                            save_fig=False, return_p_value=True, use_firing_prob=True):
        """
        assesses firing rate changes of subsets of cells (stable, inc, dec) during different sleep phases (nrem, rem)

        :param smoothing: smoothing used to smooth firing rates across epochs
        :type smoothing: int
        :param plotting: whether to plot (True) or return the results (False)
        :type plotting: bool
        :param pop_vec_threshold_rem: min. number of pop. vec. per rem epochs (shorter epochs are discarded)
        :type pop_vec_threshold_rem: int
        :param pop_vec_threshold_nrem: min. number of pop. vec. per nrem epoch (shorter epochs are discarded)
        :type pop_vec_threshold_nrem: int
        :param stats_test: which stats to use for comparing firing rate changes between nrem/rem ("t_test", "mwu", "ks",
                           "mwu_one_sided", "anova", "t_test_one_sided")
        :type stats_test: str
        :param use_only_non_stationary_periods: whether to use only periods where firing rates change (True)
        :type use_only_non_stationary_periods: bool
        :param save_fig: whether to save figure (True) or not
        :type save_fig: bool
        :return: p_dec, p_inc --> p-value for difference between NREM/REM for decreasing cells, p-value for difference
                                  between NREM/REM for increasing cells
        :rtype: float, float
        """
        # get stable, decreasing, increasing cells
        with open(self.params.pre_proc_dir + "cell_classification/" +
                  self.session_name + "_"+self.params.stable_cell_method+".pickle", "rb") as f:
            class_dic = pickle.load(f)

        stable_ids = class_dic["stable_cell_ids"]
        inc_ids = class_dic["increase_cell_ids"]
        dec_ids = class_dic["decrease_cell_ids"]

        print("#stable: "+str(stable_ids.shape[0])+", #inc: "+str(inc_ids.shape[0])+", #dec: "+str(dec_ids.shape[0]))

        # get REM and NREM rasters & times
        # --------------------------------------------------------------------------------------------------------------
        rem_rasters = []
        nrem_rasters = []
        event_times_rem = []
        event_end_times_rem = []
        event_times_nrem = []
        event_end_times_nrem = []

        first = 0
        for l_s in self.long_sleep:
            duration = l_s.get_duration_sec()
            # all_event_rasters = all_event_rasters + l_s.get_event_spike_rasters(part_to_analyze=part_to_analyze)[0]
            ls_rem_raster, _, start_times_rem, end_times_rem = l_s.get_event_spike_rasters(part_to_analyze="rem",
                                                                                return_event_times=True,
                                                                                pop_vec_threshold=pop_vec_threshold_rem)

            if self.session_name == "mjc169R1R_0114":
                # nrem labels for this session are missing --> use sw instead
                part_to_analyze = "all_swr"
            else:
                part_to_analyze = "nrem"

            ls_nrem_raster, _, start_times_nrem, end_times_nrem = \
                l_s.get_event_spike_rasters(part_to_analyze=part_to_analyze, return_event_times=True,
                                            pop_vec_threshold=pop_vec_threshold_nrem)
            rem_rasters.extend(ls_rem_raster)
            nrem_rasters.extend(ls_nrem_raster)

            event_times_rem.extend(first+start_times_rem)
            event_times_nrem.extend(first + start_times_nrem)
            event_end_times_rem.extend(first + end_times_rem)
            event_end_times_nrem.extend(first + end_times_nrem)
            first += duration

        # compute duration in hours
        duration_h = first/60/60

        event_times_nrem = np.vstack(event_times_nrem)
        event_times_rem = np.vstack(event_times_rem)

        # combine rem and nrem data to order it in the right way (chronologically)
        # --------------------------------------------------------------------------------------------------------------
        all_events_raster = rem_rasters + nrem_rasters
        labels_events = np.zeros(len(all_events_raster))
        labels_events[:len(rem_rasters)] = 1
        all_times = np.vstack((event_times_rem, event_times_nrem))
        all_end_times = np.hstack((event_end_times_rem, event_end_times_nrem))

        # sort events according to time
        sorted_events_raster = [x for _, x in sorted(zip(all_times, all_events_raster))]
        sorted_labels_events = [x for _, x in sorted(zip(all_times, labels_events))]
        sorted_times = [x for x in sorted(all_times)]
        sorted_end_times = [x for _, x in sorted(zip(all_times, all_end_times))]
        if sorted_labels_events[0] == 1:
            first_event_label = "rem"
        else:
            first_event_label = "nrem"

        # merge neighboring epochs if they have the same label (rem events == 1, nrem events == 0)
        merged_event_rasters = []
        merged_event_labels = []
        tmp_merged_event_labels = []
        tmp_merged_event_time = np.zeros(2)
        prev_end_time = None
        tmp_merged_event_raster = np.empty((sorted_events_raster[0].shape[0],0))
        prev = sorted_labels_events[0]
        tmp_merged_event_time[0] = sorted_times[0]
        merged_events_times = []
        for label, raster, start_time, end_time in zip(sorted_labels_events, sorted_events_raster,
                                                       sorted_times, sorted_end_times):
            if label == prev:
                # need to merge the two
                tmp_merged_event_raster = np.hstack((tmp_merged_event_raster, raster))
                tmp_merged_event_labels.append(label)
            else:
                # add last merged event
                merged_event_rasters.append(tmp_merged_event_raster)
                merged_event_labels.append(tmp_merged_event_labels)
                # get end time from prev. event (is the last event belonging to the merged event)
                tmp_merged_event_time[1] = prev_end_time
                merged_events_times.append(tmp_merged_event_time)
                # need to start with new merged event
                tmp_merged_event_raster = np.empty((sorted_events_raster[0].shape[0],0))
                tmp_merged_event_labels = []
                tmp_merged_event_time = np.zeros(2)
                # need start time since it is a new merged event
                tmp_merged_event_time[0] = start_time
                tmp_merged_event_raster = np.hstack((tmp_merged_event_raster, raster))
                tmp_merged_event_labels.append(label)
                # remember current end time, in case this is the last event belonging to the merged event
            prev_end_time = end_time
            prev = label

        len_new_events = [x.shape[1] for x in merged_event_rasters]
        merged_event_labels = [x[0] for x in merged_event_labels]
        one_raster_all_events = np.hstack(merged_event_rasters)
        # get nr of all cells
        nr_cells = one_raster_all_events.shape[0]
        # split raster for each subset
        one_raster_all_events_stable = one_raster_all_events[stable_ids, :]
        one_raster_all_events_inc = one_raster_all_events[inc_ids, :]
        one_raster_all_events_dec = one_raster_all_events[dec_ids, :]

        if use_firing_prob:
            # firing probability
            one_raster_all_events_dec_mean = np.sum(one_raster_all_events_dec, axis=0) / \
                                             self.params.spikes_per_bin
            one_raster_all_events_inc_mean = np.sum(one_raster_all_events_inc, axis=0) / \
                                             self.params.spikes_per_bin
            one_raster_all_events_stable_mean = np.sum(one_raster_all_events_stable, axis=0) / \
                                                self.params.spikes_per_bin

        else:
            # compute mean firing for all subsets of cells (stable, inc, dec) and apply smoothing
            one_raster_all_events_dec_mean = np.mean(one_raster_all_events_dec, axis=0)
            one_raster_all_events_inc_mean = np.mean(one_raster_all_events_inc, axis=0)
            one_raster_all_events_stable_mean = np.mean(one_raster_all_events_stable, axis=0)


        one_raster_all_events_dec_mean_smooth = moving_average(one_raster_all_events_dec_mean, n=smoothing)

        one_raster_all_events_inc_mean_smooth = moving_average(one_raster_all_events_inc_mean, n=smoothing)

        one_raster_all_events_stable_mean_smooth = moving_average(one_raster_all_events_stable_mean, n=smoothing)

        pop_vec_per_h = one_raster_all_events_dec_mean_smooth.shape[0]/duration_h

        window_size = int(pop_vec_per_h/2)

        # check if mean firing is actually decreasing
        dec_mean_smooth_decreasing = np.zeros(one_raster_all_events_dec_mean_smooth.shape[0])
        inc_mean_smooth_increasing = np.zeros(one_raster_all_events_inc_mean_smooth.shape[0])

        for window_id in range(int(one_raster_all_events_dec_mean_smooth.shape[0]/window_size)):
            x = np.arange(window_size)
            segment_dec = one_raster_all_events_dec_mean_smooth[window_id*window_size:(window_id+1)*window_size]
            segment_inc = one_raster_all_events_inc_mean_smooth[window_id*window_size:(window_id+1)*window_size]
            coef_dec = np.polyfit(x, segment_dec, 1)
            coef_inc = np.polyfit(x, segment_inc, 1)
            # poly1d_fn = np.poly1d(coef)
            # plt.plot(x, poly1d_fn(x), "r")
            # plt.plot(x, segment)
            # plt.title(coef)
            # plt.show()
            # check if coefficient of regression is negative --> cells decrease their firing
            if coef_dec[0] < 0:
                dec_mean_smooth_decreasing[window_id*window_size:(window_id+1)*window_size] = 1
            if coef_inc[0] > 0:
                inc_mean_smooth_increasing[window_id*window_size:(window_id+1)*window_size] = 1

        # split raster into single epochs/events again
        dec_smooth_rem = []
        dec_smooth_nrem = []
        dec_smooth = []
        dec_smooth_delta = []
        inc_smooth_rem = []
        inc_smooth_nrem = []
        inc_smooth = []
        inc_smooth_delta = []
        stable_smooth_rem = []
        stable_smooth_nrem = []
        stable_smooth = []
        stable_smooth_delta = []
        valid_dec_event = np.zeros(len(merged_event_labels))
        valid_inc_event = np.zeros(len(merged_event_labels))

        start = 0

        for i, (event_len, label) in enumerate(zip(len_new_events, merged_event_labels)):

            end = min(start + event_len, one_raster_all_events_dec_mean_smooth.shape[0]-1)

            # check if start still lies within raster start --> get shorter through smoothing
            if start >= one_raster_all_events_dec_mean_smooth.shape[0]:
                break
            # check if firing rates were actually decreasing: 70% of event need to lie in decreasing period
            if (end - start) != 0 and np.count_nonzero(dec_mean_smooth_decreasing[start:end]) / (end - start) > 0.7:
                valid_dec_event[i] = 1

            if (end - start) != 0 and np.count_nonzero(inc_mean_smooth_increasing[start:end]) / (end - start) > 0.7:
                valid_inc_event[i] = 1

            # check which label (rem == 1, nrem == 0)
            if label == 1:
                dec_smooth_rem.append(one_raster_all_events_dec_mean_smooth[start:end])
                inc_smooth_rem.append(one_raster_all_events_inc_mean_smooth[start:end])
                stable_smooth_rem.append(one_raster_all_events_stable_mean_smooth[start:end])
            else:
                dec_smooth_nrem.append(one_raster_all_events_dec_mean_smooth[start:end])
                inc_smooth_nrem.append(one_raster_all_events_inc_mean_smooth[start:end])
                stable_smooth_nrem.append(one_raster_all_events_stable_mean_smooth[start:end])
            dec_smooth.append(one_raster_all_events_dec_mean_smooth[start:end])
            dec_smooth_delta.append((one_raster_all_events_dec_mean_smooth[end]-
                                     one_raster_all_events_dec_mean_smooth[start]))
            inc_smooth.append(one_raster_all_events_inc_mean_smooth[start:end])
            inc_smooth_delta.append(one_raster_all_events_inc_mean_smooth[end]-
                                    one_raster_all_events_inc_mean_smooth[start])
            stable_smooth.append(one_raster_all_events_stable_mean_smooth[start:end])
            stable_smooth_delta.append(one_raster_all_events_stable_mean_smooth[end]-
                                       one_raster_all_events_stable_mean_smooth[start])
            start = end

        # might need to delete last label
        merged_event_labels = merged_event_labels[:len(dec_smooth)]
        # correlation of delta average firing rate and epoch length
        # compute duration of event
        merged_times = np.vstack(merged_events_times)
        merged_dur = merged_times[:, 1] - merged_times[:, 0]

        # REM vs. NREM
        merged_event_labels_arr = np.array(merged_event_labels)

        # # REM vs. NREM
        # rem_raster = np.hstack(rem_rasters)
        # rem_raster_dec = rem_raster[dec_ids,:]
        # mean_firing_rem_dec = np.mean(rem_raster_dec, axis=0)
        # mean_firing_rem_dec_smooth = moving_average(a=mean_firing_rem_dec, n=2000)
        #
        # nrem_raster = np.hstack(nrem_rasters)
        # nrem_raster_dec = nrem_raster[dec_ids,:]
        # mean_firing_nrem_dec = np.mean(nrem_raster_dec, axis=0)
        # mean_firing_nrem_dec_smooth = moving_average(a=mean_firing_nrem_dec, n=200)
        #
        # plt.plot(mean_firing_rem_dec_smooth, color="r")
        # plt.plot(mean_firing_nrem_dec_smooth, color="b")
        # plt.show()

        # filter stationary periods if requested
        if use_only_non_stationary_periods:
            rem_dec_smooth = np.array(dec_smooth_delta)[(merged_event_labels_arr == 1) & (valid_dec_event==1)]
            nrem_dec_smooth = np.array(dec_smooth_delta)[(merged_event_labels_arr == 0) & (valid_dec_event==1)]

            rem_inc_smooth = np.array(inc_smooth_delta)[(merged_event_labels_arr == 1) & (valid_inc_event==1)]
            nrem_inc_smooth = np.array(inc_smooth_delta)[(merged_event_labels_arr == 0) & (valid_inc_event==1)]
        else:
            rem_dec_smooth = np.array(dec_smooth_delta)[merged_event_labels_arr == 1]
            nrem_dec_smooth = np.array(dec_smooth_delta)[merged_event_labels_arr == 0]

            rem_inc_smooth = np.array(inc_smooth_delta)[merged_event_labels_arr == 1]
            nrem_inc_smooth = np.array(inc_smooth_delta)[merged_event_labels_arr == 0]

        rem_stable_smooth = np.array(stable_smooth_delta)[merged_event_labels_arr == 1]
        nrem_stable_smooth = np.array(stable_smooth_delta)[merged_event_labels_arr == 0]

        # plt.plot(rem_dec_smooth, c="red")
        # plt.plot(nrem_dec_smooth, c="blue")
        # plt.show()

        # --------------------------------------------------------------------------------------------------------------
        # remove first part of sleep if nothing is happening
        # --------------------------------------------------------------------------------------------------------------

        if self.session_name == "mjc163R1L_0114":
            # cut the beginning of this session --> firing rates do not change initially
            rem_dec_smooth = rem_dec_smooth[20:]
            nrem_dec_smooth = nrem_dec_smooth[20:]
            rem_inc_smooth = rem_inc_smooth[20:]
            nrem_inc_smooth = nrem_inc_smooth[20:]
        if self.session_name == "mjc163R4R_0114":
            # cut the beginning of this session --> firing rates do not change initially
            rem_dec_smooth = rem_dec_smooth[30:]
            nrem_dec_smooth = nrem_dec_smooth[30:]
            rem_inc_smooth = rem_inc_smooth[30:]
            nrem_inc_smooth = nrem_inc_smooth[30:]

        # --------------------------------------------------------------------------------------------------------------
        # check directionality
        # --------------------------------------------------------------------------------------------------------------
        rem_dec_pos = np.count_nonzero(rem_dec_smooth > 0)
        rem_dec_neg = np.count_nonzero(rem_dec_smooth < 0)

        rem_bin_dist = np.hstack((np.zeros(rem_dec_pos),np.ones(rem_dec_neg)))

        nrem_dec_pos = np.count_nonzero(nrem_dec_smooth > 0)
        nrem_dec_neg = np.count_nonzero(nrem_dec_smooth < 0)

        nrem_bin_dist = np.hstack((np.zeros(nrem_dec_pos),np.ones(nrem_dec_neg)))

        # --------------------------------------------------------------------------------------------------------------
        # perform statistical test
        # --------------------------------------------------------------------------------------------------------------

        if stats_test == "mwu":
            p_dec = mannwhitneyu(rem_dec_smooth, nrem_dec_smooth, alternative="greater")[1]
            p_inc = mannwhitneyu(rem_inc_smooth, nrem_inc_smooth, alternative="less")[1]
        elif stats_test == "mwu_two_sided":
            p_dec = mannwhitneyu(rem_dec_smooth, nrem_dec_smooth)[1]
            p_inc = mannwhitneyu(rem_inc_smooth, nrem_inc_smooth)[1]
        elif stats_test == "ks":
            p_dec = ks_2samp(rem_dec_smooth, nrem_dec_smooth, alternative="less")[1]
            p_inc = ks_2samp(rem_inc_smooth, nrem_inc_smooth, alternative="greater")[1]
        elif stats_test == "anova":
            p_dec = f_oneway(rem_dec_smooth, nrem_dec_smooth)[1]
            p_inc = f_oneway(rem_inc_smooth, nrem_inc_smooth)[1]
        elif stats_test == "t_test":
            p_dec = ttest_ind(rem_dec_smooth, nrem_dec_smooth, alternative="greater")[1]
            p_inc = ttest_ind(rem_inc_smooth, nrem_inc_smooth, alternative="less")[1]
        elif stats_test == "t_test_two_sided":
            p_dec = ttest_ind(rem_dec_smooth, nrem_dec_smooth)[1]
            p_inc = ttest_ind(rem_inc_smooth, nrem_inc_smooth)[1]
        else:
            raise Exception("THIS STATS TEST IS NOT IMPLEMENTED!!!")

        print("Stats test for decreasing cells ("+stats_test+"):")
        print("p-value = "+str(p_dec))
        print("Stats test for increasing cells ("+stats_test+"):")
        print("p-value = "+str(p_inc))

        # --------------------------------------------------------------------------------------------------------------
        # compute CDF
        # --------------------------------------------------------------------------------------------------------------

        rem_dec_sorted = np.sort(rem_dec_smooth)
        nrem_dec_sorted = np.sort(nrem_dec_smooth)
        # calculate the proportional values of samples
        p_dec_data = 1. * np.arange(rem_dec_sorted.shape[0]) / (rem_dec_sorted.shape[0] - 1)
        p_dec_shuffle = 1. * np.arange(nrem_dec_sorted.shape[0]) / (nrem_dec_sorted.shape[0] - 1)

        rem_inc_sorted = np.sort(rem_inc_smooth)
        nrem_inc_sorted = np.sort(nrem_inc_smooth)
        # calculate the proportional values of samples
        p_inc_data = 1. * np.arange(rem_inc_sorted.shape[0]) / (rem_inc_sorted.shape[0] - 1)
        p_inc_shuffle = 1. * np.arange(nrem_inc_sorted.shape[0]) / (nrem_inc_sorted.shape[0] - 1)

        # --------------------------------------------------------------------------------------------------------------
        # plot CDF
        # --------------------------------------------------------------------------------------------------------------
        if save_fig:
            plt.style.use('default')

        if save_fig or plotting:
            plt.figure(figsize=(6,2))
            plt.plot(rem_dec_sorted, p_dec_data, label="REM", color="r")
            plt.plot(nrem_dec_sorted, p_dec_shuffle, label="NREM", color="b")
            plt.legend()
            plt.ylabel("CDF")
            plt.xlabel("DELTA FIRING PROBABILITY")
            plt.xlim(np.min(nrem_dec_sorted), np.max(rem_dec_sorted))
            plt.ylim(0,1)
            plt.yticks([0,0.5,1])
            plt.xticks([np.min(nrem_dec_sorted), 0, np.max(rem_dec_sorted)])
            plt.grid(axis="x")
            plt.title("Decreasing cells")
            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig("dec_firing_changes.svg", transparent="True")
                plt.close()
            elif plotting:
                plt.show()

        if save_fig or plotting:
            # increasing cells
            plt.figure(figsize=(6,2))
            plt.plot(rem_inc_sorted, p_inc_data, label="REM", color="r")
            plt.plot(nrem_inc_sorted, p_inc_shuffle, label="NREM", color="b")
            plt.legend()
            plt.ylabel("CDF")
            plt.xlabel("DELTA FIRING PROBABILITY")
            plt.ylim(0,1)
            plt.yticks([0,0.5,1])
            # plt.xticks([-0.035, 0, 0.035])
            plt.grid(axis="x")
            plt.title("Increasing cells")
            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig("inc_firing_changes.svg", transparent="True")
            elif plotting:
                plt.show()

        if save_fig:
            dur_per_pop_vec = duration_h/np.hstack(inc_smooth).shape[0]
            fig = plt.figure()
            ax = fig.add_subplot()
            first = 0
            for label, res in zip(merged_event_labels, inc_smooth):
                # rem
                if label == 1:
                    ax.plot(first + np.arange(res.shape[0]), res, c="r", label="REM")
                else:
                    ax.plot(first + np.arange(res.shape[0]), res, c="b", label="NREM")
                first += res.shape[0]
            handles, labels = ax.get_legend_handles_labels()
            by_label = OrderedDict(zip(labels, handles))
            ax.legend(by_label.values(), by_label.keys())
            plt.xlabel("Duration sleep")
            plt.ylabel("Firing probability")
            # plt.yticks([])
            plt.xlim([51200,52900])
            plt.xticks([51200,52900],[str(np.round(51200*dur_per_pop_vec,2)),str(np.round(52900*dur_per_pop_vec,2))])
            plt.ylim([0.22,0.38])
            plt.rcParams['svg.fonttype'] = 'none'
            plt.savefig("firing_changes.svg", transparent="True")
            # plt.show()

        # --------------------------------------------------------------------------------------------------------------
        # plot additional figures & controls
        # --------------------------------------------------------------------------------------------------------------
        if plotting:
            # plot CDF for stable cells
            # ----------------------------------------------------------------------------------------------------------
            rem_sorted = np.sort(rem_stable_smooth)
            nrem_sorted = np.sort(nrem_stable_smooth)
            # calculate the proportional values of samples
            p_stable_rem = 1. * np.arange(rem_sorted.shape[0]) / (rem_sorted.shape[0] - 1)
            p_stable_nrem = 1. * np.arange(nrem_sorted.shape[0]) / (nrem_sorted.shape[0] - 1)

            plt.plot(rem_sorted, p_stable_rem, label="REM", color="r")
            plt.plot(nrem_sorted, p_stable_nrem, label="NREM", color="b")
            plt.legend()
            plt.ylabel("CDF")
            plt.xlabel("DELTA MEAN FIRING")
            plt.title("Stable")
            plt.show()

            # plot histograms
            # ----------------------------------------------------------------------------------------------------------
            plt.hist(rem_inc_smooth, color="r", label="REM", orientation='horizontal', density=True)
            plt.hist(nrem_inc_smooth, color="b", label="NREM", alpha=0.5, orientation='horizontal', density=True)
            plt.title("DELTA MEAN FIRING: INCREASING CELLS")
            plt.legend()
            plt.xlabel("DENSITY")
            plt.ylabel("DELTA MEAN FIRING PER EPOCH")
            plt.ylim(-max(abs(np.array(inc_smooth_delta))), max(abs(np.array(inc_smooth_delta))))
            plt.show()

            plt.hist(rem_stable_smooth, color="r", label="REM", orientation='horizontal', density=True)
            plt.hist(nrem_stable_smooth, color="b", label="NREM", alpha=0.5, orientation='horizontal', density=True)
            plt.title("DELTA MEAN FIRING: STABLE CELLS")
            plt.legend()
            plt.xlabel("DENSITY")
            plt.ylabel("DELTA MEAN FIRING PER EPOCH")
            plt.ylim(-max(abs(np.array(stable_smooth_delta))), max(abs(np.array(stable_smooth_delta))))
            plt.show()

            plt.hist(rem_dec_smooth, color="r", label="REM", orientation='horizontal', density=True)
            plt.hist(nrem_dec_smooth, color="b", label="NREM", alpha=0.5, orientation='horizontal', density=True)
            plt.title("DELTA MEAN FIRING: DECREASING CELLS")
            plt.legend()
            plt.xlabel("DENSITY")
            plt.ylabel("DELTA MEAN FIRING PER EPOCH")
            plt.ylim(-max(abs(np.array(dec_smooth_delta))), max(abs(np.array(dec_smooth_delta))))
            plt.show()

            # plot delta mean firing values per epoch
            # ----------------------------------------------------------------------------------------------------------
            fig = plt.figure()
            ax = fig.add_subplot()
            # plot results
            if use_only_non_stationary_periods:
                first = 0
                for label, res, val in zip(merged_event_labels, dec_smooth_delta, valid_dec_event):
                    # rem
                    if label == 1 and val == 1:
                        ax.scatter(first, res, c="r", label="REM")
                    elif label == 0 and val == 1:
                        ax.scatter(first, res, c="b", label="NREM")
                    first += 1
            else:
                first = 0
                for label, res in zip(merged_event_labels, dec_smooth_delta):
                    # rem
                    if label == 1:
                        ax.scatter(first, res, c="r", label="REM")
                    else:
                        ax.scatter(first, res, c="b", label="NREM")
                    first += 1

            handles, labels = ax.get_legend_handles_labels()
            by_label = OrderedDict(zip(labels, handles))
            ax.legend(by_label.values(), by_label.keys())
            plt.title("DECREASING CELLS")
            plt.ylabel("DELTA IN MEAN FIRING RATE")
            plt.xlabel("EPOCH ID")
            plt.show()

            fig = plt.figure()
            ax = fig.add_subplot()
            if use_only_non_stationary_periods:
                first = 0
                for label, res, val in zip(merged_event_labels, inc_smooth_delta, valid_inc_event):
                    # rem
                    if label == 1 and val == 1:
                        ax.scatter(first, res, c="r", label="REM")
                    elif label == 0 and val == 1:
                        ax.scatter(first, res, c="b", label="NREM")
                    first += 1
            else:
                first = 0
                for label, res in zip(merged_event_labels, inc_smooth_delta):
                    # rem
                    if label == 1:
                        ax.scatter(first, res, c="r", label="REM")
                    else:
                        ax.scatter(first, res, c="b", label="NREM")
                    first += 1

            handles, labels = ax.get_legend_handles_labels()
            by_label = OrderedDict(zip(labels, handles))
            ax.legend(by_label.values(), by_label.keys())
            plt.title("INCREASING CELLS")
            plt.ylabel("DELTA IN MEAN FIRING RATE")
            plt.xlabel("EPOCH ID")
            plt.show()
            #
            fig = plt.figure()
            ax = fig.add_subplot()
            first = 0
            for label, res in zip(merged_event_labels, stable_smooth_delta):
                # rem
                if label == 1:
                    ax.scatter(first, res, c="r", label="REM")
                else:
                    ax.scatter(first, res, c="b", label="NREM")
                first += 1

            handles, labels = ax.get_legend_handles_labels()
            by_label = OrderedDict(zip(labels, handles))
            ax.legend(by_label.values(), by_label.keys())
            plt.title("STABLE CELLS")
            plt.ylabel("DELTA IN MEAN FIRING RATE")
            plt.xlabel("EPOCH ID")
            plt.show()

            # plot mean firing over time
            # ----------------------------------------------------------------------------------------------------------

            fig = plt.figure()
            ax = fig.add_subplot()
            first = 0
            for label, res in zip(merged_event_labels, dec_smooth):
                # rem
                if label == 1:
                    ax.plot(first + np.arange(res.shape[0]), res, c="r", label="REM")
                else:
                    ax.plot(first + np.arange(res.shape[0]), res, c="b", label="NREM")
                first += res.shape[0]
            handles, labels = ax.get_legend_handles_labels()
            by_label = OrderedDict(zip(labels, handles))
            ax.legend(by_label.values(), by_label.keys())
            plt.xlabel("POP. VEC ID")
            plt.ylabel("MEAN FIRING RATE")
            if use_only_non_stationary_periods:
                plt.plot(dec_mean_smooth_decreasing*0.05, c="yellow")
            plt.title("DECREASING CELLS")
            plt.show()

            fig = plt.figure()
            ax = fig.add_subplot()
            first = 0
            for label, res in zip(merged_event_labels, inc_smooth):
                # rem
                if label == 1:
                    ax.plot(first + np.arange(res.shape[0]), res, c="r", label="REM")
                else:
                    ax.plot(first + np.arange(res.shape[0]), res, c="b", label="NREM")
                first += res.shape[0]
            handles, labels = ax.get_legend_handles_labels()
            by_label = OrderedDict(zip(labels, handles))
            ax.legend(by_label.values(), by_label.keys())
            plt.xlabel("POP. VEC ID")
            plt.ylabel("MEAN FIRING RATE")
            if use_only_non_stationary_periods:
                plt.plot(inc_mean_smooth_increasing*0.05, c="yellow")
            plt.title("INCREASING CELLS")
            plt.show()

            fig = plt.figure()
            ax = fig.add_subplot()
            first = 0
            for label, res in zip(merged_event_labels, stable_smooth):
                # rem
                if label == 1:
                    ax.plot(first + np.arange(res.shape[0]), res, c="r", label="REM")
                else:
                    ax.plot(first + np.arange(res.shape[0]), res, c="b", label="NREM")
                first += res.shape[0]
            handles, labels = ax.get_legend_handles_labels()
            by_label = OrderedDict(zip(labels, handles))
            ax.legend(by_label.values(), by_label.keys())
            plt.xlabel("POP. VEC ID")
            plt.ylabel("MEAN FIRING RATE")
            plt.title("STABLE CELLS")
            plt.show()

            # plot delta firing for neighbouring epochs (scatter plot)
            # ----------------------------------------------------------------------------------------------------------

            max_len = min(rem_dec_smooth.shape[0], nrem_dec_smooth.shape[0])
            plt.scatter(rem_dec_smooth[:max_len], nrem_dec_smooth[:max_len])
            plt.title("NEIGBOURING PERIODS: DECREASING CELLS\nR=" + str(np.round(pearsonr(
                rem_dec_smooth[:max_len], nrem_dec_smooth[:max_len])[0], 2)))
            plt.xlabel("DELTA MEAN FIRING REM")
            plt.ylabel("DELTA MEAN FIRING NREM")
            plt.show()

            plt.scatter(rem_inc_smooth[:max_len], nrem_inc_smooth[:max_len])
            plt.title("NEIGBOURING PERIODS: INCREASING CELLS\nR=" + str(np.round(pearsonr(
                rem_inc_smooth[:max_len], nrem_inc_smooth[:max_len])[0], 2)))
            plt.xlabel("DELTA MEAN FIRING REM")
            plt.ylabel("DELTA MEAN FIRING NREM")
            plt.show()

            plt.scatter(rem_stable_smooth[:max_len], nrem_stable_smooth[:max_len])
            plt.title("NEIGBOURING PERIODS: STABLE CELLS\nR=" + str(np.round(pearsonr(
                rem_stable_smooth[:max_len], nrem_stable_smooth[:max_len])[0], 2)))
            plt.xlabel("DELTA MEAN FIRING REM")
            plt.ylabel("DELTA MEAN FIRING NREM")
            plt.show()

            plt.scatter(merged_dur, np.array(dec_smooth_delta), label="DEC. CELLS, R = "+str(
                np.round(pearsonr(merged_dur, np.array(dec_smooth_delta))[0],2)), marker="_", color="lightcoral")
            plt.scatter(merged_dur, np.array(inc_smooth_delta), label="INC. CELLS, R = "+str(
                np.round(pearsonr(merged_dur, np.array(inc_smooth_delta))[0],2)), marker="+", color="cornflowerblue")
            plt.scatter(merged_dur, np.array(stable_smooth_delta), label="STABLE. CELLS, R = "+str(
                np.round(pearsonr(merged_dur, np.array(stable_smooth_delta))[0],2)), s=2.5, color="white")
            plt.xlabel("DURATION EPOCH (s)")
            plt.ylabel("DELTA MEAN FIRING")
            plt.legend()
            plt.title("DURATION EPOCH VS. DELTA MEAN FIRING")
            plt.show()

            # control for epoch length
            # ----------------------------------------------------------------------------------------------------------
            thresh_dur = 3000
            plt.scatter(merged_dur[merged_dur < thresh_dur], np.array(dec_smooth_delta)[merged_dur < thresh_dur],
                        label="DEC. CELLS, R = "+str(np.round(pearsonr(merged_dur[merged_dur < thresh_dur],
                        np.array(dec_smooth_delta)[merged_dur < thresh_dur])[0],2)), marker="_", color="lightcoral")
            plt.scatter(merged_dur[merged_dur < thresh_dur],
                        np.array(inc_smooth_delta)[merged_dur < thresh_dur], label="INC. CELLS, R = "+str(
                        np.round(pearsonr(merged_dur[merged_dur < thresh_dur],
                        np.array(inc_smooth_delta)[merged_dur < thresh_dur])[0],2)), marker="+", color="cornflowerblue")
            plt.scatter(merged_dur[merged_dur < thresh_dur], np.array(stable_smooth_delta)[merged_dur < thresh_dur],
                        label="STABLE. CELLS, R = "+str(np.round(pearsonr(merged_dur[merged_dur < thresh_dur],
                                np.array(stable_smooth_delta)[merged_dur < thresh_dur])[0],2)), s=2.5, color="white")
            plt.xlabel("DURATION EPOCH (s)")
            plt.ylabel("DELTA MEAN FIRING")
            plt.legend()
            plt.title("DURATION EPOCH VS. DELTA MEAN FIRING\n DURATION < "+str(thresh_dur)+"s")
            plt.show()

        else:
            if return_p_value:
                return p_dec, p_inc
            else:
                return nrem_dec_smooth, rem_dec_smooth, rem_inc_smooth, nrem_inc_smooth, first_event_label

    def firing_rate_changes_neighbouring_epochs(self):

        nrem_dec_smooth, rem_dec_smooth, rem_inc_smooth, nrem_inc_smooth, first_event_label = \
            self.firing_rate_changes(return_p_value=False, use_only_non_stationary_periods=False)

        if nrem_dec_smooth.shape[0] < rem_dec_smooth.shape[0] and first_event_label == "rem":
            rem_dec_smooth = rem_dec_smooth[:-1]
            rem_inc_smooth = rem_inc_smooth[:-1]
        elif nrem_dec_smooth.shape[0] > rem_dec_smooth.shape[0] and first_event_label == "nrem": 
            nrem_dec_smooth = nrem_dec_smooth[:-1]
            nrem_inc_smooth = nrem_inc_smooth[:-1]
        #
        # pearsonr(nrem_dec_smooth, rem_dec_smooth)
        # pearsonr(nrem_inc_smooth, rem_inc_smooth)

        return nrem_dec_smooth, rem_dec_smooth, nrem_inc_smooth, rem_inc_smooth

    def firing_rate_distributions(self, cells_to_use="stable", plotting=True, separate_sleep_phases=True,
                                  chunks_in_min=2, measure="mean"):
        print("Processing " +self.session_name)

        with open(self.params.pre_proc_dir + "cell_classification/" +
                  self.session_name + "_" + self.params.stable_cell_method + ".pickle", "rb") as f:
            class_dic = pickle.load(f)

        if cells_to_use == "stable":
            cell_ids = class_dic["stable_cell_ids"].flatten()
        elif cells_to_use =="increasing":
            cell_ids = class_dic["increase_cell_ids"].flatten()
        elif cells_to_use == "decreasing":
            cell_ids = class_dic["decrease_cell_ids"].flatten()

        chunk_size = int(chunks_in_min/self.params.time_bin_size)

        if separate_sleep_phases:
            raster_nrem = self.get_sleep_phase_raster(sleep_phase="nrem")
            raster_rem = self.get_sleep_phase_raster(sleep_phase="rem")

            # go through REM
            nr_chunks_rem = int(raster_rem.shape[1] / chunk_size)
            firing_rem = np.zeros((raster_rem.shape[0], nr_chunks_rem))
            for chunk in range(nr_chunks_rem):
                if measure == "mean":
                    firing_rem[:, chunk] = np.mean(
                        raster_rem[:, chunk * chunk_size:(chunk + 1) * chunk_size], axis=1) / self.params.time_bin_size
                elif measure == "max":
                    firing_rem[:, chunk] = np.max(
                        raster_rem[:, chunk * chunk_size:(chunk + 1) * chunk_size], axis=1) / self.params.time_bin_size

            # go through NREM
            nr_chunks_nrem = int(raster_nrem.shape[1] / chunk_size)
            firing_nrem = np.zeros((raster_nrem.shape[0], nr_chunks_nrem))
            for chunk in range(nr_chunks_nrem):
                if measure == "mean":
                    firing_nrem[:, chunk] = np.mean(
                        raster_nrem[:, chunk * chunk_size:(chunk + 1) * chunk_size], axis=1) / self.params.time_bin_size
                elif measure == "max":
                    firing_nrem[:, chunk] = np.max(
                        raster_nrem[:, chunk * chunk_size:(chunk + 1) * chunk_size], axis=1) / self.params.time_bin_size

            # combine to z-score for each cell
            all_firing = np.hstack((firing_rem, firing_nrem))
            all_firing_z = zscore(all_firing, axis=1)

            # split again into rem and nrem
            firing_rem_z = all_firing_z[cell_ids, :firing_rem.shape[1]].flatten()
            firing_nrem_z = all_firing_z[cell_ids, firing_rem.shape[1]:].flatten()

            if plotting:
                p_rem = 1. * np.arange(firing_rem_z.shape[0]) / (firing_rem_z.shape[0] - 1)
                p_nrem = 1. * np.arange(firing_nrem_z.shape[0]) / (firing_nrem_z.shape[0] - 1)

                plt.plot(np.sort(firing_rem_z), p_rem, label="PRE")
                plt.plot(np.sort(firing_nrem_z), p_nrem, label="NREM")
                plt.title(cells_to_use)
                if measure == "mean":
                    plt.xlabel("Mean firing rate (z-scored)")
                elif measure == "max":
                    plt.xlabel("Max firing rate (z-scored)")
                plt.ylabel("CDF")
                plt.legend()
                plt.show()
            else:
                return firing_rem_z, firing_nrem_z

        else:
            raster_sleep = self.get_sleep_raster()
            # go through entire sleep and chunk data
            nr_chunks_sleep = int(raster_sleep.shape[1] / chunk_size)
            firing_sleep = np.zeros((raster_sleep.shape[0], nr_chunks_sleep))
            for chunk in range(nr_chunks_sleep):
                if measure == "mean":
                    firing_sleep[:, chunk] = np.mean(
                        raster_sleep[:, chunk * chunk_size:(chunk + 1) * chunk_size], axis=1) / self.params.time_bin_size

                elif measure == "max":
                    firing_sleep[:, chunk] = np.max(
                        raster_sleep[:, chunk * chunk_size:(chunk + 1) * chunk_size], axis=1) / self.params.time_bin_size

            firing_sleep_z = zscore(firing_sleep[cell_ids,:], axis=1).flatten()

            if plotting:

                p_sleep = 1. * np.arange(firing_sleep_z.shape[0]) / (firing_sleep_z.shape[0] - 1)

                plt.plot(np.sort(firing_sleep_z), p_sleep, label="sleep")
                plt.title(cells_to_use)
                if measure == "mean":
                    plt.xlabel("Mean firing rate (z-scored)")
                elif measure == "max":
                    plt.xlabel("Max firing rate (z-scored)")
                plt.ylabel("CDF")
                plt.legend()
                plt.show()
            else:
                return firing_sleep_z

    def save_spike_rasters(self, part_to_analyze, file_name):
        """
        save constant #spike rasters for either REM, SWR, or NREM

        :param part_to_analyze: which sleep epoch to return ("REM","NREM")
        :type part_to_analyze: str
        :param file_name: where to save
        :type file_name: str
        """
        all_rasters = []
        for l_s in self.long_sleep:
            event_spike_rasters, event_spike_window_lenghts=l_s.get_event_spike_rasters(part_to_analyze=part_to_analyze)
            all_rasters.extend(event_spike_rasters)
        with open(file_name, "wb") as fp:  # Pickling
            pickle.dump(all_rasters, fp)

    """#################################################################################################################
    #  memory drift
    #################################################################################################################"""

    # support functions
    # ------------------------------------------------------------------------------------------------------------------

    @staticmethod
    def compute_values_from_probabilities(pre_prob_list, post_prob_list, pre_prob_z_list, post_prob_z_list):
        # per event results
        event_pre_post_ratio = []
        event_pre_post_ratio_z = []
        event_pre_prob = []
        event_post_prob = []
        event_len_seq = []

        # per population vector results
        pop_vec_pre_post_ratio = []
        pre_seq_list = []
        pre_seq_list_z = []
        post_seq_list = []
        pre_seq_list_prob = []
        post_seq_list_prob = []
        pop_vec_post_prob = []
        pop_vec_pre_prob = []

        # go trough all events
        for pre_array, post_array, pre_array_z, post_array_z in zip(pre_prob_list, post_prob_list, pre_prob_z_list,
                                                                    post_prob_z_list):
            # make sure that there is any data for the current SWR
            if pre_array.shape[0] > 0:
                pre_sequence = np.argmax(pre_array, axis=1)
                pre_sequence_z = np.argmax(pre_array_z, axis=1)
                pre_sequence_prob = np.max(pre_array, axis=1)
                post_sequence = np.argmax(post_array, axis=1)
                post_sequence_prob = np.max(post_array, axis=1)
                pre_seq_list_z.extend(pre_sequence_z)
                pre_seq_list.extend(pre_sequence)
                post_seq_list.extend(post_sequence)
                pre_seq_list_prob.extend(pre_sequence_prob)
                post_seq_list_prob.extend(post_sequence_prob)

                # check how likely observed sequence is considering transitions from model (awake behavior)
                event_len_seq.append(pre_sequence.shape[0])

                # per SWR computations
                # ----------------------------------------------------------------------------------------------
                # arrays: [nr_pop_vecs_per_SWR, nr_time_spatial_time_bins]
                # get maximum value per population vector and take average across the SWR
                if pre_array.shape[0] > 0:
                    # save pre and post probabilities
                    event_pre_prob.append(np.mean(np.max(pre_array, axis=1)))
                    event_post_prob.append(np.mean(np.max(post_array, axis=1)))
                    # compute ratio by picking "winner" mode by first comparing z scored probabilities
                    # then the probability of the most over expressed mode (highest z-score) is used
                    pre_sequence_z = np.argmax(pre_array_z, axis=1)
                    prob_pre_z = np.mean(pre_array[:, pre_sequence_z])
                    post_sequence_z = np.argmax(post_array_z, axis=1)
                    prob_post_z = np.mean(post_array[:, post_sequence_z])
                    event_pre_post_ratio_z.append((prob_post_z - prob_pre_z) / (prob_post_z + prob_pre_z))

                    # compute ratio using probabilites
                    prob_pre = np.mean(np.max(pre_array, axis=1))
                    prob_post = np.mean(np.max(post_array, axis=1))
                    event_pre_post_ratio.append((prob_post - prob_pre) / (prob_post + prob_pre))
                else:
                    event_pre_prob.append(np.nan)
                    event_post_prob.append(np.nan)
                    event_pre_post_ratio.append(np.nan)

                # per population vector computations
                # ----------------------------------------------------------------------------------------------
                # compute per population vector similarity score
                prob_post = np.max(post_array, axis=1)
                prob_pre = np.max(pre_array, axis=1)
                pop_vec_pre_post_ratio.extend((prob_post - prob_pre) / (prob_post + prob_pre))

                if pre_array.shape[0] > 0:
                    pop_vec_pre_prob.extend(np.max(pre_array, axis=1))
                    pop_vec_post_prob.extend(np.max(post_array, axis=1))
                else:
                    pop_vec_pre_prob.extend([np.nan])
                    pop_vec_post_prob.extend([np.nan])

        pop_vec_pre_prob = np.array(pop_vec_pre_prob)
        pop_vec_post_prob = np.array(pop_vec_post_prob)
        pop_vec_pre_post_ratio = np.array(pop_vec_pre_post_ratio)
        pre_seq_list = np.array(pre_seq_list)

        return event_pre_post_ratio, event_pre_post_ratio_z, event_pre_prob, event_post_prob, event_len_seq,\
               pop_vec_pre_post_ratio, pre_seq_list, pre_seq_list_z, post_seq_list, pre_seq_list_prob, \
               post_seq_list_prob, pop_vec_post_prob, pop_vec_pre_prob

    def memory_drift_compute_results(self, template_type, pre_file_name=None, post_file_name=None, cells_to_use="all",
                                     shuffling=False, sleep_classification_method="std", speed_threshold=None):

        for i, l_s in enumerate(self.long_sleep):
            # compute all REM results
            l_s.decode_activity_using_pre_post(template_type=template_type, pre_file_name=pre_file_name,
                                               post_file_name=post_file_name, part_to_analyze="rem",
                                               cells_to_use=cells_to_use, return_results=False, shuffling=shuffling,
                                               speed_threshold=speed_threshold,
                                               sleep_classification_method=sleep_classification_method)
            # compute all NREM results
            l_s.decode_activity_using_pre_post(template_type=template_type, pre_file_name=pre_file_name,
                                               post_file_name=post_file_name, part_to_analyze="nrem",
                                               cells_to_use=cells_to_use, return_results=False, shuffling=shuffling,
                                               speed_threshold=speed_threshold,
                                               sleep_classification_method=sleep_classification_method)

    def memory_drift_long_sleep_get_results(self, template_type, part_to_analyze, pop_vec_threshold,
                                            measure="normalized_ratio", pre_file_name=None, post_file_name=None,
                                            cells_to_use="all", shuffling=False, sleep_classification_method="std"):

        first = 0
        pre_prob_list = []
        post_prob_list = []
        event_times_list = []
        for i, l_s in enumerate(self.long_sleep):
            if not i:
                default_pre_phmm_model, default_post_phmm_model,_ ,_ = l_s.get_pre_post_templates()
            duration = l_s.get_duration_sec()
            pre_prob, post_prob, ev_t, _ = l_s.decode_activity_using_pre_post(template_type=template_type,
                                                                              pre_file_name=pre_file_name,
                                                                              post_file_name=post_file_name,
                                                                              part_to_analyze=part_to_analyze,
                                                                              cells_to_use=cells_to_use,
                                                                              shuffling=shuffling,
                                                                              sleep_classification_method=
                                                                          sleep_classification_method)
            pre_prob_list.extend(pre_prob)
            post_prob_list.extend(post_prob)
            event_times_list.extend(ev_t + first)
            first += duration

        pre_prob_arr = np.vstack(pre_prob_list).astype(np.float32)
        post_prob_arr = np.vstack(post_prob_list).astype(np.float32)
        if measure == "normalized_ratio":
            result_all = (np.max(post_prob_arr, axis=1) - np.max(pre_prob_arr, axis=1)) / \
                        (np.max(pre_prob_arr, axis=1) + np.max(post_prob_arr, axis=1))
        elif measure == "log_like_ratio":
            result_all = np.log(np.max(post_prob_arr, axis=1)/np.max(pre_prob_arr, axis=1))
        elif measure == "model_evidence":
            if template_type == "phmm":
                # load pHMM models
                with open(self.params.pre_proc_dir + "phmm/" + default_pre_phmm_model + '.pkl', 'rb') as f:
                    model_pre_dic = pickle.load(f)
                stationary_dist_pre = model_pre_dic.get_stationary_distribution()
                with open(self.params.pre_proc_dir + "phmm/" + default_post_phmm_model + '.pkl', 'rb') as f:
                    model_post_dic = pickle.load(f)
                stationary_dist_post = model_post_dic.get_stationary_distribution()

                pre_prob = np.sum(pre_prob_arr * stationary_dist_pre, axis=1)
                post_prob = np.sum(post_prob_arr * stationary_dist_post, axis=1)
                result_all = np.log(post_prob/pre_prob)

        # assign array chunks to events again (either SWR or rem phases)
        length_per_event = [x.shape[0] for x in pre_prob_list]
        result_per_event = []
        first = 0
        for swr_id in range(len(length_per_event)):
            result_per_event.append(result_all[first:first + length_per_event[swr_id]])
            first += length_per_event[swr_id]

        # delete swr that are too short (< 2 population vectors)
        short_nrem_events_to_delete = np.where(np.array(length_per_event) < pop_vec_threshold)[0]
        for index in sorted(short_nrem_events_to_delete, reverse=True):
            del result_per_event[index]
            del length_per_event[index]
            del event_times_list[index]

        event_times = np.vstack(event_times_list)

        # compute length in seconds
        duration_event_in_s = event_times[:, 1] - event_times[:, 0]

        return result_per_event, event_times, length_per_event, duration_event_in_s, pre_prob_arr, post_prob_arr

    def memory_drift_long_sleep_get_raw_results(self, template_type, part_to_analyze, pop_vec_threshold=2,
                                                pre_file_name=None, post_file_name=None, cells_to_use="all"):

        first = 0
        pre_prob_list = []
        post_prob_list = []
        event_times_list = []
        for l_s in self.long_sleep:
            duration = l_s.get_duration_sec()
            pre_prob, post_prob, ev_t, _ = l_s.decode_activity_using_pre_post(template_type=template_type,
                                                                              pre_file_name=pre_file_name,
                                                                              post_file_name=post_file_name,
                                                                              part_to_analyze=part_to_analyze,
                                                                              cells_to_use=cells_to_use)
            pre_prob_list.extend(pre_prob)
            post_prob_list.extend(post_prob)
            event_times_list.extend(ev_t + first)
            first += duration

        length_per_event = [x.shape[0] for x in pre_prob_list]
        # delete events that are too short (< 2 population vectors)
        short_nrem_events_to_delete = np.where(np.array(length_per_event) < pop_vec_threshold)[0]
        for index in sorted(short_nrem_events_to_delete, reverse=True):
            del pre_prob_list[index]
            del post_prob_list[index]
            del event_times_list[index]

        return pre_prob_list, post_prob_list, event_times_list

    def memory_drift_long_sleep_save_raw_data(self, part_to_analyze, pre_or_post, file_name):
        pre_prob_list, post_prob_list, event_times_list = self.memory_drift_long_sleep_get_raw_results(
                                                               template_type="phmm",
                                                               part_to_analyze=part_to_analyze)
        if pre_or_post == "post":
            with open(file_name, 'wb') as f:
                pickle.dump(post_prob_list, f)

    # decoding analysis (Ising or pHMM)
    # ------------------------------------------------------------------------------------------------------------------

    def memory_drift(self, template_type, measure="normalized_ratio", pre_file_name=None, post_file_name=None,
                     n_moving_average_pop_vec=200, rem_pop_vec_threshold=10, nrem_pop_vec_threshold=2,
                     plotting=False, cells_to_use="all", sleep_classification_method="std", shuffling=False):
        """
        detailed memory drift analysis: opposing effects of NREM/REM etc.

        :param template_type: which template to use ("phmm" or "ising")
        :type template_type: str
        :param measure: which measure to use to compare PRE and POST similarity (model_evidence, log_like_ratio,
                        normalized_ratio
        :type measure: str
        :param pre_file_name: name of PRE template, if None --> default is used
        :type pre_file_name: str
        :param post_file_name: name of POST template, if None --> default is used
        :type post_file_name: str_decoding_similarity_temporal
        :param n_moving_average_pop_vec: how much smoothing to apply across population vectors
        :type n_moving_average_pop_vec: int
        :param rem_pop_vec_threshold: minimum length of REM epochs (shorter ones are discarded)
        :type rem_pop_vec_threshold: int
        :param plotting: whether to plot
        :type plotting: bool
        :param cells_to_use: which cells to use ("all", "stable", "inc", "dec")
        :type cells_to_use: str
        :return: ds_rem_sum, ds_nrem_sum, ratio_per_merged_nrem_event, ratio_per_merged_rem_event
        :rtype:
        """
        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_rem_event, event_times_rem, event_lengths_rem, event_duration_rem_in_s, _, _ = \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold, measure=measure,
                                                     cells_to_use=cells_to_use, sleep_classification_method=
                                                     sleep_classification_method, shuffling=shuffling)

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_nrem_event, event_times_nrem, event_lengths_nrem, event_duration_nrem_in_s, _ ,_ = \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="nrem",
                                                     pop_vec_threshold=nrem_pop_vec_threshold, measure=measure,
                                                     cells_to_use=cells_to_use, sleep_classification_method=
                                                     sleep_classification_method, shuffling=shuffling)

        # combine rem and nrem data to order it in the right way (chronologically)
        # --------------------------------------------------------------------------------------_decoding_similarity_temporal------------------------

        all_events_ratio = ratio_per_rem_event + ratio_per_nrem_event
        all_events_length = event_lengths_rem + event_lengths_nrem
        all_events_length_s = np.hstack((event_duration_rem_in_s, event_duration_nrem_in_s))
        labels_events = np.zeros(len(ratio_per_rem_event)+len(ratio_per_nrem_event))
        labels_events[:len(ratio_per_rem_event)] = 1
        all_times = np.hstack((event_times_rem[:, 0], event_times_nrem[:,0]))
        all_end_times = np.hstack((event_times_rem[:, 1], event_times_nrem[:, 2]))

        # sort events according to time
        sorted_events_ratio_list = [x for _, x in sorted(zip(all_times, all_events_ratio))]
        sorted_labels_events = [x for _, x in sorted(zip(all_times, labels_events))]
        sorted_lengths = [x for _, x in sorted(zip(all_times, all_events_length))]
        sorted_lengths_s = [x for _, x in sorted(zip(all_times, all_events_length_s))]
        sorted_pop_vec_ratio = np.hstack(sorted_events_ratio_list)
        sorted_times = [x for x in sorted(all_times)]
        sorted_end_times = [x for _, x in sorted(zip(all_times, all_end_times))]

        # compute labels per population vector --> rem events = 1, nrem events = 0
        # --------------------------------------------------------------------------------------------------------------
        labels_per_pop_vec = []
        for len_event, label in zip(sorted_lengths, sorted_labels_events):
            if label:
                labels_per_pop_vec.extend(np.ones(len_event))
            elif label == 0:
                labels_per_pop_vec.extend(np.zeros(len_event))

        labels_per_pop_vec = np.array(labels_per_pop_vec)

        # concatenate events that have the same label
        # --------------------------------------------------------------------------------------------------------------
        sorted_labels_events = np.array(sorted_labels_events)
        label_change = np.diff(sorted_labels_events)
        merged_events_length_s = []
        merged_events_labels = []
        merged_events_times = []
        first = 0
        while True:
            dat = label_change[first:]
            if first > label_change.shape[0]:
                break
            if sorted_labels_events[first] == 0:
                # nrem event
                # check if this is the last event
                if np.where(dat==1)[0].shape[0] == 0:
                    trans = label_change.shape[0] - first + 1
                else:
                    trans = np.where(dat == 1)[0][0]+1
                merged_events_length_s.append(np.sum(sorted_lengths_s[first:first+trans]))
                merged_events_labels.append(np.unique(sorted_labels_events[first:first + trans]))
                merged_events_times.append([sorted_times[first], sorted_end_times[first + trans - 1]])
            elif sorted_labels_events[first] == 1:
                # rem event
                # check if this is the last event
                if np.where(dat==-1)[0].shape[0] == 0:
                    trans = label_change.shape[0] - first + 1
                else:
                    trans = np.where(dat == -1)[0][0]+1
                merged_events_length_s.append(np.sum(sorted_lengths_s[first:first+trans]))
                merged_events_labels.append(np.unique(sorted_labels_events[first:first + trans]))
                merged_events_times.append([sorted_times[first], sorted_end_times[first+trans-1]])
            first += trans

        merged_events_labels = np.squeeze(np.array(merged_events_labels))
        merged_events_length_s = np.array(merged_events_length_s)
        merged_events_times = np.vstack(merged_events_times)

        # merged_rem_event_times = merged_events_times[merged_events_labels == 1]
        # merged_nrem_event_times = merged_events_times[merged_events_labels == 0]
        merged_events_length_s = merged_events_times[:,1]-merged_events_times[:,0]


        # concatenate population vectors that have the same label: TODO: there might be a better way to combine this
        # step and the previous step
        # --------------------------------------------------------------------------------------------------------------
        new_event_time_stamps = np.abs(np.diff(labels_per_pop_vec))

        # need to add one element at the beginning
        new_event_time_stamps = np.insert(new_event_time_stamps, 0, 1)
        start = np.nonzero(new_event_time_stamps==1)[0]
        end = start[1:]
        end = np.append(end, new_event_time_stamps.shape[0])

        # only select data (pre_post ratio) from population vectors that belong to selected rem/nrem events
        # --> results per merged event
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_merged_rem_event = []
        ratio_per_merged_nrem_event = []
        ratio_rem_nrem_events = []
        rem_nrem_events_label = []
        ratio_rem_nrem_pop_vec = []
        rem_nrem_pop_vec_label = []

        for start_event, end_event in zip(start, end):
            # rem event
            if labels_per_pop_vec[start_event + 1] == 1:
                ratio_per_merged_rem_event.append(sorted_pop_vec_ratio[start_event:end_event])
            # nrem event
            else:
                ratio_per_merged_nrem_event.append(sorted_pop_vec_ratio[start_event:end_event])

            ratio_rem_nrem_events.append(sorted_pop_vec_ratio[start_event:end_event])
            rem_nrem_events_label.extend([labels_per_pop_vec[start_event + 1]])

            ratio_rem_nrem_pop_vec.extend(sorted_pop_vec_ratio[start_event:end_event])
            rem_nrem_pop_vec_label.extend(labels_per_pop_vec[start_event:end_event])


        # smooth across population vectors --> that means also across REM/NREM epochs
        # --------------------------------------------------------------------------------------------------------------
        len_new_events = [x.shape[0] for x in ratio_rem_nrem_events]
        ratio_per_pop_vec_new = np.hstack(ratio_rem_nrem_events)
        ratio_per_pop_vec_new_smooth = moving_average(a=np.array(ratio_per_pop_vec_new), n=n_moving_average_pop_vec)

        ratio_rem_nrem_events_smooth = []
        first = 0
        for event_id in range(len(len_new_events)):
            # check if data is still available (due to smoothing --> data gets shorter)
            # if not --> leave loop
            if first >= ratio_per_pop_vec_new_smooth.shape[0]:
                break
            end_event = min(first + len_new_events[event_id], ratio_per_pop_vec_new_smooth.shape[0])
            ratio_rem_nrem_events_smooth.append(ratio_per_pop_vec_new_smooth[first:end_event])
            first += len_new_events[event_id]

        # need to delete indices of events that happend after the last data point after smoothing
        rem_nrem_events_label = rem_nrem_events_label[:len(ratio_rem_nrem_events_smooth)]

        # get rem data of merged events after smoothing
        # --------------------------------------------------------------------------------------------------------------
        rem_events_indices = np.squeeze(np.argwhere(np.array(rem_nrem_events_label) == 1))
        ds_rem = []
        ds_rem_smoothed_within = []
        merged_events_rem_length = []
        ratio_rem_events_smooth = []
        for rem_index in rem_events_indices:
            ds_rem.append(ratio_rem_nrem_events_smooth[rem_index][-1]-ratio_rem_nrem_events_smooth[rem_index][0])
            smooth_event = moving_average(a=ratio_rem_nrem_events[rem_index], n=10)
            ds_rem_smoothed_within.append(smooth_event[-1]-smooth_event[0])
            merged_events_rem_length.append(ratio_rem_nrem_events_smooth[rem_index].shape[0])
            ratio_rem_events_smooth.append(ratio_rem_nrem_events_smooth[rem_index])

        # get nrem data of merged events after smoothing
        # --------------------------------------------------------------------------------------------------------------
        nrem_events_indices = np.squeeze(np.argwhere(np.array(rem_nrem_events_label) == 0))
        ds_nrem = []
        ds_nrem_smoothed_within = []
        merged_events_nrem_length = []
        ratio_nrem_events_smooth = []
        for nrem_index in nrem_events_indices:
            ds_nrem.append(ratio_rem_nrem_events_smooth[nrem_index][-1]-ratio_rem_nrem_events_smooth[nrem_index][0])
            merged_events_nrem_length.append(ratio_rem_nrem_events_smooth[nrem_index].shape[0])
            ratio_nrem_events_smooth.append(ratio_rem_nrem_events_smooth[nrem_index])

        # compute delta score sum & cum sum
        # --------------------------------------------------------------------------------------------------------------
        ds_nrem_sum_smoothed_within = np.sum(np.array(ds_nrem_smoothed_within))
        ds_rem_sum_smoothed_within = np.sum(np.array(ds_rem_smoothed_within))
        ds_nrem_sum = np.sum(np.array(ds_nrem))
        ds_rem_sum = np.sum(np.array(ds_rem))
        ds_nrem_cum = np.cumsum(np.array(ds_nrem))
        ds_rem_cum = np.cumsum(np.array(ds_rem))

        # compute cross correlation of delta scores NREM/REM
        # --------------------------------------------------------------------------------------------------------------
        min_len = min(len(ds_rem), len(ds_nrem))
        ds_rem_arr = np.array(ds_rem)[:min_len]
        ds_nrem_arr = np.array(ds_nrem)[:min_len]
        corr_list_pos = []
        corr_list_neg = []
        shift_array_cross_corr = [0, 1, 2, 3, 4, 5, 6]
        for shift in shift_array_cross_corr:
            corr_list_pos.append(
                np.round(pearsonr(ds_rem_arr[shift:], ds_nrem_arr[:ds_nrem_arr.shape[0] - shift])[0], 2))
            corr_list_neg.append(
                np.round(pearsonr(ds_nrem_arr[shift:], ds_rem_arr[:ds_rem_arr.shape[0] - shift])[0], 2))
        corr_list = np.hstack((np.flip(np.array(corr_list_neg)), np.array(corr_list_pos)))

        # get mean score per event
        # --------------------------------------------------------------------------------------------------------------
        mean_nrem = []
        for i, event in enumerate(ratio_per_merged_nrem_event):
            mean_nrem.append(np.mean(event))
        mean_rem = []
        for i, event in enumerate(ratio_per_merged_rem_event):
            mean_rem.append(np.mean(event))

        # get data smoothed only across REM events and only across NREM events
        # --------------------------------------------------------------------------------------------------------------
        ratio_merged_nrem_events_arr = np.hstack(ratio_per_merged_nrem_event)
        ratio_merged_nrem_events_arr_smooth = moving_average(a=ratio_merged_nrem_events_arr, n=n_moving_average_pop_vec)
        ratio_merged_rem_events_arr = np.hstack(ratio_per_merged_rem_event)
        ratio_merged_rem_events_arr_smooth = moving_average(a=ratio_merged_rem_events_arr, n=n_moving_average_pop_vec)

        # compute oscillations of REM/NREM periods
        # --------------------------------------------------------------------------------------------------------------
        # moving window
        window_size = 100
        step_size = 100

        window_start = np.arange(0, ratio_merged_rem_events_arr_smooth.shape[0] - window_size, step_size)
        min_max_rem = []
        std_rem = []
        for w_s in window_start:
            min_max_rem.append(max(ratio_merged_rem_events_arr_smooth[w_s:w_s + step_size])
                               - min(ratio_merged_rem_events_arr_smooth[w_s:w_s + step_size]))
            std_rem.append(np.std(ratio_merged_rem_events_arr_smooth[w_s:w_s + step_size]))
        window_start = np.arange(0, ratio_merged_nrem_events_arr_smooth.shape[0] - window_size, step_size)
        min_max_nrem = []
        std_nrem = []
        for w_s in window_start:
            min_max_nrem.append(max(ratio_merged_nrem_events_arr_smooth[w_s:w_s + step_size])
                                - min(ratio_merged_nrem_events_arr_smooth[w_s:w_s + step_size]))
            std_nrem.append(np.std(ratio_merged_nrem_events_arr_smooth[w_s:w_s + step_size]))

        if plotting:

            # plotting
            fig = plt.figure()
            ax = fig.add_subplot()
            start = 0
            for event, label in zip(ratio_rem_nrem_events_smooth, rem_nrem_events_label):
                event_length = event.shape[0]
                if label:
                    ax.plot(np.arange(start, start + event_length), event, c="r", label="REM", linewidth=0.4)
                else:
                    ax.plot(np.arange(start, start + event_length), event, c="b", label="NREM")
                start += event_length
            handles, labels = ax.get_legend_handles_labels()
            by_label = OrderedDict(zip(labels, handles))
            ax.legend(by_label.values(), by_label.keys())
            plt.grid()
            plt.title("SMOOTHING: n="+str(n_moving_average_pop_vec))
            plt.xlabel("POPULATION VECTOR ID")
            if measure == "normalized_ratio":
                plt.ylim(-1, 1)
                plt.ylabel("PRE_POST RATIO")
            elif measure == "log_like_ratio":
                plt.ylabel("LOG-LIKELIHOOD RATIO")
                y_min, y_max = ax.get_ylim()
                plt.ylim(y_min, -1*y_min)

            elif measure == "model_evidence":
                plt.ylabel("MODEL EVIDENCE")
                y_min, y_max = ax.get_ylim()
                plt.ylim(y_min, -1*y_min)

            plt.show()
            plt.plot(ratio_merged_rem_events_arr_smooth, c="r", label="REM")
            plt.plot(ratio_merged_nrem_events_arr_smooth, c="b", label="NREM", alpha=0.8)
            plt.legend()
            plt.xlabel("POP.VEC.ID")
            plt.ylabel("PRE_POST_SCORE")
            plt.show()

            plt.hist(min_max_rem, color="r", density=True, label="REM")
            plt.hist(min_max_nrem, color="b", density=True, alpha=0.6, label="NREM")
            plt.legend()
            plt.xlabel("MAX-MIN IN SLIDING WINDOW OF "+str(window_size)+" POP.VEC.")
            plt.ylabel("DENSITY")
            plt.show()

            plt.hist(std_rem, color="r", density=True, label="REM")
            plt.hist(std_nrem, color="b", density=True, alpha=0.6, label="NREM")
            plt.legend()
            plt.xlabel("STD IN SLIDING WINDOW OF "+str(window_size)+" POP.VEC.")
            plt.ylabel("DENSITY")
            plt.show()

            def make_square_axes(ax):
                """Make an axes square in screen units.

                Should be called after plotting.
                """
                ax.set_aspect(1 / ax.get_data_ratio())

            plt.scatter(ds_rem_arr, ds_nrem_arr)
            plt.title("NEIGHBOURING PERIODS, R="+str(np.round(pearsonr(ds_rem_arr, ds_nrem_arr)[0], 2)))
            plt.xlabel("DELTA SCORE REM")
            plt.ylabel("DELTA SCORE NREM")
            make_square_axes(plt.gca())
            plt.show()

            x_axis_cross_corr = -1 * np.flip(np.array(shift_array_cross_corr)+1)
            x_axis_cross_corr = np.hstack((x_axis_cross_corr, np.array(shift_array_cross_corr)+1))
            plt.plot(x_axis_cross_corr, corr_list, marker=".")
            plt.xlabel("REM OFFSET (+X MEANS REM BEHIND NREM BY X)")
            plt.ylabel("CORRELATION DELTA SCORE REM VS. NREM")
            plt.title("CROSS CORRELATION OF DELTA SCORES REM VS. NREM")
            plt.show()
            #
            # plt.plot(ds_rem, color="gray", linewidth=0.8)
            # plt.scatter(range(len(ds_rem)), ds_rem, c=merged_events_length_s[merged_events_labels == 1], cmap=cm.Reds)
            # a = plt.colorbar()
            # a.set_label("DURATION / s")
            # plt.hlines(0, 0, len(ds_rem), color="w", linewidth=0.5, zorder=-1000)
            # plt.ylabel("DELTA SCORE")
            # plt.xlabel("EVENT ID")
            # plt.title("REM: PER EVENT DELTA SCORE & DURATION")
            # plt.show()
            #
            # plt.plot(ds_nrem, color="gray", linewidth=0.8)
            # plt.scatter(range(len(ds_nrem)), ds_nrem, c=merged_events_length_s[merged_events_labels == 0], cmap=cm.Reds)
            # a = plt.colorbar()
            # a.set_label("DURATION / s")
            # plt.hlines(0, 0, len(ds_nrem), color="w", linewidth=0.5, zorder=-1000)
            # plt.ylabel("DELTA SCORE")
            # plt.xlabel("EVENT ID")
            # plt.title("NREM: PER EVENT DELTA SCORE & DURATION")
            # plt.show()

            # plt.plot(merged_events_length_s[merged_events_labels == 1], color="gray", linewidth=0.8)
            # plt.scatter(range(len(merged_events_length_s[merged_events_labels == 1])),
            #             merged_events_length_s[merged_events_labels == 1], c=ds_rem, cmap=cm.coolwarm)
            # a = plt.colorbar()
            # a.set_label("DELTA SCORE")
            # plt.ylabel("DURATION EVENT")
            # plt.xlabel("EVENT ID")
            # plt.title("REM: PER EVENT DELTA SCORE & DURATION")
            # plt.show()
            #
            # plt.plot(merged_events_length_s[merged_events_labels == 0], color="gray", linewidth=0.8)
            # plt.scatter(range(len(merged_events_length_s[merged_events_labels == 0])),
            #             merged_events_length_s[merged_events_labels == 0], c=ds_nrem, cmap=cm.coolwarm)
            # a = plt.colorbar()
            # a.set_label("DELTA SCORE")
            # plt.ylabel("DURATION EVENT")
            # plt.xlabel("EVENT ID")
            # plt.title("NREM: PER EVENT DELTA SCORE & DURATION")
            # plt.show()

            # check which phase came first
            if rem_nrem_events_label[0] == 1:
                # first event was a rem event
                rem_x_axis = np.arange(0,2*len(ds_rem), 2)
                nrem_x_axis = np.arange(1, 2 * len(ds_nrem) + 1, 2)
            elif rem_nrem_events_label[0] == 0:
                # first event was a nrem event
                nrem_x_axis = np.arange(0,2*len(ds_nrem), 2)
                rem_x_axis = np.arange(1, 2 * len(ds_rem) + 1, 2)

            plt.plot(rem_x_axis, ds_rem, marker="." ,label="REM", color="r")
            plt.plot(nrem_x_axis, ds_nrem, marker=".", label="NREM", color="b")
            plt.legend()
            plt.hlines(0, 0, rem_x_axis.shape[0]+nrem_x_axis.shape[0], color="gray")
            plt.ylabel("DELTA SCORE")
            plt.xlabel("EVENT ID")
            plt.title("PER EVENT DELTA SCORE")
            plt.show()

            # plt.figure(figsize=(11, 6))
            # plt.subplot(1, 2, 1)
            # plt.plot(rem_x_axis, merged_events_rem_length, marker="." ,label="REM", color="r")
            # plt.plot(nrem_x_axis, merged_events_nrem_length, marker=".", label="NREM", color="b")
            # plt.legend()
            # plt.ylabel("#POPULATION VECTORS")
            # plt.xlabel("EVENT ID")
            # plt.title("#POPULATION VECTORS PER EVENT")
            # plt.subplot(1, 2, 2)
            # plt.plot(rem_x_axis, merged_events_length_s[merged_events_labels == 1], marker="." ,label="REM", color="r")
            # plt.plot(nrem_x_axis, merged_events_length_s[merged_events_labels == 0], marker=".", label="NREM", color="b")
            # plt.ylabel("DURATION / s")
            # plt.xlabel("EVENT ID")
            # plt.title("DURATION OF EACH EVENT")
            # plt.legend()
            # plt.show()


            plt.hist(merged_events_length_s[merged_events_labels == 1], bins=10, label="REM", density=True, color="r")
            plt.hist(merged_events_length_s[merged_events_labels == 0], bins=10, label="NREM", density=True, color="b")
            plt.title("EVENT DURATION (REM.POP.VEC.THRS.="+str(rem_pop_vec_threshold)+")")
            plt.xlabel("DURATION OF EVENT / s")
            plt.ylabel("DENSITY")
            plt.legend()
            plt.show()


            plt.figure(figsize=(11, 6))
            plt.subplot(1, 2, 1)
            plt.scatter(merged_events_length_s[rem_events_indices], ds_rem, color="r")
            plt.hlines(0, 0, plt.gca().get_xlim()[1], color="gray", linewidth=0.5)
            plt.ylabel("DELTA SCORE")
            plt.xlabel("DURATION / s")
            plt.title("REM PER EVENT (POP.THR.REM.="+str(rem_pop_vec_threshold)+")")
            make_square_axes(plt.gca())
            plt.subplot(1, 2, 2)
            plt.scatter(merged_events_length_s[nrem_events_indices], ds_nrem, color="b")
            plt.xlabel("DURATION / s")
            plt.hlines(0, 0, plt.gca().get_xlim()[1], color="gray", linewidth=0.5)
            plt.title("NREM PER EVENT")
            make_square_axes(plt.gca())
            plt.show()

            plt.scatter(merged_events_length_s[rem_events_indices], ds_rem, color="r", label="REM")
            plt.scatter(merged_events_length_s[nrem_events_indices], ds_nrem, color="b", label="NREM")
            plt.hlines(0, 0, plt.gca().get_xlim()[1], color="gray", linewidth=0.5)
            plt.ylabel("DELTA SCORE")
            plt.xlabel("DURATION / s")
            plt.title("DURATION - DELTA SCORE (POP.THR.REM.=" + str(rem_pop_vec_threshold) + ")")
            make_square_axes(plt.gca())
            plt.legend()
            plt.show()


            plt.figure(figsize=(11, 6))
            plt.subplot(1, 2, 1)
            plt.scatter(merged_events_rem_length, ds_rem, color="r")
            plt.hlines(0, 0, plt.gca().get_xlim()[1], color="gray", linewidth=0.5)
            plt.ylabel("DELTA SCORE")
            plt.xlabel("#POP.VEC.")
            plt.title("REM PER EVENT (POP.THR.REM.="+str(rem_pop_vec_threshold)+")")
            make_square_axes(plt.gca())
            plt.subplot(1, 2, 2)
            plt.scatter(merged_events_nrem_length, ds_nrem, color="b")
            plt.xlabel("#POP.VEC.")
            plt.hlines(0, 0, plt.gca().get_xlim()[1], color="gray", linewidth=0.5)
            plt.title("NREM PER EVENT")
            make_square_axes(plt.gca())
            plt.show()

            plt.title("SMOOTHING n=" + str(n_moving_average_pop_vec) + ", POP.THR.REM=" + str(rem_pop_vec_threshold))
            plt.plot(mean_nrem, marker=".", color="b", label="NREM")
            plt.plot(mean_rem, marker=".", color="r", label="REM")
            plt.xlabel("EVENT ID")
            plt.ylabel("MEAN SCORE PER EVENT")
            plt.legend()
            plt.show()
            common_nr = min(len(mean_nrem), len(mean_rem))
            diff_in_mean = np.abs(np.array(mean_nrem[:common_nr]) - np.array(mean_rem[:common_nr]))
            plt.title("DIFF REM NREM, SMOOTHING n=" + str(n_moving_average_pop_vec) + ", POP.THR.REM=" + str(rem_pop_vec_threshold))
            plt.plot(diff_in_mean, marker=".", color="w")
            plt.xlabel("EVENT ID")
            plt.ylabel("ABS. DIFF. BETWEEN MEAN SCORE PER EVENT")
            plt.show()

            plt.scatter(1, ds_nrem_sum, color="b", label="NREM", zorder=1000)
            # plt.scatter(1, ds_nrem_cum_smoothed_within, color="lightskyblue", label="NREM - WITHIN", zorder=1000)
            plt.scatter(1, ds_rem_sum, color="r", label="REM", zorder=1000)
            # plt.scatter(1, ds_rem_cum_smoothed_within, color="lightcoral", label="REM - WITHIN", zorder=1000)
            plt.legend()
            plt.ylabel("CUMULATIVE DELTA SCORE")
            plt.ylim(-(max(abs(ds_nrem_sum_smoothed_within), abs(ds_rem_sum_smoothed_within), abs(ds_rem_sum), abs(ds_nrem_sum))+1),
                     (max(abs(ds_nrem_sum_smoothed_within), abs(ds_rem_sum_smoothed_within), abs(ds_rem_sum), abs(ds_nrem_sum))+1))
            plt.title("SMOOTHING: n="+str(n_moving_average_pop_vec))
            plt.grid()
            plt.show()

            plt.title("SMOOTHING n=" + str(n_moving_average_pop_vec) + ", POP.THR.REM=" + str(rem_pop_vec_threshold))
            plt.plot(ds_nrem_cum, color="b", label="NREM")
            plt.plot(ds_rem_cum, color="r", label="REM")
            plt.grid()
            plt.xlabel("EVENT ID")
            plt.legend()
            plt.ylabel("CUM SUM")
            plt.show()

            y_rem,_,_ =plt.hist(ds_rem, color="r", label="REM", density=True)
            y_nrem,_,_ = plt.hist(ds_nrem, color="b", alpha=0.5, label="NREM", density=True)
            plt.vlines(np.mean(ds_nrem),0,y_nrem.max(), color="lightskyblue", label="MEAN NREM")
            plt.vlines(np.mean(ds_rem), 0, y_rem.max(), color="lightcoral", label="MEAN REM")
            plt.xlabel("DELTA SCORE")
            plt.ylabel("DENSITY")
            plt.title("SMOOTHING: n=" + str(n_moving_average_pop_vec))
            plt.legend()
            plt.show()

            # event shape

            plt.figure(figsize=(12, 6))
            plt.subplot(1, 2, 1)

            x_len = len(ratio_rem_events_smooth)
            y_len = len(max(ratio_rem_events_smooth, key=lambda x: len(x)))

            b = np.empty((x_len, y_len))
            b[:] = np.nan
            for i, j in enumerate(ratio_rem_events_smooth):
                b[i][0:len(j)] = j

            # subtract means to center
            b = b - np.nanmean(b, axis=1, keepdims=True)
            # scale values to lie between -1 and 1
            b = b / np.nanmax(np.abs(b), axis=1, keepdims= True)

            mean_shape = np.nanmean(b, axis=0)
            for shape in b:
                plt.plot(shape, c="gray")
            plt.plot(mean_shape, c="r")
            plt.ylabel("PRE_POST SCORE - SINGLE EVENTS + MEAN")
            plt.title("REM")
            plt.xlabel("POP.VEC.ID. FROM EVENT ONSET")

            plt.subplot(1, 2, 2)
            x_len = len(ratio_nrem_events_smooth)
            y_len = len(max(ratio_nrem_events_smooth, key=lambda x: len(x)))

            b = np.empty((x_len, y_len))
            b[:] = np.nan
            for i, j in enumerate(ratio_nrem_events_smooth):
                b[i][0:len(j)] = j

            # subtract means to center
            b = b - np.nanmean(b, axis=1, keepdims=True)
            # scale values to lie between -1 and 1
            b = b / np.nanmax(np.abs(b), axis=1, keepdims= True)

            mean_shape = np.nanmean(b, axis=0)
            for shape in b:
                plt.plot(shape, c="gray")
            plt.plot(mean_shape, c="b")
            plt.title("NREM")
            plt.xlabel("POP.VEC.ID. FROM EVENT ONSET")
            plt.show()

        else:
            return ds_rem_sum, ds_nrem_sum, ratio_per_merged_nrem_event, ratio_per_merged_rem_event

    def memory_drift_entire_sleep(self, random_seed=1, shuffling=False, n_smoothing=400, rem_pop_vec_threshold=10,
                                  nrem_pop_vec_threshold=2, plotting=True, swapping_factor=10):
        """
        instead of getting results from single shorter sleep files, this function computes results for the concatenated
        data --> needed for shuffling
        """

        # get model names
        # --------------------------------------------------------------------------------------------------------------
        pre_file_name = self.session_params.default_pre_phmm_model
        post_file_name = self.session_params.default_post_phmm_model

        # compute constant nr spike bins
        # --------------------------------------------------------------------------------------------------------------
        event_spike_raster_rem = []
        event_spike_window_lengths_rem = []
        start_times_rem = []
        end_times_rem = []
        event_spike_raster_nrem = []
        event_spike_window_lengths_nrem = []
        start_times_nrem = []
        end_times_nrem = []

        start_sleep_chunk = 0
        for i, l_s in enumerate(self.long_sleep):
            duration = l_s.get_duration_sec()
            esr_rem, eswl_rem, st_rem, et_rem = l_s.get_event_spike_rasters(part_to_analyze="rem",
                                                                             return_event_times=True)
            event_spike_raster_rem.extend(esr_rem)
            event_spike_window_lengths_rem.extend(eswl_rem)
            start_times_rem.extend(st_rem+start_sleep_chunk)
            end_times_rem.extend(et_rem+start_sleep_chunk)

            esr_nrem, eswl_nrem, st_nrem, et_nrem = l_s.get_event_spike_rasters(part_to_analyze="nrem",
                                                                             return_event_times=True)
            event_spike_raster_nrem.extend(esr_nrem)
            event_spike_window_lengths_nrem.extend(eswl_nrem)
            start_times_nrem.extend(st_nrem+start_sleep_chunk)
            end_times_nrem.extend(et_nrem+start_sleep_chunk)
            start_sleep_chunk += duration

        duration_sleep_h = start_sleep_chunk/60/60

        # filter nrem/rem events that are too short!
        # --------------------------------------------------------------------------------------------------------------

        # assign array chunks to events again (either SWR or rem phases)
        rem_event_length = [x.shape[0] for x in event_spike_raster_rem]
        nrem_event_length = [x.shape[0] for x in event_spike_raster_nrem]

        # delete nrem events that are too short
        short_nrem_events_to_delete = np.where(np.array(nrem_event_length) < nrem_pop_vec_threshold)[0]
        for index in sorted(short_nrem_events_to_delete, reverse=True):
            del event_spike_raster_nrem[index]
            del start_times_nrem[index]

        # delete rem events that are too short (< 2 population vectors)
        short_rem_events_to_delete = np.where(np.array(rem_event_length) < rem_pop_vec_threshold)[0]
        for index in sorted(short_rem_events_to_delete, reverse=True):
            del event_spike_raster_rem[index]
            del start_times_rem[index]

        # sort according to time
        # --------------------------------------------------------------------------------------------------------------
        all_spike_rasters = event_spike_raster_nrem + event_spike_raster_rem
        all_times = np.array(start_times_nrem + start_times_rem)

        # sort all events according to start time
        # --------------------------------------------------------------------------------------------------------------
        new_order = np.argsort(all_times)
        all_spike_rasters_ordered = []
        for id in new_order:
            all_spike_rasters_ordered.append(all_spike_rasters[id])

        with open(self.params.pre_proc_dir + "phmm/" + pre_file_name + '.pkl', 'rb') as f:
            model_dic_pre = pickle.load(f)
            # get means of model (lambdas) for decoding
        mode_means_pre = model_dic_pre.means_

        time_bin_size_encoding = model_dic_pre.time_bin_size

        with open(self.params.pre_proc_dir + "phmm/" + post_file_name + '.pkl', 'rb') as f:
            model_dic_post = pickle.load(f)
            # get means of model (lambdas) for decoding
        mode_means_post = model_dic_post.means_

        # check if const. #spike bins are correct for the loaded compression factor
        if not self.params.spikes_per_bin == 12:
            raise Exception("TRYING TO LOAD COMPRESSION FACTOR FOR 12 SPIKES PER BIN, "
                            "BUT CURRENT #SPIKES PER BIN != 12")

        # load correct compression factor (as defined in parameter file of the session)
        if time_bin_size_encoding == 0.01:
            compression_factor = np.round(self.session_params.sleep_compression_factor_12spikes_100ms * 10, 3)
        elif time_bin_size_encoding == 0.1:
            compression_factor = self.session_params.sleep_compression_factor_12spikes_100ms
        else:
            raise Exception("COMPRESSION FACTOR NEITHER PROVIDED NOR FOUND IN PARAMETER FILE")

        cell_selection = "all"
        cells_ids = np.empty(0)

        # shuffling
        # --------------------------------------------------------------------------------------------------------------
        if shuffling:
            print("\nStarted shuffling procedure (random seed:"+str(random_seed)+") ... ")
            np.random.seed(random_seed)
            conc_data = np.hstack(all_spike_rasters_ordered)
            nr_swaps = conc_data.shape[1]*swapping_factor
            for shuffle_id in range(nr_swaps):
                # select two random time bins
                t1 = 1
                t2 = 1
                while (t1 == t2):
                    t1 = np.random.randint(conc_data.shape[1])
                    t2 = np.random.randint(conc_data.shape[1])
                # check in both time bins which cells are active
                act_cells_t1 = np.argwhere(conc_data[:, t1].flatten() > 0).flatten()
                act_cells_t2 = np.argwhere(conc_data[:, t2].flatten() > 0).flatten()
                # find intersect (same cells need to be firing in t1 and t2 in order to exchange spikes)
                # original code
                # ------------------------------------------------------------------------------------------------------
                # cells_firing_in_both = np.intersect1d(act_cells_t1, act_cells_t2)
                # if cells_firing_in_both.shape[0] > 1:
                #     # select first cell to swap
                #     cell_1 = 1
                #     cell_2 = 1
                #     while (cell_1 == cell_2):
                #         cell_1 = np.random.choice(cells_firing_in_both)
                #         cell_2 = np.random.choice(cells_firing_in_both)
                #     # do the actual swapping
                #     conc_data[cell_1, t1] += 1
                #     conc_data[cell_1, t2] -= 1
                #     conc_data[cell_2, t1] -= 1
                #     conc_data[cell_2, t2] += 1

                # modified code
                # ------------------------------------------------------------------------------------------------------
                if act_cells_t1.shape[0] > 1 and act_cells_t2.shape[0] > 1:
                    # select first cell to swap
                    cell_1 = 1
                    cell_2 = 1
                    while (cell_1 == cell_2):
                        cell_1 = np.random.choice(act_cells_t2)
                        cell_2 = np.random.choice(act_cells_t1)
                    # do the actual swapping
                    conc_data[cell_1, t1] += 1
                    conc_data[cell_1, t2] -= 1
                    conc_data[cell_2, t1] -= 1
                    conc_data[cell_2, t2] += 1

            print(" -- ... done!")
            # split data again into list
            event_lengths = [x.shape[1] for x in all_spike_rasters_ordered]

            event_spike_rasters_shuffled = []
            start = 0
            for el in event_lengths:
                event_spike_rasters_shuffled.append(conc_data[:, start:start + el])
                start = el

            all_spike_rasters_ordered = event_spike_rasters_shuffled

        # do the actual decoding: PRE and POST
        # --------------------------------------------------------------------------------------------------------------
        results_list_pre = decode_using_phmm_modes(mode_means=mode_means_pre,
                                               event_spike_rasters=all_spike_rasters_ordered,
                                               compression_factor=compression_factor,
                                               cell_selection=cell_selection, cells_to_use=cells_ids)

        results_list_post = decode_using_phmm_modes(mode_means=mode_means_post,
                                               event_spike_rasters=all_spike_rasters_ordered,
                                               compression_factor=compression_factor,
                                               cell_selection=cell_selection, cells_to_use=cells_ids)

        pre_likeli = np.vstack(results_list_pre)
        post_likeli = np.vstack(results_list_post)

        pre_max_likeli = np.max(pre_likeli, axis=1)
        post_max_likeli = np.max(post_likeli, axis=1)

        # compute and smooth similarity ratio
        # --------------------------------------------------------------------------------------------------------------
        sim_ratio = (post_max_likeli-pre_max_likeli)/(post_max_likeli+pre_max_likeli)
        sim_ratio_smooth = moving_average(a=sim_ratio, n=n_smoothing)

        if plotting:
            plt.plot(sim_ratio_smooth)
            plt.plot(np.linspace(0,np.round(duration_sleep_h,0),sim_ratio_smooth.shape[0]),sim_ratio_smooth)
            if shuffling:
                plt.title("Shuffled data")
                plt.ylabel("sim_ratio - shuffled")
            else:
                plt.title("Similarity ratio")
                plt.ylabel("sim_ratio")
            plt.ylim(-1,1)
            plt.xlabel("Time (h)")
            plt.show()
        else:
            return sim_ratio, sim_ratio_smooth, duration_sleep_h

    def memory_drift_entire_sleep_spike_shuffle_vs_data(self, save_fig=False, nr_shuffles=2, n_smoothing=400,
                                                        swapping_factor=10):

        # get data
        # --------------------------------------------------------------------------------------------------------------
        print("Computing results for data ...")
        data, data_smooth, duration_sleep_h = self.memory_drift_entire_sleep(shuffling=False, plotting=False,
                                                                             n_smoothing=n_smoothing)
        # compute shuffle
        # --------------------------------------------------------------------------------------------------------------

        if nr_shuffles > 1:
            print("Computing results for shuffled data (#"+str(nr_shuffles)+") ...")
            with mp.Pool(nr_shuffles) as p:
                # use partial to pass several arguments to cross_val_model, only nr_clusters_array is a changing one,
                # the others are constant
                multi_arg = partial(self.memory_drift_entire_sleep, plotting=False,  n_smoothing=n_smoothing, 
                                    swapping_factor=swapping_factor, shuffling=True)

                # provide random seed to each parallel run
                results = p.map(multi_arg, np.random.randint(0,5000,nr_shuffles))
                # result format: sim_ratio, sim_ratio_smooth, duration_sleep_h

                data_shuffle_smooth = []
                data_shuffle = []
                # put results together
                for shuffle_id in range(len(results)):
                    data_shuffle.append(results[shuffle_id][0])
                    data_shuffle_smooth.append(results[shuffle_id][1])

                data_shuffle_smooth = np.vstack(data_shuffle_smooth)
                data_shuffle = np.vstack(data_shuffle)

                data_shuffle_smooth_mean = np.mean(data_shuffle_smooth, axis=0)
                data_shuffle_smooth_std = np.std(data_shuffle_smooth, axis=0)

        else:

            data_shuffled, data_shuffled_smooth, _ = self.memory_drift_entire_sleep(shuffling=True, plotting=False,
                                                                                      n_smoothing=n_smoothing)
            # if we only do one shuffle --> we get a constant mean/std for the entire duration
            data_shuffle_smooth_mean = np.mean(data_shuffled_smooth)
            data_shuffle_smooth_std = np.std(data_shuffled_smooth)
            # if only one shuffle: can only compute mean and std for entire duration
            data_shuffle_smooth_mean = np.repeat(data_shuffle_smooth_mean, data_shuffled_smooth.shape[0])
            data_shuffle_smooth_std = np.repeat(data_shuffle_smooth_std, data_shuffled_smooth.shape[0])

        # save z-scored data to compute p-values
        data_smooth_z_scored_with_shuffle = (data_smooth - data_shuffle_smooth_mean)/data_shuffle_smooth_std
        np.save(self.params.pre_proc_dir+"/drift_vs_spike_shuffle/"+self.session_name, data_smooth_z_scored_with_shuffle)

        if save_fig:
            plt.style.use('default')

        # plotting
        fig = plt.figure()
        ax = fig.add_subplot()
        ax.plot(np.linspace(0, np.round(duration_sleep_h,0),data_shuffle_smooth_mean.shape[0]),
                data_shuffle_smooth_mean + data_shuffle_smooth_std, color="#B4915C",
                linestyle="dashed", linewidth=0.1)
        ax.plot(np.linspace(0,np.round(duration_sleep_h,0),data_shuffle_smooth_mean.shape[0]),
                data_shuffle_smooth_mean - data_shuffle_smooth_std, color="#B4915C",
                linestyle="dashed", linewidth=0.1)
        ax.fill_between(np.linspace(0,np.round(duration_sleep_h,0),data_shuffle_smooth_mean.shape[0]),
                        data_shuffle_smooth_mean-data_shuffle_smooth_std,
                        data_shuffle_smooth_mean+data_shuffle_smooth_std, facecolor="#D7BA8F")
        ax.plot(np.linspace(0,np.round(duration_sleep_h,0),data_shuffle_smooth_mean.shape[0]),data_shuffle_smooth_mean,
                color="#B4915C",
                label="Shuffle", linewidth=1)
        ax.plot(np.linspace(0,np.round(duration_sleep_h,0),data_smooth.shape[0]),data_smooth, c="#3D7865", label="Data")
        plt.legend()
        plt.grid(axis='y')
        plt.xlabel("Duration (h)")
        y_axis = [0, 7, 14, 21]
        plt.xticks(y_axis, y_axis)
        plt.yticks([-1,-0.5,0,0.5,1])
        plt.xlim(0, np.round(duration_sleep_h,0))
        plt.ylim(-1, 1)
        plt.ylabel("sim_ratio")
        if save_fig:
            plt.rcParams['font.family'] = 'arial'
            plt.rcParams['svg.fonttype'] = 'none'
            plt.savefig("drift.svg", transparent="True")
        else:
            plt.show()

    def memory_drift_entire_sleep_spike_shuffle_vs_data_compute_p_values(self):

        z_scored_data = np.load(self.params.pre_proc_dir+"drift_vs_spike_shuffle/"+self.session_name+".npy")

        # p-value for initial part of sleep: true data < shuffle --> use first 100 points
        z_scored_initial = np.mean(z_scored_data[:100])
        # p-value for last part of sleep: true data > shuffle --> use last 100 points
        z_scored_end = np.mean(z_scored_data[-100:])

        return z_scored_initial, z_scored_end

    # plotting for poster/papers
    # ------------------------------------------------------------------------------------------------------------------

    def memory_drift_plot_epochs_separate(self, template_type, part_to_analyze, pre_file_name=None, post_file_name=None,
                                n_moving_average_swr=15, n_moving_average_pop_vec=60, only_stable_cells=False):
        """
        plot results of memory drift analysis for epochs separately (or in same plot, but not concatenated)

        :param template_type: which template to use ("phmm", "ising")
        :type template_type: str
        :param part_to_analyze: which sleep epoch to analyze ("rem", "nrem", "nrem_rem")
        :type part_to_analyze: str
        :param pre_file_name: name of PRE template, if None --> use default
        :type pre_file_name: str
        :param post_file_name: name of POST template, if None --> use default
        :type post_file_name: str
        :param n_moving_average_swr: how much smoothing to apply across SWR
        :type n_moving_average_swr: int
        :param n_moving_average_pop_vec: how much smoothing to apply across population vectors
        :type n_moving_average_pop_vec: int
        :param only_stable_cells: whether to only use stable cells
        :type only_stable_cells: bool
        """
        if part_to_analyze == "nrem_rem":
            first = 0
            # plot nrem data first
            # ----------------------------------------------------------------------------------------------------------
            pre_prob_list = []
            post_prob_list = []
            event_times_list = []
            for l_s in self.long_sleep:
                duration = l_s.get_duration_sec()
                pre_prob, post_prob, ev_t, _ = l_s.decode_activity_using_pre_post(template_type=template_type,
                                                                                  pre_file_name=pre_file_name, post_file_name=post_file_name, part_to_analyze="nrem",
                                                                                  only_stable_cells=only_stable_cells)
                pre_prob_list.extend(pre_prob)
                post_prob_list.extend(post_prob)
                event_times_list.extend(ev_t+first)
                first += duration

            pre_prob_arr = np.vstack(pre_prob_list)
            post_prob_arr = np.vstack(post_prob_list)
            event_times = np.vstack(event_times_list)

            # z-scoring of probabilites
            pre_prob_arr_z = zscore(pre_prob_arr, axis=0)
            post_prob_arr_z = zscore(post_prob_arr, axis=0)

            # assign array chunks to events again (either SWR or rem phases)
            event_lengths = [x.shape[0] for x in pre_prob_list]
            pre_prob_z = []
            post_prob_z = []
            first = 0
            for swr_id in range(len(event_lengths)):
                pre_prob_z.append(pre_prob_arr_z[first:first + event_lengths[swr_id], :])
                post_prob_z.append(post_prob_arr_z[first:first + event_lengths[swr_id], :])
                first += event_lengths[swr_id]

            event_pre_post_ratio, event_pre_post_ratio_z, event_pre_prob, event_post_prob, event_len_seq, \
            pop_vec_pre_post_ratio, pre_seq_list, pre_seq_list_z, post_seq_list, pre_seq_list_prob, \
            post_seq_list_prob, pop_vec_post_prob, pop_vec_pre_prob = self.compute_values_from_probabilities(
                pre_prob_list=pre_prob_list, post_prob_list=post_prob_list, post_prob_z_list=post_prob_z,
                pre_prob_z_list=pre_prob_z)

            # smoothen
            # ------------------------------------------------------------------------------------------------------
            event_pre_post_ratio_smooth = moving_average(a=np.array(event_pre_post_ratio), n=n_moving_average_swr)
            event_pre_post_ratio_smooth_z = moving_average(a=np.array(event_pre_post_ratio_z), n=n_moving_average_swr)
            pop_vec_pre_post_ratio = np.array(pop_vec_pre_post_ratio)
            # compute moving average to smooth signal
            pop_vec_pre_post_ratio_smooth = moving_average(a=pop_vec_pre_post_ratio, n=n_moving_average_pop_vec)
            event_len_seq_smooth = moving_average(a=np.array(event_len_seq), n=n_moving_average_swr)

            event_times = event_times[:event_pre_post_ratio_smooth.shape[0], :]
            # # plot per nrem phase
            fig = plt.figure()
            ax = fig.add_subplot()
            # for nrem_id in range(swr_to_nrem.shape[0]):
            #     ax.plot(event_times[swr_to_nrem[nrem_id,:]==1,1],
            #             event_pre_post_ratio_smooth[swr_to_nrem[nrem_id,:]==1],c="blue", label="NREM")

            # plot per nrem phase
            start = 0
            for rem_length, rem_time in zip(event_lengths, event_times):
                if start + rem_length > pop_vec_pre_post_ratio_smooth.shape[0]:
                    continue
                ax.plot(np.linspace(rem_time[0], rem_time[1], rem_length),
                        pop_vec_pre_post_ratio_smooth[start:start + rem_length], c="blue", label="NREM")
                start += rem_length


            # plot rem data
            # ----------------------------------------------------------------------------------------------------------
            first = 0

            pre_prob_list = []
            post_prob_list = []
            event_times_list = []
            for l_s in self.long_sleep:
                duration = l_s.get_duration_sec()
                pre_prob, post_prob, ev_t, _ = l_s.decode_activity_using_pre_post(template_type=template_type,
                                                                                  pre_file_name=pre_file_name, post_file_name=post_file_name, part_to_analyze="rem",
                                                                                  only_stable_cells=only_stable_cells)
                pre_prob_list.extend(pre_prob)
                post_prob_list.extend(post_prob)
                event_times_list.extend(ev_t+first)
                first += duration

            pre_prob_arr = np.vstack(pre_prob_list)
            post_prob_arr = np.vstack(post_prob_list)
            event_times = np.vstack(event_times_list)

            # assign array chunks to events again (either SWR or rem phases)
            event_lengths = [x.shape[0] for x in pre_prob_list]
            pre_prob_z = []
            post_prob_z = []
            first = 0
            for swr_id in range(len(event_lengths)):
                pre_prob_z.append(pre_prob_arr_z[first:first + event_lengths[swr_id], :])
                post_prob_z.append(post_prob_arr_z[first:first + event_lengths[swr_id], :])
                first += event_lengths[swr_id]

            event_pre_post_ratio, event_pre_post_ratio_z, event_pre_prob, event_post_prob, event_len_seq, \
            pop_vec_pre_post_ratio, pre_seq_list, pre_seq_list_z, post_seq_list, pre_seq_list_prob, \
            post_seq_list_prob, pop_vec_post_prob, pop_vec_pre_prob = self.compute_values_from_probabilities(
                pre_prob_list=pre_prob_list, post_prob_list=post_prob_list, post_prob_z_list=post_prob_z,
            pre_prob_z_list=pre_prob_z)

            # smoothen
            # ------------------------------------------------------------------------------------------------------
            event_pre_post_ratio_smooth = moving_average(a=np.array(event_pre_post_ratio), n=n_moving_average_swr)
            event_pre_post_ratio_smooth_z = moving_average(a=np.array(event_pre_post_ratio_z), n=n_moving_average_swr)
            pop_vec_pre_post_ratio = np.array(pop_vec_pre_post_ratio)
            # compute moving average to smooth signal
            pop_vec_pre_post_ratio_smooth = moving_average(a=pop_vec_pre_post_ratio, n=n_moving_average_pop_vec)
            event_len_seq_smooth = moving_average(a=np.array(event_len_seq), n=n_moving_average_swr)

            # plot per nrem phase
            start = 0
            for rem_length, rem_time in zip(event_lengths, event_times):
                if start + rem_length > pop_vec_pre_post_ratio_smooth.shape[0]:
                    continue
                ax.plot(np.linspace(rem_time[0], rem_time[1], rem_length),
                        pop_vec_pre_post_ratio_smooth[start:start+rem_length],c="red", label="REM", alpha=0.5)
                start += rem_length

            handles, labels = ax.get_legend_handles_labels()
            by_label = OrderedDict(zip(labels, handles))
            ax.legend(by_label.values(), by_label.keys())
            plt.xlabel("TIME / s")
            plt.ylabel("PRE_POST SIMILARITY")
            plt.ylim(-1,1)
            plt.grid()
            plt.show()

        else:
            first = 0
            pre_prob_list = []
            post_prob_list = []
            event_times_list = []
            for l_s in self.long_sleep:
                duration = l_s.get_duration_sec()
                pre_prob, post_prob, ev_t, _ = l_s.decode_activity_using_pre_post(template_type=template_type,
                                                                                  pre_file_name=pre_file_name, post_file_name=post_file_name, part_to_analyze=part_to_analyze)
                pre_prob_list.extend(pre_prob)
                post_prob_list.extend(post_prob)
                event_times_list.extend(ev_t+first)
                first += duration

            pre_prob_arr = np.vstack(pre_prob_list)
            post_prob_arr = np.vstack(post_prob_list)

            # z-scoring of probabilites
            pre_prob_arr_z = zscore(pre_prob_arr, axis=0)
            post_prob_arr_z = zscore(post_prob_arr, axis=0)

            # assign array chunks to events again (either SWR or rem phases)
            event_lengths = [x.shape[0] for x in pre_prob_list]
            pre_prob_z = []
            post_prob_z = []
            first = 0
            for swr_id in range(len(event_lengths)):
                pre_prob_z.append(pre_prob_arr_z[first:first + event_lengths[swr_id], :])
                post_prob_z.append(post_prob_arr_z[first:first + event_lengths[swr_id], :])
                first += event_lengths[swr_id]

            nr_modes_pre = pre_prob_list[0].shape[1]
            nr_modes_post = post_prob_list[0].shape[1]

            # per event results
            event_pre_post_ratio = []
            event_pre_post_ratio_z = []
            event_pre_prob = []
            event_post_prob = []
            event_len_seq = []

            # per population vector results
            pop_vec_pre_post_ratio = []
            pre_seq_list = []
            pre_seq_list_z = []
            post_seq_list = []
            pre_seq_list_prob = []
            post_seq_list_prob = []
            pop_vec_post_prob = []
            pop_vec_pre_prob = []

            # go trough all events
            for pre_array, post_array, pre_array_z, post_array_z in zip(pre_prob_list, post_prob_list, pre_prob_z,
                                                                        post_prob_z):
                # make sure that there is any data for the current SWR
                if pre_array.shape[0] > 0:
                    pre_sequence = np.argmax(pre_array, axis=1)
                    pre_sequence_z = np.argmax(pre_array_z, axis=1)
                    pre_sequence_prob = np.max(pre_array, axis=1)
                    post_sequence = np.argmax(post_array, axis=1)
                    post_sequence_prob = np.max(post_array, axis=1)
                    pre_seq_list_z.extend(pre_sequence_z)
                    pre_seq_list.extend(pre_sequence)
                    post_seq_list.extend(post_sequence)
                    pre_seq_list_prob.extend(pre_sequence_prob)
                    post_seq_list_prob.extend(post_sequence_prob)

                    # check how likely observed sequence is considering transitions from model (awake behavior)
                    mode_before = pre_sequence[:-1]
                    mode_after = pre_sequence[1:]
                    event_len_seq.append(pre_sequence.shape[0])

                    # per SWR computations
                    # ----------------------------------------------------------------------------------------------
                    # arrays: [nr_pop_vecs_per_SWR, nr_time_spatial_time_bins]
                    # get maximum value per population vector and take average across the SWR
                    if pre_array.shape[0] > 0:
                        # save pre and post probabilities
                        event_pre_prob.append(np.mean(np.max(pre_array, axis=1)))
                        event_post_prob.append(np.mean(np.max(post_array, axis=1)))
                        # compute ratio by picking "winner" mode by first comparing z scored probabilities
                        # then the probability of the most over expressed mode (highest z-score) is used
                        pre_sequence_z = np.argmax(pre_array_z, axis=1)
                        prob_pre_z = np.mean(pre_array[:, pre_sequence_z])
                        post_sequence_z = np.argmax(post_array_z, axis=1)
                        prob_post_z = np.mean(post_array[:, post_sequence_z])
                        event_pre_post_ratio_z.append((prob_post_z - prob_pre_z) / (prob_post_z + prob_pre_z))

                        # compute ratio using probabilites
                        prob_pre = np.mean(np.max(pre_array, axis=1))
                        prob_post = np.mean(np.max(post_array, axis=1))
                        event_pre_post_ratio.append((prob_post - prob_pre) / (prob_post + prob_pre))
                    else:
                        event_pre_prob.append(np.nan)
                        event_post_prob.append(np.nan)
                        event_pre_post_ratio.append(np.nan)

                    # per population vector computations
                    # ----------------------------------------------------------------------------------------------
                    # compute per population vector similarity score
                    prob_post = np.max(post_array, axis=1)
                    prob_pre = np.max(pre_array, axis=1)
                    pop_vec_pre_post_ratio.extend((prob_post - prob_pre) / (prob_post + prob_pre))

                    if pre_array.shape[0] > 0:
                        pop_vec_pre_prob.extend(np.max(pre_array, axis=1))
                        pop_vec_post_prob.extend(np.max(post_array, axis=1))
                    else:
                        pop_vec_pre_prob.extend([np.nan])
                        pop_vec_post_prob.extend([np.nan])

            pop_vec_pre_prob = np.array(pop_vec_pre_prob)
            pop_vec_post_prob = np.array(pop_vec_post_prob)
            pop_vec_pre_post_ratio = np.array(pop_vec_pre_post_ratio)
            pre_seq_list = np.array(pre_seq_list)

            r_to_plot = range(0, 200)
            plt.figure(figsize=(10, 15))
            plt.subplot(3, 1, 1)
            plt.plot(pop_vec_pre_prob[r_to_plot], label="MAX. PROB. PRE")
            plt.plot(pop_vec_post_prob[r_to_plot], label="MAX. PROB. POST")
            # plt.plot(pre_SWR_prob_arr[r_to_plot, 10], c="r", label="PROB. MODE 60")
            plt.legend()
            plt.ylabel("PROB")
            plt.grid()
            plt.yscale("log")
            plt.subplot(3, 1, 2)
            plt.scatter(r_to_plot, pop_vec_pre_post_ratio[r_to_plot], c="magenta")
            plt.ylabel("PRE_POST RATIO")
            plt.grid()
            plt.subplot(3, 1, 3)
            plt.scatter(r_to_plot, pre_seq_list[r_to_plot], c="y")
            plt.ylabel("PRE MODE ID")
            plt.grid()
            plt.xlabel("POP. VEC. ID")
            plt.show()

            # smoothen
            # ------------------------------------------------------------------------------------------------------
            event_pre_post_ratio_smooth = moving_average(a=np.array(event_pre_post_ratio), n=n_moving_average_swr)
            event_pre_post_ratio_smooth_z = moving_average(a=np.array(event_pre_post_ratio_z), n=n_moving_average_swr)
            pop_vec_pre_post_ratio = np.array(pop_vec_pre_post_ratio)
            # compute moving average to smooth signal
            pop_vec_pre_post_ratio_smooth = moving_average(a=pop_vec_pre_post_ratio, n=n_moving_average_pop_vec)
            event_len_seq_smooth = moving_average(a=np.array(event_len_seq), n=n_moving_average_swr)

            # compute per mode info
            # ------------------------------------------------------------------------------------------------------
            pre_seq = np.array(pre_seq_list)
            post_seq = np.array(post_seq_list)
            mode_score_mean_pre = np.zeros(pre_prob_list[0].shape[1])
            mode_score_std_pre = np.zeros(pre_prob_list[0].shape[1])
            mode_score_mean_post = np.zeros(post_prob_list[0].shape[1])
            mode_score_std_post = np.zeros(post_prob_list[0].shape[1])

            # go through all pre modes and check the average score
            for i in range(pre_prob_list[0].shape[1]):
                ind_sel = np.where(pre_seq == i)[0]
                if ind_sel.size == 0 or ind_sel.size == 1:
                    mode_score_mean_pre[i] = np.nan
                    mode_score_std_pre[i] = np.nan
                else:
                    # delete all indices that are too large (becaue of moving average)
                    ind_sel = ind_sel[ind_sel < pop_vec_pre_post_ratio.shape[0]]
                    mode_score_mean_pre[i] = np.mean(pop_vec_pre_post_ratio[ind_sel])
                    mode_score_std_pre[i] = np.std(pop_vec_pre_post_ratio[ind_sel])

            # go through all post modes and check the average score
            for i in range(post_prob_list[0].shape[1]):
                ind_sel = np.where(post_seq == i)[0]
                if ind_sel.size == 0 or ind_sel.size == 1:
                    mode_score_mean_post[i] = np.nan
                    mode_score_std_post[i] = np.nan
                else:
                    # delete all indices that are too large (becaue of moving average)
                    ind_sel = ind_sel[ind_sel < pop_vec_pre_post_ratio.shape[0]]
                    mode_score_mean_post[i] = np.mean(pop_vec_pre_post_ratio[ind_sel])
                    mode_score_std_post[i] = np.std(pop_vec_pre_post_ratio[ind_sel])

            low_score_modes = np.argsort(mode_score_mean_pre)
            # need to skip nans
            nr_nans = np.count_nonzero(np.isnan(mode_score_mean_pre))
            high_score_modes = np.flip(low_score_modes)[nr_nans:]

            # check if modes get more often/less often reactivated over time
            pre_seq_list = np.array(pre_seq_list)
            nr_pop_vec = 20
            nr_windows = int(pre_seq_list.shape[0] / nr_pop_vec)
            occurence_modes_pre = np.zeros((nr_modes_pre, nr_windows))
            for i in range(nr_windows):
                seq = pre_seq_list[i * nr_pop_vec:(i + 1) * nr_pop_vec]
                mode, counts = np.unique(seq, return_counts=True)
                occurence_modes_pre[mode, i] = counts

            # check if modes get more often/less often reactivated over time
            post_seq_list = np.array(post_seq_list)
            nr_pop_vec = 20
            nr_windows = int(post_seq_list.shape[0] / nr_pop_vec)
            occurence_modes_post = np.zeros((nr_modes_pre, nr_windows))
            for i in range(nr_windows):
                seq = post_seq_list[i * nr_pop_vec:(i + 1) * nr_pop_vec]
                mode, counts = np.unique(seq, return_counts=True)
                occurence_modes_post[mode, i] = counts

            # plot similarity scores for SWR & pop vec
            # ------------------------------------------------------------------------------------------------------
            plt.plot(event_pre_post_ratio_smooth, c="r", label="n_mov_avg = " + str(n_moving_average_swr))
            plt.title("PRE-POST RATIO FOR EACH EVENT: PHMM")
            if part_to_analyze == "nrem":
                plt.xlabel("SWR ID")
            elif part_to_analyze == "rem":
                plt.xlabel("REM PHASE ID")
            plt.ylabel("PRE-POST SIMILARITY")
            plt.ylim(-1, 1)
            plt.grid()
            plt.legend()
            plt.show()

            plt.plot(event_pre_post_ratio_smooth_z, c="r", label="n_mov_avg = " + str(n_moving_average_swr))
            plt.title("PRE-POST RATIO FOR EACH EVENT: PHMM\n Z-SCORED TO SELECT WINNER")
            if part_to_analyze == "nrem":
                plt.xlabel("SWR ID")
            elif part_to_analyze == "rem":
                plt.xlabel("REM PHASE ID")
            plt.ylabel("PRE-POST SIMILARITY")
            plt.ylim(-1, 1)
            plt.grid()
            plt.legend()
            plt.show()

            plt.plot(pop_vec_pre_post_ratio_smooth, label="n_mov_avg = " + str(n_moving_average_pop_vec))
            plt.title("PRE-POST RATIO FOR EACH POP. VECTOR: PHMM")
            plt.xlabel("POP.VEC. ID")
            plt.ylabel("PRE-POST SIMILARITY")
            plt.ylim(-1, 1)
            plt.grid()
            plt.legend()
            plt.show()

            plt.plot(event_len_seq_smooth, label="n_mov_avg = " + str(n_moving_average_swr))
            plt.title("EVENT LENGTH")
            if part_to_analyze == "nrem":
                plt.xlabel("SWR ID")
            elif part_to_analyze == "rem":
                plt.xlabel("REM PHASE ID")
            plt.ylabel("#POP.VEC. PER SWR")
            plt.grid()
            plt.legend()
            plt.show()

            plt.imshow(occurence_modes_pre, interpolation='nearest', aspect='auto')
            plt.ylabel("MODE ID")
            plt.xlabel("WINDOW ID")
            a = plt.colorbar()
            a.set_label("#WINS/" + str(nr_pop_vec) + " POP. VEC. WINDOW")
            plt.title("PRE: OCCURENCE (#WINS) OF MODES IN WINDOWS OF FIXED LENGTH")
            plt.show()

            plt.imshow(occurence_modes_post, interpolation='nearest', aspect='auto')
            plt.ylabel("MODE ID")
            plt.xlabel("WINDOW ID")
            a = plt.colorbar()
            a.set_label("#WINS/" + str(nr_pop_vec) + " POP. VEC. WINDOW")
            plt.title("POST: OCCURENCE (#WINS) OF MODES IN WINDOWS OF FIXED LENGTH")
            plt.show()

            plt.errorbar(range(pre_prob[0].shape[1]), mode_score_mean_pre, yerr=mode_score_std_pre,
                         linestyle="")
            plt.scatter(range(pre_prob[0].shape[1]), mode_score_mean_pre)
            plt.title("PRE-POST SCORE PER MODE: PRE")
            plt.xlabel("MODE ID")
            plt.ylabel("PRE-POST SCORE: MEAN AND STD")
            plt.show()

            plt.errorbar(range(post_prob[0].shape[1]), mode_score_mean_post, yerr=mode_score_std_post,
                         linestyle="")
            plt.scatter(range(post_prob[0].shape[1]), mode_score_mean_post)
            plt.title("PRE-POST SCORE PER MODE: POST")
            plt.xlabel("MODE ID")
            plt.ylabel("PRE-POST SCORE: MEAN AND STD")
            plt.show()

    def memory_drift_time_course(self, template_type, n_moving_average_pop_vec=20000, rem_pop_vec_threshold=10,
                                        cells_to_use="all"):

        r_smooth = self.memory_drift_plot_temporal_trend(template_type=template_type, measure="normalized_ratio",
                                                         n_moving_average_pop_vec=n_moving_average_pop_vec,
                                                         rem_pop_vec_threshold=rem_pop_vec_threshold,
                                                         plotting=False, cells_to_use=cells_to_use, save_fig=False)

        r_first_half = r_smooth[:int(0.5*r_smooth.shape[0])]
        r_second_half = r_smooth[int(0.5 * r_smooth.shape[0]):]
        r_delta_first_half = np.mean(r_first_half[-50:]) - np.mean(r_first_half[:50])
        r_delta_second_half = np.mean(r_second_half[-50:]) - np.mean(r_second_half[:50])

        return r_delta_first_half, r_delta_second_half

    def memory_drift_plot_temporal_trend(self, template_type, measure="normalized_ratio", pre_file_name=None,
                                         post_file_name=None, n_moving_average_pop_vec=200, rem_pop_vec_threshold=10,
                                         plotting=False, cells_to_use="all", save_fig=False):
        """
        detailed memory drift analysis: opposing effects of NREM/REM etc.

        :param template_type: which template to use ("phmm" or "ising")
        :type template_type: str
        :param measure: which measure to use to compare PRE and POST similarity (model_evidence, log_like_ratio,
                        normalized_ratio
        :type measure: str
        :param pre_file_name: name of PRE template, if None --> default is used
        :type pre_file_name: str
        :param post_file_name: name of POST template, if None --> default is used
        :type post_file_name: str_decoding_similarity_temporal
        :param n_moving_average_pop_vec: how much smoothing to apply across population vectors
        :type n_moving_average_pop_ - np.mean(dec, axis=0)[0]vec: int
        :param rem_pop_vec_threshold: minimum length of REM epochs (shorter ones are discarded)
        :type rem_pop_vec_threshold: int
        :param plotting: whether to plot
        :type plotting: bool
        :param only_stable_cells: whether to only use stable cells
        :type only_stable_cells: bool
        :return: ds_rem_sum, ds_nrem_sum, ratio_per_merged_nrem_event, ratio_per_merged_rem_event
        :rtype:
        """
        # get length of sleep in seconds

        len_sleep = []
        for l_s in self.long_sleep:
            len_sleep.append(l_s.get_duration_sec())

        len_sleep = np.sum(np.array(len_sleep))
        len_sleep_h = len_sleep/60/60

        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_rem_event, event_times_rem, event_lengths_rem, event_duration_rem_in_s, _, _ = \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold, measure=measure,
                                                     cells_to_use=cells_to_use)

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_nrem_event, event_times_nrem, event_lengths_nrem, event_duration_nrem_in_s, _ ,_ = \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="nrem",
                                                     pop_vec_threshold=2, measure=measure,
                                                     cells_to_use=cells_to_use)

        # combine rem and nrem data to order it in the right way (chronologically)
        # --------------------------------------------------------------------------------------_decoding_similarity_temporal------------------------

        all_events_ratio = ratio_per_rem_event + ratio_per_nrem_event
        all_events_length = event_lengths_rem + event_lengths_nrem
        all_events_length_s = np.hstack((event_duration_rem_in_s, event_duration_nrem_in_s))
        labels_events = np.zeros(len(ratio_per_rem_event)+len(ratio_per_nrem_event))
        labels_events[:len(ratio_per_rem_event)] = 1
        all_times = np.hstack((event_times_rem[:, 0], event_times_nrem[:,0]))
        all_end_times = np.hstack((event_times_rem[:, 1], event_times_nrem[:, 2]))

        # sort events according to time
        sorted_events_ratio_list = [x for _, x in sorted(zip(all_times, all_events_ratio))]
        sorted_labels_events = [x for _, x in sorted(zip(all_times, labels_events))]
        sorted_lengths = [x for _, x in sorted(zip(all_times, all_events_length))]
        sorted_lengths_s = [x for _, x in sorted(zip(all_times, all_events_length_s))]
        sorted_pop_vec_ratio = np.hstack(sorted_events_ratio_list)
        sorted_times = [x for x in sorted(all_times)]
        sorted_end_times = [x for _, x in sorted(zip(all_times, all_end_times))]

        # compute labels per population vector --> rem events = 1, nrem events = 0
        # --------------------------------------------------------------------------------------------------------------
        labels_per_pop_vec = []
        for len_event, label in zip(sorted_lengths, sorted_labels_events):
            if label:
                labels_per_pop_vec.extend(np.ones(len_event))
            elif label == 0:
                labels_per_pop_vec.extend(np.zeros(len_event))

        labels_per_pop_vec = np.array(labels_per_pop_vec)

        # concatenate events that have the same label
        # --------------------------------------------------------------------------------------------------------------
        sorted_labels_events = np.array(sorted_labels_events)
        label_change = np.diff(sorted_labels_events)
        merged_events_length_s = []
        merged_events_labels = []
        merged_events_times = []
        first = 0
        while True:
            dat = label_change[first:]
            if first > label_change.shape[0]:
                break
            if sorted_labels_events[first] == 0:
                # nrem event
                # check if this is the last event
                if np.where(dat==1)[0].shape[0] == 0:
                    trans = label_change.shape[0] - first + 1
                else:
                    trans = np.where(dat == 1)[0][0]+1
                merged_events_length_s.append(np.sum(sorted_lengths_s[first:first+trans]))
                merged_events_labels.append(np.unique(sorted_labels_events[first:first + trans]))
                merged_events_times.append([sorted_times[first], sorted_end_times[first + trans - 1]])
            elif sorted_labels_events[first] == 1:
                # rem event
                # check if this is the last event
                if np.where(dat==-1)[0].shape[0] == 0:
                    trans = label_change.shape[0] - first + 1
                else:
                    trans = np.where(dat == -1)[0][0]+1
                merged_events_length_s.append(np.sum(sorted_lengths_s[first:first+trans]))
                merged_events_labels.append(np.unique(sorted_labels_events[first:first + trans]))
                merged_events_times.append([sorted_times[first], sorted_end_times[first+trans-1]])
            first += trans

        merged_events_labels = np.squeeze(np.array(merged_events_labels))
        merged_events_length_s = np.array(merged_events_length_s)
        merged_events_times = np.vstack(merged_events_times)

        # merged_rem_event_times = merged_events_times[merged_events_labels == 1]
        # merged_nrem_event_times = merged_events_times[merged_events_labels == 0]
        merged_events_length_s = merged_events_times[:,1]-merged_events_times[:,0]

        # concatenate population vectors that have the same label: TODO: there might be a better way to combine this
        # step and the previous step
        # --------------------------------------------------------------------------------------------------------------
        new_event_time_stamps = np.abs(np.diff(labels_per_pop_vec))

        # need to add one element at the beginning
        new_event_time_stamps = np.insert(new_event_time_stamps, 0, 1)
        start = np.nonzero(new_event_time_stamps==1)[0]
        end = start[1:]
        end = np.append(end, new_event_time_stamps.shape[0])

        # only select data (pre_post ratio) from population vectors that belong to selected rem/nrem events
        # --> results per merged event
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_merged_rem_event = []
        ratio_per_merged_nrem_event = []
        ratio_rem_nrem_events = []
        rem_nrem_events_label = []
        ratio_rem_nrem_pop_vec = []
        rem_nrem_pop_vec_label = []

        for start_event, end_event in zip(start, end):
            # rem event
            if labels_per_pop_vec[start_event + 1] == 1:
                ratio_per_merged_rem_event.append(sorted_pop_vec_ratio[start_event:end_event])
            # nrem event
            else:
                ratio_per_merged_nrem_event.append(sorted_pop_vec_ratio[start_event:end_event])

            ratio_rem_nrem_events.append(sorted_pop_vec_ratio[start_event:end_event])
            rem_nrem_events_label.extend([labels_per_pop_vec[start_event + 1]])

            ratio_rem_nrem_pop_vec.extend(sorted_pop_vec_ratio[start_event:end_event])
            rem_nrem_pop_vec_label.extend(labels_per_pop_vec[start_event:end_event])


        # smooth across population vectors --> that means also across REM/NREM epochs
        # --------------------------------------------------------------------------------------------------------------
        len_new_events = [x.shape[0] for x in ratio_rem_nrem_events]
        ratio_per_pop_vec_new = np.hstack(ratio_rem_nrem_events)
        ratio_per_pop_vec_new_smooth = moving_average(a=np.array(ratio_per_pop_vec_new), n=n_moving_average_pop_vec)

        ratio_rem_nrem_events_smooth = []
        first = 0
        for event_id in range(len(len_new_events)):
            # check if data is still available (due to smoothing --> data gets shorter)
            # if not --> leave loop
            if first >= ratio_per_pop_vec_new_smooth.shape[0]:
                break
            end_event = min(first + len_new_events[event_id], ratio_per_pop_vec_new_smooth.shape[0])
            ratio_rem_nrem_events_smooth.append(ratio_per_pop_vec_new_smooth[first:end_event])
            first += len_new_events[event_id]

        # need to delete indices of events that happend after the last data point after smoothing
        rem_nrem_events_label = rem_nrem_events_label[:len(ratio_rem_nrem_events_smooth)]

        r_smooth = np.hstack(ratio_rem_nrem_events_smooth)

        control = []
        slope_control = []
        s = np.copy(ratio_per_pop_vec_new)
        for i in range(500):
            np.random.shuffle(s)
            s_smooth = moving_average(a=np.array(s), n=n_moving_average_pop_vec)
            control.append(s_smooth)
            Y = np.expand_dims(s_smooth,0)
            X = np.expand_dims(np.linspace(0,np.round(len_sleep_h,0),s_smooth.shape[0]),0)
            slope = ((X*Y).mean(axis=1) - X.mean()*Y.mean(axis=1)) / ((X**2).mean() - (X.mean())**2)
            slope_control.append(slope)

        slope_control = np.array(slope_control)
        Y = np.expand_dims(r_smooth, 0)
        X = np.expand_dims(np.linspace(0, np.round(len_sleep_h, 0), r_smooth.shape[0]),0)
        slope_data = ((X * Y).mean(axis=1) - X.mean() * Y.mean(axis=1)) / ((X ** 2).mean() - (X.mean()) ** 2)

        slope_control_99 = np.percentile(slope_control, 99)

        if slope_data > slope_control_99:
            print("STATS OK: > 99%ile")
        else:
            print("STATS NOT OK")

        control = np.array(control)
        con_mean = np.mean(control, axis=0)
        con_std = np.std(control, axis=0)

        if plotting or save_fig:

            if save_fig:
                plt.style.use('default')

            # plotting
            fig = plt.figure()
            ax = fig.add_subplot()
            ax.plot(np.linspace(0,np.round(len_sleep_h,0),con_mean.shape[0]),con_mean + con_std, color="#B4915C", linestyle="dashed", linewidth=0.1)
            ax.plot(np.linspace(0,np.round(len_sleep_h,0),con_mean.shape[0]),con_mean - con_std, color="#B4915C", linestyle="dashed", linewidth=0.1)
            ax.fill_between(np.linspace(0,np.round(len_sleep_h,0),con_mean.shape[0]), con_mean-con_std, con_mean+con_std, facecolor="#D7BA8F")
            ax.plot(np.linspace(0,np.round(len_sleep_h,0),con_mean.shape[0]),con_mean, color="#B4915C", label="Shuffle", linewidth=1)
            ax.plot(np.linspace(0,np.round(len_sleep_h,0),r_smooth.shape[0]),r_smooth, c="#3D7865", label="Data")
            plt.legend()
            plt.grid(axis='y')
            plt.xlabel("Duration (h)")
            y_axis = [0, int(len_sleep_h * 0.25), int(len_sleep_h * 0.5), int(len_sleep_h * 0.75), int(len_sleep_h)]
            plt.xticks(y_axis, y_axis)
            plt.yticks([-1,-0.5,0,0.5,1])
            plt.xlim(0, np.round(len_sleep_h,0))
            plt.ylim(-1, 1)
            plt.ylabel("sim_ratio")
            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig("drift.svg", transparent="True")
            else:
                plt.show()

        else:
            return r_smooth

    def memory_drift_plot_likelihoods_temporal(self, template_type, pre_file_name=None,
                                         post_file_name=None, n_moving_average_pop_vec=4000, rem_pop_vec_threshold=10,
                                         plotting=False, cells_to_use="all", save_fig=False):
        """
        detailed memory drift analysis: opposing effects of NREM/REM etc.

        :param template_type: which template to use ("phmm" or "ising")
        :type template_type: str
        :param measure: which measure to use to compare PRE and POST similarity (model_evidence, log_like_ratio,
                        normalized_ratio
        :type measure: str
        :param pre_file_name: name of PRE template, if None --> default is used
        :type pre_file_name: str
        :param post_file_name: name of POST template, if None --> default is used
        :type post_file_name: str_decoding_similarity_temporal
        :param n_moving_average_pop_vec: how much smoothing to apply across population vectors
        :type n_moving_average_pop_ - np.mean(dec, axis=0)[0]vec: int
        :param rem_pop_vec_threshold: minimum length of REM epochs (shorter ones are discarded)
        :type rem_pop_vec_threshold: int
        :param plotting: whether to plot
        :type plotting: bool
        :param only_stable_cells: whether to only use stable cells
        :type only_stable_cells: bool
        :return: ds_rem_sum, ds_nrem_sum, ratio_per_merged_nrem_event, ratio_per_merged_rem_event
        :rtype:
        """
        # get length of sleep in seconds

        len_sleep = []
        for l_s in self.long_sleep:
            len_sleep.append(l_s.get_duration_sec())

        len_sleep = np.sum(np.array(len_sleep))
        len_sleep_h = len_sleep/60/60

        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        pre_prob_rem, post_prob_rem, event_times_rem = \
            self.memory_drift_long_sleep_get_raw_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold, cells_to_use=cells_to_use)

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        pre_prob_nrem, post_prob_nrem, event_times_nrem  = \
            self.memory_drift_long_sleep_get_raw_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="nrem",
                                                     pop_vec_threshold=2, cells_to_use=cells_to_use)

        event_times_rem = np.vstack(event_times_rem)
        event_times_nrem = np.vstack(event_times_nrem)

        # combine rem and nrem data to order it in the right way (chronologically)
        # --------------------------------------------------------------------------------------
        all_events_pre_prob = pre_prob_rem + pre_prob_nrem
        all_events_post_prob = post_prob_rem + post_prob_nrem
        event_lengths_rem = [x.shape[0] for x in pre_prob_rem]
        event_lengths_nrem = [x.shape[0] for x in pre_prob_nrem]
        all_events_length = event_lengths_rem + event_lengths_nrem
        labels_events = np.zeros(len(pre_prob_rem)+len(pre_prob_nrem))
        labels_events[:len(pre_prob_rem)] = 1
        all_times = np.hstack((event_times_rem[:, 0], event_times_nrem[:,0]))
        all_end_times = np.hstack((event_times_rem[:, 1], event_times_nrem[:, 2]))

        # sort events according to time
        sorted_events_pre_prob_list = [x for _, x in sorted(zip(all_times, all_events_pre_prob))]
        sorted_events_post_prob_list = [x for _, x in sorted(zip(all_times, all_events_post_prob))]
        sorted_labels_events = [x for _, x in sorted(zip(all_times, labels_events))]
        sorted_lengths = [x for _, x in sorted(zip(all_times, all_events_length))]
        sorted_pre_prob = np.vstack(sorted_events_pre_prob_list)
        sorted_post_prob = np.vstack(sorted_events_post_prob_list)

        sorted_pre_prob = np.max(sorted_pre_prob, axis=1)
        sorted_post_prob = np.max(sorted_post_prob, axis=1)

        smooth_pre_prob = moving_average(a=sorted_pre_prob, n=n_moving_average_pop_vec)
        smooth_post_prob = moving_average(a=sorted_post_prob, n=n_moving_average_pop_vec)
        if save_fig:
            plt.style.use('default')
        plt.plot(smooth_pre_prob, label="PRE")
        plt.plot(smooth_post_prob, label="POST")
        plt.yscale("log")
        plt.ylabel("Likelihood")
        plt.xlabel("Duration (h)")
        y_axis = [0, int(len_sleep_h*0.25), int(len_sleep_h*0.5), int(len_sleep_h*0.75), int(len_sleep_h)]
        y_ticks_pos = [0, int(smooth_pre_prob.shape[0]*0.25), int(smooth_pre_prob.shape[0]*0.5),
                       int(smooth_pre_prob.shape[0]*0.75), int(smooth_pre_prob.shape[0])]
        plt.xticks(y_ticks_pos, y_axis)
        plt.legend()
        if save_fig:
            plt.rcParams['svg.fonttype'] = 'none'
            plt.savefig("pre_post_likelihoods"+template_type+".svg", transparent="True")
        else:
            plt.show()

    def memory_drift_plot_temporal_trend_stable_cells(self, n_moving_average_pop_vec=400, rem_pop_vec_threshold=10,
                                                      plotting=True, nr_parts_to_split_data=4, save_fig=False):
        """
        detailed memory drift analysis: opposing effects of NREM/REM etc.

        :param template_type: which template to use ("phmm" or "ising")
        :type template_type: str
        :param measure: which measure to use to compare PRE and POST similarity (model_evidence, log_like_ratio,
                        normalized_ratio
        :type measure: str
        :param pre_file_name: name of PRE template, if None --> default is used
        :type pre_file_name: str
        :param post_file_name: name of POST template, if None --> default is used
        :type post_file_name: str_decoding_similarity_temporal
        :param n_moving_average_pop_vec: how much smoothing to apply across population vectors
        :type n_moving_average_pop_ - np.mean(dec, axis=0)[0]vec: int
        :param rem_pop_vec_threshold: minimum length of REM epochs (shorter ones are discarded)
        :type rem_pop_vec_threshold: int
        :param plotting: whether to plot
        :type plotting: bool
        :param only_stable_cells: whether to only use stable cells
        :type only_stable_cells: bool
        :return: ds_rem_sum, ds_nrem_sum, ratio_per_merged_nrem_event, ratio_per_merged_rem_event
        :rtype:
        """

        # get length of sleep in seconds
        len_sleep = []
        for l_s in self.long_sleep:
            len_sleep.append(l_s.get_duration_sec())

        len_sleep = np.sum(np.array(len_sleep))
        len_sleep_h = len_sleep/60/60

        r_smooth_stable = self.memory_drift_plot_temporal_trend(template_type="phmm", measure="normalized_ratio",
                                                                pre_file_name=None, post_file_name=None,
                                                                n_moving_average_pop_vec=n_moving_average_pop_vec,
                                                                rem_pop_vec_threshold=rem_pop_vec_threshold,
                                                                plotting=False, cells_to_use="stable")

        r_smooth_all = self.memory_drift_plot_temporal_trend(template_type="phmm", measure="normalized_ratio",
                                                                pre_file_name=None, post_file_name=None,
                                                                n_moving_average_pop_vec=n_moving_average_pop_vec,
                                                                rem_pop_vec_threshold=rem_pop_vec_threshold,
                                                                plotting=False, cells_to_use="all")

        coef_all_list = []
        coef_stable_list = []
        # pop_vec_per_h  = r_smooth_all.shape[0] / len_sleep_h
        window_size=int(r_smooth_all.shape[0]/nr_parts_to_split_data)
        for window_id in range(int(r_smooth_all.shape[0]/window_size)):
            x = np.linspace(0,1/nr_parts_to_split_data, window_size)
            segment_all = r_smooth_all[window_id*window_size:(window_id+1)*window_size]
            segment_stable = r_smooth_stable[window_id*window_size:(window_id+1)*window_size]
            coef_all = np.polyfit(x, segment_all, 1)
            coef_all_list.append(coef_all[0])
            coef_stable = np.polyfit(x, segment_stable, 1)
            coef_stable_list.append(coef_stable[0])

        slope_all = np.mean(np.array(coef_all_list))
        slope_stable = np.mean(np.array(coef_stable_list))

        perc_slope_stable = slope_stable/slope_all

        if plotting or save_fig:
            if save_fig:
                plt.style.use('default')
            # plotting
            fig = plt.figure(figsize=(5,3))
            ax = fig.add_subplot()
            ax.plot(np.linspace(0,np.round(len_sleep_h,0),r_smooth_all.shape[0]),r_smooth_all, c="grey", label="all cells")
            ax.plot(np.linspace(0,np.round(len_sleep_h,0),r_smooth_stable.shape[0]),r_smooth_stable, c="#6B345C",
                    label="stable cells", alpha=0.8)
            plt.legend()
            plt.grid(axis='y')
            plt.xlabel("Sleep duration (h)")
            plt.xticks([0,5,15,20])
            plt.yticks([-1, -0.5, 0])
            plt.xlim(0, np.round(len_sleep_h,0))
            # plt.ylim(-0.35, 0.35)
            plt.ylabel("sim_ratio")
            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig("stable_drift.svg", transparent="True")
            else:
                plt.show()

            # coef_inc = np.polyfit(x, segment_inc, 1)
            # poly1d_fn = np.poly1d(coef_all)
            # x_plotting = np.arange(window_id*window_size, (window_id+1)*window_size)

        else:
            return slope_all, slope_stable

    # NREM / REM analysis
    # ------------------------------------------------------------------------------------------------------------------
    def memory_drift_plot_rem_nrem(self, template_type, measure="normalized_ratio", pre_file_name=None,
                                   post_file_name=None, n_moving_average_pop_vec=20, rem_pop_vec_threshold=10,
                                   plotting=False, cells_to_use="all"):
        """
        detailed memory drift analysis: opposing effects of NREM/REM etc.

        :param template_type: which template to use ("phmm" or "ising")
        :type template_type: str
        :param measure: which measure to use to compare PRE and POST similarity (model_evidence, log_like_ratio,
                        normalized_ratio
        :type measure: str
        :param pre_file_name: name of PRE template, if None --> default is used
        :type pre_file_name: str
        :param post_file_name: name of POST template, if None --> default is used
        :type post_file_name: str_decoding_similarity_temporal
        :param n_moving_average_pop_vec: how much smoothing to apply across population vectors
        :type n_moving_average_pop_ - np.mean(dec, axis=0)[0]vec: int
        :param rem_pop_vec_threshold: minimum length of REM epochs (shorter ones are discarded)
        :type rem_pop_vec_threshold: int
        :param plotting: whether to plot
        :type plotting: bool
        :param only_stable_cells: whether to only use stable cells
        :type only_stable_cells: bool
        :return: ds_rem_sum, ds_nrem_sum, ratio_per_merged_nrem_event, ratio_per_merged_rem_event
        :rtype:
        """

        # get length of sleep in seconds

        len_sleep = []
        for l_s in self.long_sleep:
            len_sleep.append(l_s.get_duration_sec())

        len_sleep = np.sum(np.array(len_sleep))
        len_sleep_h = len_sleep/60/60

        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_rem_event, event_times_rem, event_lengths_rem, event_duration_rem_in_s, _, _ = \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold, measure=measure,
                                                     cells_to_use=cells_to_use)

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_nrem_event, event_times_nrem, event_lengths_nrem, event_duration_nrem_in_s, _ ,_ = \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="nrem",
                                                     pop_vec_threshold=2, measure=measure,
                                                     cells_to_use=cells_to_use)

        # combine rem and nrem data to order it in the right way (chronologically)
        # --------------------------------------------------------------------------------------_decoding_similarity_temporal------------------------

        all_events_ratio = ratio_per_rem_event + ratio_per_nrem_event
        all_events_length = event_lengths_rem + event_lengths_nrem
        all_events_length_s = np.hstack((event_duration_rem_in_s, event_duration_nrem_in_s))
        labels_events = np.zeros(len(ratio_per_rem_event)+len(ratio_per_nrem_event))
        labels_events[:len(ratio_per_rem_event)] = 1
        all_times = np.hstack((event_times_rem[:, 0], event_times_nrem[:,0]))
        all_end_times = np.hstack((event_times_rem[:, 1], event_times_nrem[:, 2]))

        # sort events according to time
        sorted_events_ratio_list = [x for _, x in sorted(zip(all_times, all_events_ratio))]
        sorted_labels_events = [x for _, x in sorted(zip(all_times, labels_events))]
        sorted_lengths = [x for _, x in sorted(zip(all_times, all_events_length))]
        sorted_lengths_s = [x for _, x in sorted(zip(all_times, all_events_length_s))]
        sorted_pop_vec_ratio = np.hstack(sorted_events_ratio_list)
        sorted_times = [x for x in sorted(all_times)]
        sorted_end_times = [x for _, x in sorted(zip(all_times, all_end_times))]

        # compute labels per population vector --> rem events = 1, nrem events = 0
        # --------------------------------------------------------------------------------------------------------------
        labels_per_pop_vec = []
        for len_event, label in zip(sorted_lengths, sorted_labels_events):
            if label:
                labels_per_pop_vec.extend(np.ones(len_event))
            elif label == 0:
                labels_per_pop_vec.extend(np.zeros(len_event))

        labels_per_pop_vec = np.array(labels_per_pop_vec)

        # concatenate events that have the same label
        # --------------------------------------------------------------------------------------------------------------
        sorted_labels_events = np.array(sorted_labels_events)
        label_change = np.diff(sorted_labels_events)
        merged_events_length_s = []
        merged_events_labels = []
        merged_events_times = []
        first = 0
        while True:
            dat = label_change[first:]
            if first > label_change.shape[0]:
                break
            if sorted_labels_events[first] == 0:
                # nrem event
                # check if this is the last event
                if np.where(dat==1)[0].shape[0] == 0:
                    trans = label_change.shape[0] - first + 1
                else:
                    trans = np.where(dat == 1)[0][0]+1
                merged_events_length_s.append(np.sum(sorted_lengths_s[first:first+trans]))
                merged_events_labels.append(np.unique(sorted_labels_events[first:first + trans]))
                merged_events_times.append([sorted_times[first], sorted_end_times[first + trans - 1]])
            elif sorted_labels_events[first] == 1:
                # rem event
                # check if this is the last event
                if np.where(dat==-1)[0].shape[0] == 0:
                    trans = label_change.shape[0] - first + 1
                else:
                    trans = np.where(dat == -1)[0][0]+1
                merged_events_length_s.append(np.sum(sorted_lengths_s[first:first+trans]))
                merged_events_labels.append(np.unique(sorted_labels_events[first:first + trans]))
                merged_events_times.append([sorted_times[first], sorted_end_times[first+trans-1]])
            first += trans

        merged_events_labels = np.squeeze(np.array(merged_events_labels))
        merged_events_length_s = np.array(merged_events_length_s)
        merged_events_times = np.vstack(merged_events_times)

        # merged_rem_event_times = merged_events_times[merged_events_labels == 1]
        # merged_nrem_event_times = merged_events_times[merged_events_labels == 0]
        merged_events_length_s = merged_events_times[:,1]-merged_events_times[:,0]


        # concatenate population vectors that have the same label: TODO: there might be a better way to combine this
        # step and the previous step
        # --------------------------------------------------------------------------------------------------------------
        new_event_time_stamps = np.abs(np.diff(labels_per_pop_vec))

        # need to add one element at the beginning
        new_event_time_stamps = np.insert(new_event_time_stamps, 0, 1)
        start = np.nonzero(new_event_time_stamps==1)[0]
        end = start[1:]
        end = np.append(end, new_event_time_stamps.shape[0])

        # only select data (pre_post ratio) from population vectors that belong to selected rem/nrem events
        # --> results per merged event
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_merged_rem_event = []
        ratio_per_merged_nrem_event = []
        ratio_rem_nrem_events = []
        rem_nrem_events_label = []
        ratio_rem_nrem_pop_vec = []
        rem_nrem_pop_vec_label = []

        for start_event, end_event in zip(start, end):
            # rem event
            if labels_per_pop_vec[start_event + 1] == 1:
                ratio_per_merged_rem_event.append(sorted_pop_vec_ratio[start_event:end_event])
            # nrem event
            else:
                ratio_per_merged_nrem_event.append(sorted_pop_vec_ratio[start_event:end_event])

            ratio_rem_nrem_events.append(sorted_pop_vec_ratio[start_event:end_event])
            rem_nrem_events_label.extend([labels_per_pop_vec[start_event + 1]])

            ratio_rem_nrem_pop_vec.extend(sorted_pop_vec_ratio[start_event:end_event])
            rem_nrem_pop_vec_label.extend(labels_per_pop_vec[start_event:end_event])


        # smooth across population vectors --> that means also across REM/NREM epochs
        # --------------------------------------------------------------------------------------------------------------
        len_new_events = [x.shape[0] for x in ratio_rem_nrem_events]
        ratio_per_pop_vec_new = np.hstack(ratio_rem_nrem_events)
        ratio_per_pop_vec_new_smooth = moving_average(a=np.array(ratio_per_pop_vec_new), n=n_moving_average_pop_vec)

        ratio_rem_nrem_events_smooth = []
        first = 0
        for event_id in range(len(len_new_events)):
            # check if data is still available (due to smoothing --> data gets shorter)
            # if not --> leave loop
            if first >= ratio_per_pop_vec_new_smooth.shape[0]:
                break
            end_event = min(first + len_new_events[event_id], ratio_per_pop_vec_new_smooth.shape[0])
            ratio_rem_nrem_events_smooth.append(ratio_per_pop_vec_new_smooth[first:end_event])
            first += len_new_events[event_id]

        # need to delete indices of events that happend after the last data point after smoothing
        rem_nrem_events_label = rem_nrem_events_label[:len(ratio_rem_nrem_events_smooth)]

        # get rem data of merged events after smoothing
        # --------------------------------------------------------------------------------------------------------------
        rem_events_indices = np.squeeze(np.argwhere(np.array(rem_nrem_events_label) == 1))
        ds_rem = []
        ds_rem_smoothed_within = []
        merged_events_rem_length = []
        ratio_rem_events_smooth = []
        for rem_index in rem_events_indices:
            ds_rem.append(ratio_rem_nrem_events_smooth[rem_index][-1]-ratio_rem_nrem_events_smooth[rem_index][0])
            smooth_event = moving_average(a=ratio_rem_nrem_events[rem_index], n=10)
            ds_rem_smoothed_within.append(smooth_event[-1]-smooth_event[0])
            merged_events_rem_length.append(ratio_rem_nrem_events_smooth[rem_index].shape[0])
            ratio_rem_events_smooth.append(ratio_rem_nrem_events_smooth[rem_index])

        # get nrem data of merged events after smoothing
        # --------------------------------------------------------------------------------------------------------------
        nrem_events_indices = np.squeeze(np.argwhere(np.array(rem_nrem_events_label) == 0))
        ds_nrem = []
        ds_nrem_smoothed_within = []
        merged_events_nrem_length = []
        ratio_nrem_events_smooth = []
        for nrem_index in nrem_events_indices:
            ds_nrem.append(ratio_rem_nrem_events_smooth[nrem_index][-1]-ratio_rem_nrem_events_smooth[nrem_index][0])
            merged_events_nrem_length.append(ratio_rem_nrem_events_smooth[nrem_index].shape[0])
            ratio_nrem_events_smooth.append(ratio_rem_nrem_events_smooth[nrem_index])

        # compute delta score sum & cum sum
        # --------------------------------------------------------------------------------------------------------------
        ds_nrem_sum_smoothed_within = np.sum(np.array(ds_nrem_smoothed_within))
        ds_rem_sum_smoothed_within = np.sum(np.array(ds_rem_smoothed_within))

        # do mann-whitney-u test
        print(self.session_name+" , NREM vs. REM delta scores (MWU): \n")
        print(mannwhitneyu(ds_rem, ds_nrem, alternative="less"))

        ds_nrem_cum = np.cumsum(np.array(ds_nrem))
        ds_rem_cum = np.cumsum(np.array(ds_rem))

        # compute cross correlation of delta scores NREM/REM
        # --------------------------------------------------------------------------------------------------------------
        min_len = min(len(ds_rem), len(ds_nrem))
        ds_rem_arr = np.array(ds_rem)[:min_len]
        ds_nrem_arr = np.array(ds_nrem)[:min_len]
        corr_list_pos = []
        corr_list_neg = []
        shift_array_cross_corr = [0, 1, 2, 3, 4, 5, 6]
        for shift in shift_array_cross_corr:
            corr_list_pos.append(
                np.round(pearsonr(ds_rem_arr[shift:], ds_nrem_arr[:ds_nrem_arr.shape[0] - shift])[0], 2))
            corr_list_neg.append(
                np.round(pearsonr(ds_nrem_arr[shift:], ds_rem_arr[:ds_rem_arr.shape[0] - shift])[0], 2))
        corr_list = np.hstack((np.flip(np.array(corr_list_neg)), np.array(corr_list_pos)))

        # get mean score per event
        # --------------------------------------------------------------------------------------------------------------
        mean_nrem = []
        for i, event in enumerate(ratio_per_merged_nrem_event):
            mean_nrem.append(np.mean(event))
        mean_rem = []
        for i, event in enumerate(ratio_per_merged_rem_event):
            mean_rem.append(np.mean(event))

        # get data smoothed only across REM events and only across NREM events
        # --------------------------------------------------------------------------------------------------------------
        ratio_merged_nrem_events_arr = np.hstack(ratio_per_merged_nrem_event)
        ratio_merged_nrem_events_arr_smooth = moving_average(a=ratio_merged_nrem_events_arr, n=n_moving_average_pop_vec)
        ratio_merged_rem_events_arr = np.hstack(ratio_per_merged_rem_event)
        ratio_merged_rem_events_arr_smooth = moving_average(a=ratio_merged_rem_events_arr, n=n_moving_average_pop_vec)

        # compute oscillations of REM/NREM periods
        # --------------------------------------------------------------------------------------------------------------
        # moving window
        window_size = 100
        step_size = 100

        window_start = np.arange(0, ratio_merged_rem_events_arr_smooth.shape[0] - window_size, step_size)
        min_max_rem = []
        std_rem = []
        for w_s in window_start:
            min_max_rem.append(max(ratio_merged_rem_events_arr_smooth[w_s:w_s + step_size])
                               - min(ratio_merged_rem_events_arr_smooth[w_s:w_s + step_size]))
            std_rem.append(np.std(ratio_merged_rem_events_arr_smooth[w_s:w_s + step_size]))
        window_start = np.arange(0, ratio_merged_nrem_events_arr_smooth.shape[0] - window_size, step_size)
        min_max_nrem = []
        std_nrem = []
        for w_s in window_start:
            min_max_nrem.append(max(ratio_merged_nrem_events_arr_smooth[w_s:w_s + step_size])
                                - min(ratio_merged_nrem_events_arr_smooth[w_s:w_s + step_size]))
            std_nrem.append(np.std(ratio_merged_nrem_events_arr_smooth[w_s:w_s + step_size]))

        if plotting:
            # check which phase came first
            if rem_nrem_events_label[0] == 1:
                # first event was a rem event
                rem_x_axis = np.arange(0, 2 * len(ds_rem), 2)
                nrem_x_axis = np.arange(1, 2 * len(ds_nrem) + 1, 2)
            elif rem_nrem_events_label[0] == 0:
                # first event was a nrem event
                nrem_x_axis = np.arange(0, 2 * len(ds_nrem), 2)
                rem_x_axis = np.arange(1, 2 * len(ds_rem) + 1, 2)


            # plotting
            fig = plt.figure()
            ax = fig.add_subplot()
            start = 0
            for event, label in zip(ratio_rem_nrem_events_smooth, rem_nrem_events_label):
                event_length = event.shape[0]
                if label:
                    ax.plot(np.arange(start, start + event_length), event, c="r", label="REM")
                else:
                    ax.plot(np.arange(start, start + event_length), event, c="b", label="NREM")
                start += event_length
            handles, labels = ax.get_legend_handles_labels()
            by_label = OrderedDict(zip(labels, handles))
            ax.legend(by_label.values(), by_label.keys())

            plt.grid(axis="y")
            plt.ylabel("sim_ratio")

            plt.xlabel("Duration (h)")
            plt.yticks([-1, -0.5, 0, 0.5, 1],["-1","-0.5","0","0.5","1"])
            plt.xticks([0,start/3, 2*start/3, start], ["0", "7", "14", "21"])
            plt.xlim(0,start)
            plt.rcParams['svg.fonttype'] = 'none'
            # plt.savefig("rem_nrem.svg", transparent="True")
            plt.show()

            fig = plt.figure()
            ax = fig.add_subplot()
            start = 0
            for event, label in zip(ratio_rem_nrem_events_smooth, rem_nrem_events_label):
                event_length = event.shape[0]
                if label:
                    ax.plot(np.arange(start, start + event_length), event, c="r", label="REM")
                else:
                    ax.plot(np.arange(start, start + event_length), event, c="b", label="NREM")
                start += event_length
            handles, labels = ax.get_legend_handles_labels()
            by_label = OrderedDict(zip(labels, handles))
            plt.xlim(101600,102879)
            plt.ylim(-0.34, -0.14)
            plt.xticks([])
            plt.yticks([])
            plt.rcParams['svg.fonttype'] = 'none'
            #plt.savefig("rem_nrem_zoom.svg", transparent="True")
            plt.show()
        else:
            return ds_rem_cum[-1], ds_nrem_cum[-1]

    def memory_drift_plot_rem_nrem_delta_score(self, template_type, measure="normalized_ratio", pre_file_name=None,
                                               post_file_name=None, n_moving_average_pop_vec=20,
                                               rem_pop_vec_threshold=10, plotting=False, cells_to_use="all",
                                               save_fig=False):
        """
        detailed memory drift analysis: opposing effects of NREM/REM etc.

        :param template_type: which template to use ("phmm" or "ising")
        :type template_type: str
        :param measure: which measure to use to compare PRE and POST similarity (model_evidence, log_like_ratio,
                        normalized_ratio
        :type measure: str
        :param pre_file_name: name of PRE template, if None --> default is used
        :type pre_file_name: str
        :param post_file_name: name of POST template, if None --> default is used
        :type post_file_name: str_decoding_similarity_temporal
        :param n_moving_average_pop_vec: how much smoothing to apply across population vectors
        :type n_moving_average_pop_ - np.mean(dec, axis=0)[0]vec: int
        :param rem_pop_vec_threshold: minimum length of REM epochs (shorter ones are discarded)
        :type rem_pop_vec_threshold: int
        :param plotting: whether to plot
        :type plotting: bool
        :param only_stable_cells: whether to only use stable cells
        :type only_stable_cells: bool
        :return: ds_rem_sum, ds_nrem_sum, ratio_per_merged_nrem_event, ratio_per_merged_rem_event
        :rtype:
        """

        # get length of sleep in seconds

        len_sleep = []
        for l_s in self.long_sleep:
            len_sleep.append(l_s.get_duration_sec())

        len_sleep = np.sum(np.array(len_sleep))
        len_sleep_h = len_sleep/60/60

        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_rem_event, event_times_rem, event_lengths_rem, event_duration_rem_in_s, _, _ = \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold, measure=measure,
                                                     cells_to_use=cells_to_use)

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_nrem_event, event_times_nrem, event_lengths_nrem, event_duration_nrem_in_s, _ ,_ = \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="nrem",
                                                     pop_vec_threshold=2, measure=measure,
                                                     cells_to_use=cells_to_use)

        # combine rem and nrem data to order it in the right way (chronologically)
        # --------------------------------------------------------------------------------------_decoding_similarity_temporal------------------------

        all_events_ratio = ratio_per_rem_event + ratio_per_nrem_event
        all_events_length = event_lengths_rem + event_lengths_nrem
        all_events_length_s = np.hstack((event_duration_rem_in_s, event_duration_nrem_in_s))
        labels_events = np.zeros(len(ratio_per_rem_event)+len(ratio_per_nrem_event))
        labels_events[:len(ratio_per_rem_event)] = 1
        all_times = np.hstack((event_times_rem[:, 0], event_times_nrem[:,0]))
        all_end_times = np.hstack((event_times_rem[:, 1], event_times_nrem[:, 2]))

        # sort events according to time
        sorted_events_ratio_list = [x for _, x in sorted(zip(all_times, all_events_ratio))]
        sorted_labels_events = [x for _, x in sorted(zip(all_times, labels_events))]
        sorted_lengths = [x for _, x in sorted(zip(all_times, all_events_length))]
        sorted_lengths_s = [x for _, x in sorted(zip(all_times, all_events_length_s))]
        sorted_pop_vec_ratio = np.hstack(sorted_events_ratio_list)
        sorted_times = [x for x in sorted(all_times)]
        sorted_end_times = [x for _, x in sorted(zip(all_times, all_end_times))]

        # compute labels per population vector --> rem events = 1, nrem events = 0
        # --------------------------------------------------------------------------------------------------------------
        labels_per_pop_vec = []
        for len_event, label in zip(sorted_lengths, sorted_labels_events):
            if label:
                labels_per_pop_vec.extend(np.ones(len_event))
            elif label == 0:
                labels_per_pop_vec.extend(np.zeros(len_event))

        labels_per_pop_vec = np.array(labels_per_pop_vec)

        # concatenate events that have the same label
        # --------------------------------------------------------------------------------------------------------------
        sorted_labels_events = np.array(sorted_labels_events)
        label_change = np.diff(sorted_labels_events)
        merged_events_length_s = []
        merged_events_labels = []
        merged_events_times = []
        first = 0
        while True:
            dat = label_change[first:]
            if first > label_change.shape[0]:
                break
            if sorted_labels_events[first] == 0:
                # nrem event
                # check if this is the last event
                if np.where(dat==1)[0].shape[0] == 0:
                    trans = label_change.shape[0] - first + 1
                else:
                    trans = np.where(dat == 1)[0][0]+1
                merged_events_length_s.append(np.sum(sorted_lengths_s[first:first+trans]))
                merged_events_labels.append(np.unique(sorted_labels_events[first:first + trans]))
                merged_events_times.append([sorted_times[first], sorted_end_times[first + trans - 1]])
            elif sorted_labels_events[first] == 1:
                # rem event
                # check if this is the last event
                if np.where(dat==-1)[0].shape[0] == 0:
                    trans = label_change.shape[0] - first + 1
                else:
                    trans = np.where(dat == -1)[0][0]+1
                merged_events_length_s.append(np.sum(sorted_lengths_s[first:first+trans]))
                merged_events_labels.append(np.unique(sorted_labels_events[first:first + trans]))
                merged_events_times.append([sorted_times[first], sorted_end_times[first+trans-1]])
            first += trans

        merged_events_labels = np.squeeze(np.array(merged_events_labels))
        merged_events_length_s = np.array(merged_events_length_s)
        merged_events_times = np.vstack(merged_events_times)

        # merged_rem_event_times = merged_events_times[merged_events_labels == 1]
        # merged_nrem_event_times = merged_events_times[merged_events_labels == 0]
        merged_events_length_s = merged_events_times[:,1]-merged_events_times[:,0]


        # concatenate population vectors that have the same label: TODO: there might be a better way to combine this
        # step and the previous step
        # --------------------------------------------------------------------------------------------------------------
        new_event_time_stamps = np.abs(np.diff(labels_per_pop_vec))

        # need to add one element at the beginning
        new_event_time_stamps = np.insert(new_event_time_stamps, 0, 1)
        start = np.nonzero(new_event_time_stamps==1)[0]
        end = start[1:]
        end = np.append(end, new_event_time_stamps.shape[0])

        # only select data (pre_post ratio) from population vectors that belong to selected rem/nrem events
        # --> results per merged event
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_merged_rem_event = []
        ratio_per_merged_nrem_event = []
        ratio_rem_nrem_events = []
        rem_nrem_events_label = []
        ratio_rem_nrem_pop_vec = []
        rem_nrem_pop_vec_label = []

        for start_event, end_event in zip(start, end):
            # rem event
            if labels_per_pop_vec[start_event + 1] == 1:
                ratio_per_merged_rem_event.append(sorted_pop_vec_ratio[start_event:end_event])
            # nrem event
            else:
                ratio_per_merged_nrem_event.append(sorted_pop_vec_ratio[start_event:end_event])

            ratio_rem_nrem_events.append(sorted_pop_vec_ratio[start_event:end_event])
            rem_nrem_events_label.extend([labels_per_pop_vec[start_event + 1]])

            ratio_rem_nrem_pop_vec.extend(sorted_pop_vec_ratio[start_event:end_event])
            rem_nrem_pop_vec_label.extend(labels_per_pop_vec[start_event:end_event])


        # smooth across population vectors --> that means also across REM/NREM epochs
        # --------------------------------------------------------------------------------------------------------------
        len_new_events = [x.shape[0] for x in ratio_rem_nrem_events]
        ratio_per_pop_vec_new = np.hstack(ratio_rem_nrem_events)
        ratio_per_pop_vec_new_smooth = moving_average(a=np.array(ratio_per_pop_vec_new), n=n_moving_average_pop_vec)

        ratio_rem_nrem_events_smooth = []
        first = 0
        for event_id in range(len(len_new_events)):
            # check if data is still available (due to smoothing --> data gets shorter)
            # if not --> leave loop
            if first >= ratio_per_pop_vec_new_smooth.shape[0]:
                break
            end_event = min(first + len_new_events[event_id], ratio_per_pop_vec_new_smooth.shape[0])
            ratio_rem_nrem_events_smooth.append(ratio_per_pop_vec_new_smooth[first:end_event])
            first += len_new_events[event_id]

        # need to delete indices of events that happend after the last data point after smoothing
        rem_nrem_events_label = rem_nrem_events_label[:len(ratio_rem_nrem_events_smooth)]

        # get rem data of merged events after smoothing
        # --------------------------------------------------------------------------------------------------------------
        rem_events_indices = np.squeeze(np.argwhere(np.array(rem_nrem_events_label) == 1))
        ds_rem = []
        ds_rem_smoothed_within = []
        merged_events_rem_length = []
        ratio_rem_events_smooth = []
        for rem_index in rem_events_indices:
            ds_rem.append(ratio_rem_nrem_events_smooth[rem_index][-1]-ratio_rem_nrem_events_smooth[rem_index][0])
            smooth_event = moving_average(a=ratio_rem_nrem_events[rem_index], n=10)
            ds_rem_smoothed_within.append(smooth_event[-1]-smooth_event[0])
            merged_events_rem_length.append(ratio_rem_nrem_events_smooth[rem_index].shape[0])
            ratio_rem_events_smooth.append(ratio_rem_nrem_events_smooth[rem_index])

        # get nrem data of merged events after smoothing
        # --------------------------------------------------------------------------------------------------------------
        nrem_events_indices = np.squeeze(np.argwhere(np.array(rem_nrem_events_label) == 0))
        ds_nrem = []
        ds_nrem_smoothed_within = []
        merged_events_nrem_length = []
        ratio_nrem_events_smooth = []
        for nrem_index in nrem_events_indices:
            ds_nrem.append(ratio_rem_nrem_events_smooth[nrem_index][-1]-ratio_rem_nrem_events_smooth[nrem_index][0])
            merged_events_nrem_length.append(ratio_rem_nrem_events_smooth[nrem_index].shape[0])
            ratio_nrem_events_smooth.append(ratio_rem_nrem_events_smooth[nrem_index])

        # compute delta score sum & cum sum
        # --------------------------------------------------------------------------------------------------------------
        ds_nrem_sum_smoothed_within = np.sum(np.array(ds_nrem_smoothed_within))
        ds_rem_sum_smoothed_within = np.sum(np.array(ds_rem_smoothed_within))
        ds_nrem_sum = np.sum(np.array(ds_nrem))
        ds_rem_sum = np.sum(np.array(ds_rem))
        ds_nrem_cum = np.cumsum(np.array(ds_nrem))
        ds_rem_cum = np.cumsum(np.array(ds_rem))

        # compute cross correlation of delta scores NREM/REM
        # --------------------------------------------------------------------------------------------------------------
        min_len = min(len(ds_rem), len(ds_nrem))
        ds_rem_arr = np.array(ds_rem)[:min_len]
        ds_nrem_arr = np.array(ds_nrem)[:min_len]
        corr_list_pos = []
        corr_list_neg = []
        shift_array_cross_corr = [0, 1, 2, 3, 4, 5, 6]
        for shift in shift_array_cross_corr:
            corr_list_pos.append(
                np.round(pearsonr(ds_rem_arr[shift:], ds_nrem_arr[:ds_nrem_arr.shape[0] - shift])[0], 2))
            corr_list_neg.append(
                np.round(pearsonr(ds_nrem_arr[shift:], ds_rem_arr[:ds_rem_arr.shape[0] - shift])[0], 2))
        corr_list = np.hstack((np.flip(np.array(corr_list_neg)), np.array(corr_list_pos)))

        # get mean score per event
        # --------------------------------------------------------------------------------------------------------------
        mean_nrem = []
        for i, event in enumerate(ratio_per_merged_nrem_event):
            mean_nrem.append(np.mean(event))
        mean_rem = []
        for i, event in enumerate(ratio_per_merged_rem_event):
            mean_rem.append(np.mean(event))

        # get data smoothed only across REM events and only across NREM events
        # --------------------------------------------------------------------------------------------------------------
        ratio_merged_nrem_events_arr = np.hstack(ratio_per_merged_nrem_event)
        ratio_merged_nrem_events_arr_smooth = moving_average(a=ratio_merged_nrem_events_arr, n=n_moving_average_pop_vec)
        ratio_merged_rem_events_arr = np.hstack(ratio_per_merged_rem_event)
        ratio_merged_rem_events_arr_smooth = moving_average(a=ratio_merged_rem_events_arr, n=n_moving_average_pop_vec)

        # compute oscillations of REM/NREM periods
        # --------------------------------------------------------------------------------------------------------------
        # moving window
        window_size = 100
        step_size = 100

        window_start = np.arange(0, ratio_merged_rem_events_arr_smooth.shape[0] - window_size, step_size)
        min_max_rem = []
        std_rem = []
        for w_s in window_start:
            min_max_rem.append(max(ratio_merged_rem_events_arr_smooth[w_s:w_s + step_size])
                               - min(ratio_merged_rem_events_arr_smooth[w_s:w_s + step_size]))
            std_rem.append(np.std(ratio_merged_rem_events_arr_smooth[w_s:w_s + step_size]))
        window_start = np.arange(0, ratio_merged_nrem_events_arr_smooth.shape[0] - window_size, step_size)
        min_max_nrem = []
        std_nrem = []
        for w_s in window_start:
            min_max_nrem.append(max(ratio_merged_nrem_events_arr_smooth[w_s:w_s + step_size])
                                - min(ratio_merged_nrem_events_arr_smooth[w_s:w_s + step_size]))
            std_nrem.append(np.std(ratio_merged_nrem_events_arr_smooth[w_s:w_s + step_size]))

        if plotting or save_fig:
            # check which phase came first
            if rem_nrem_events_label[0] == 1:
                # first event was a rem event
                rem_x_axis = np.arange(0, 2 * len(ds_rem), 2)
                nrem_x_axis = np.arange(1, 2 * len(ds_nrem) + 1, 2)
            elif rem_nrem_events_label[0] == 0:
                # first event was a nrem event
                nrem_x_axis = np.arange(0, 2 * len(ds_nrem), 2)
                rem_x_axis = np.arange(1, 2 * len(ds_rem) + 1, 2)

            if save_fig:
                plt.style.use('default')
            fig = plt.figure(figsize=(5,3))
            ax = fig.add_subplot()
            plt.plot(rem_x_axis, ds_rem, marker=".", label="REM", color="r")
            plt.plot(nrem_x_axis, ds_nrem, marker=".", label="NREM", color="b")
            plt.legend()
            plt.hlines(0, 0, rem_x_axis.shape[0] + nrem_x_axis.shape[0], color="gray")
            plt.ylabel("Delta score per epoch")
            #plt.title("PER EVENT DELTA SCORE")
            plt.xlim(0, len(ds_rem)+len(ds_nrem))
            plt.xticks([0, (len(ds_rem)+len(ds_nrem))/2, len(ds_rem)+len(ds_nrem)],
                       [0,np.round(len_sleep_h/2,0).astype(int), np.round(len_sleep_h,0).astype(int)])
            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig("delta_score_"+template_type+".svg", transparent="True")
            else:
                plt.show()
            plt.close()
            fig = plt.figure(figsize=(5,2))
            ax = fig.add_subplot()
            plt.plot(ds_nrem_cum, color="b", label="NREM")
            plt.plot(ds_rem_cum, color="r", label="REM")
            plt.xlabel("Duration (h)")
            plt.legend()
            plt.ylabel("Cumulative sum")
            plt.xlim(0, max(len(ds_nrem_cum), len(ds_rem_cum)))
            plt.yticks([min(ds_rem_cum), 0, max(ds_nrem_cum)],[np.round(min(ds_rem_cum),0).astype(int), 0, np.round(max(ds_nrem_cum),0).astype(int)])
            plt.ylim(min(ds_rem_cum), max(ds_nrem_cum))
            plt.xticks([0, (max(len(ds_nrem_cum), len(ds_rem_cum)))/2, max(len(ds_nrem_cum), len(ds_rem_cum))],
                       [0,np.round(len_sleep_h/2,0).astype(int), np.round(len_sleep_h,0).astype(int)])
            plt.hlines(0, 0, rem_x_axis.shape[0] + nrem_x_axis.shape[0], color="gray")
            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig("cumulative_delta_score_"+template_type+".svg", transparent="True")
            else:
                plt.show()

        else:
            return ds_rem_cum[-1], ds_nrem_cum[-1]

    def memory_drift_neighbouring_epochs(self, template_type, measure="normalized_ratio", pre_file_name=None,
                                              post_file_name=None, n_moving_average_pop_vec=20,
                                              rem_pop_vec_threshold=10, plotting=False, cells_to_use="all",
                                         first_type="nrem"):
        """
        detailed memory drift analysis: opposing effects of NREM/REM etc.

        :param template_type: which template to use ("phmm" or "ising")
        :type template_type: str
        :param measure: which measure to use to compare PRE and POST similarity (model_evidence, log_like_ratio,
                        normalized_ratio
        :type measure: str
        :param pre_file_name: name of PRE template, if None --> default is used
        :type pre_file_name: str
        :param post_file_name: name of POST template, if None --> default is used
        :type post_file_name: str_decoding_similarity_temporal
        :param n_moving_average_pop_vec: how much smoothing to apply across population vectors
        :type n_moving_average_pop_ - np.mean(dec, axis=0)[0]vec: int
        :param rem_pop_vec_threshold: minimum length of REM epochs (shorter ones are discarded)
        :type rem_pop_vec_threshold: int
        :param plotting: whether to plot
        :type plotting: bool
        :param only_stable_cells: whether to only use stable cells
        :type only_stable_cells: bool
        :return: ds_rem_sum, ds_nrem_sum, ratio_per_merged_nrem_event, ratio_per_merged_rem_event
        :rtype:
        """

        # get length of sleep in seconds

        len_sleep = []
        for l_s in self.long_sleep:
            len_sleep.append(l_s.get_duration_sec())

        len_sleep = np.sum(np.array(len_sleep))
        len_sleep_h = len_sleep/60/60

        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_rem_event, event_times_rem, event_lengths_rem, event_duration_rem_in_s, _, _ = \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold, measure=measure,
                                                     cells_to_use=cells_to_use)

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_nrem_event, event_times_nrem, event_lengths_nrem, event_duration_nrem_in_s, _ ,_ = \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="nrem",
                                                     pop_vec_threshold=2, measure=measure,
                                                     cells_to_use=cells_to_use)

        # combine rem and nrem data to order it in the right way (chronologically)
        # --------------------------------------------------------------------------------------_decoding_similarity_temporal------------------------

        all_events_ratio = ratio_per_rem_event + ratio_per_nrem_event
        all_events_length = event_lengths_rem + event_lengths_nrem
        all_events_length_s = np.hstack((event_duration_rem_in_s, event_duration_nrem_in_s))
        labels_events = np.zeros(len(ratio_per_rem_event)+len(ratio_per_nrem_event))
        labels_events[:len(ratio_per_rem_event)] = 1
        all_times = np.hstack((event_times_rem[:, 0], event_times_nrem[:,0]))
        all_end_times = np.hstack((event_times_rem[:, 1], event_times_nrem[:, 2]))

        # sort events according to time
        sorted_events_ratio_list = [x for _, x in sorted(zip(all_times, all_events_ratio))]
        sorted_labels_events = [x for _, x in sorted(zip(all_times, labels_events))]
        sorted_lengths = [x for _, x in sorted(zip(all_times, all_events_length))]
        sorted_lengths_s = [x for _, x in sorted(zip(all_times, all_events_length_s))]
        sorted_pop_vec_ratio = np.hstack(sorted_events_ratio_list)
        sorted_times = [x for x in sorted(all_times)]
        sorted_end_times = [x for _, x in sorted(zip(all_times, all_end_times))]

        # compute labels per population vector --> rem events = 1, nrem events = 0
        # --------------------------------------------------------------------------------------------------------------
        labels_per_pop_vec = []
        for len_event, label in zip(sorted_lengths, sorted_labels_events):
            if label:
                labels_per_pop_vec.extend(np.ones(len_event))
            elif label == 0:
                labels_per_pop_vec.extend(np.zeros(len_event))

        labels_per_pop_vec = np.array(labels_per_pop_vec)

        # concatenate events that have the same label
        # --------------------------------------------------------------------------------------------------------------
        sorted_labels_events = np.array(sorted_labels_events)
        label_change = np.diff(sorted_labels_events)
        merged_events_length_s = []
        merged_events_labels = []
        merged_events_times = []
        first = 0
        while True:
            dat = label_change[first:]
            if first > label_change.shape[0]:
                break
            if sorted_labels_events[first] == 0:
                # nrem event
                # check if this is the last event
                if np.where(dat==1)[0].shape[0] == 0:
                    trans = label_change.shape[0] - first + 1
                else:
                    trans = np.where(dat == 1)[0][0]+1
                merged_events_length_s.append(np.sum(sorted_lengths_s[first:first+trans]))
                merged_events_labels.append(np.unique(sorted_labels_events[first:first + trans]))
                merged_events_times.append([sorted_times[first], sorted_end_times[first + trans - 1]])
            elif sorted_labels_events[first] == 1:
                # rem event
                # check if this is the last event
                if np.where(dat==-1)[0].shape[0] == 0:
                    trans = label_change.shape[0] - first + 1
                else:
                    trans = np.where(dat == -1)[0][0]+1
                merged_events_length_s.append(np.sum(sorted_lengths_s[first:first+trans]))
                merged_events_labels.append(np.unique(sorted_labels_events[first:first + trans]))
                merged_events_times.append([sorted_times[first], sorted_end_times[first+trans-1]])
            first += trans

        merged_events_labels = np.squeeze(np.array(merged_events_labels))
        merged_events_length_s = np.array(merged_events_length_s)
        merged_events_times = np.vstack(merged_events_times)

        # merged_rem_event_times = merged_events_times[merged_events_labels == 1]
        # merged_nrem_event_times = merged_events_times[merged_events_labels == 0]
        merged_events_length_s = merged_events_times[:,1]-merged_events_times[:,0]


        # concatenate population vectors that have the same label: TODO: there might be a better way to combine this
        # step and the previous step
        # --------------------------------------------------------------------------------------------------------------
        new_event_time_stamps = np.abs(np.diff(labels_per_pop_vec))

        # need to add one element at the beginning
        new_event_time_stamps = np.insert(new_event_time_stamps, 0, 1)
        start = np.nonzero(new_event_time_stamps==1)[0]
        end = start[1:]
        end = np.append(end, new_event_time_stamps.shape[0])

        # only select data (pre_post ratio) from population vectors that belong to selected rem/nrem events
        # --> results per merged event
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_merged_rem_event = []
        ratio_per_merged_nrem_event = []
        ratio_rem_nrem_events = []
        rem_nrem_events_label = []
        ratio_rem_nrem_pop_vec = []
        rem_nrem_pop_vec_label = []

        for start_event, end_event in zip(start, end):
            # rem event
            if labels_per_pop_vec[start_event + 1] == 1:
                ratio_per_merged_rem_event.append(sorted_pop_vec_ratio[start_event:end_event])
            # nrem event
            else:
                ratio_per_merged_nrem_event.append(sorted_pop_vec_ratio[start_event:end_event])

            ratio_rem_nrem_events.append(sorted_pop_vec_ratio[start_event:end_event])
            rem_nrem_events_label.extend([labels_per_pop_vec[start_event + 1]])

            ratio_rem_nrem_pop_vec.extend(sorted_pop_vec_ratio[start_event:end_event])
            rem_nrem_pop_vec_label.extend(labels_per_pop_vec[start_event:end_event])

        # smooth across population vectors --> that means also across REM/NREM epochs
        # --------------------------------------------------------------------------------------------------------------
        len_new_events = [x.shape[0] for x in ratio_rem_nrem_events]
        ratio_per_pop_vec_new = np.hstack(ratio_rem_nrem_events)
        ratio_per_pop_vec_new_smooth = moving_average(a=np.array(ratio_per_pop_vec_new), n=n_moving_average_pop_vec)

        ratio_rem_nrem_events_smooth = []
        first = 0
        for event_id in range(len(len_new_events)):
            # check if data is still available (due to smoothing --> data gets shorter)
            # if not --> leave loop
            if first >= ratio_per_pop_vec_new_smooth.shape[0]:
                break
            end_event = min(first + len_new_events[event_id], ratio_per_pop_vec_new_smooth.shape[0])
            ratio_rem_nrem_events_smooth.append(ratio_per_pop_vec_new_smooth[first:end_event])
            first += len_new_events[event_id]

        # need to delete indices of events that happend after the last data point after smoothing
        rem_nrem_events_label = rem_nrem_events_label[:len(ratio_rem_nrem_events_smooth)]

        # get rem data of merged events after smoothing
        # --------------------------------------------------------------------------------------------------------------
        rem_events_indices = np.squeeze(np.argwhere(np.array(rem_nrem_events_label) == 1))
        ds_rem = []
        ds_rem_smoothed_within = []
        merged_events_rem_length = []
        ratio_rem_events_smooth = []
        for rem_index in rem_events_indices:
            ds_rem.append(ratio_rem_nrem_events_smooth[rem_index][-1]-ratio_rem_nrem_events_smooth[rem_index][0])
            smooth_event = moving_average(a=ratio_rem_nrem_events[rem_index], n=10)
            ds_rem_smoothed_within.append(smooth_event[-1]-smooth_event[0])
            merged_events_rem_length.append(ratio_rem_nrem_events_smooth[rem_index].shape[0])
            ratio_rem_events_smooth.append(ratio_rem_nrem_events_smooth[rem_index])

        # get nrem data of merged events after smoothing
        # --------------------------------------------------------------------------------------------------------------
        nrem_events_indices = np.squeeze(np.argwhere(np.array(rem_nrem_events_label) == 0))
        ds_nrem = []
        ds_nrem_smoothed_within = []
        merged_events_nrem_length = []
        ratio_nrem_events_smooth = []
        for nrem_index in nrem_events_indices:
            ds_nrem.append(ratio_rem_nrem_events_smooth[nrem_index][-1]-ratio_rem_nrem_events_smooth[nrem_index][0])
            merged_events_nrem_length.append(ratio_rem_nrem_events_smooth[nrem_index].shape[0])
            ratio_nrem_events_smooth.append(ratio_rem_nrem_events_smooth[nrem_index])

        # compute delta score sum & cum sum
        # --------------------------------------------------------------------------------------------------------------
        ds_nrem_sum_smoothed_within = np.sum(np.array(ds_nrem_smoothed_within))
        ds_rem_sum_smoothed_within = np.sum(np.array(ds_rem_smoothed_within))
        ds_nrem_sum = np.sum(np.array(ds_nrem))
        ds_rem_sum = np.sum(np.array(ds_rem))
        ds_nrem_cum = np.cumsum(np.array(ds_nrem))
        ds_rem_cum = np.cumsum(np.array(ds_rem))

        # compute cross correlation of delta scores NREM/REM
        # --------------------------------------------------------------------------------------------------------------
        min_len = min(len(ds_rem), len(ds_nrem))
        ds_rem_arr = np.array(ds_rem)[:min_len]
        ds_nrem_arr = np.array(ds_nrem)[:min_len]
        corr_list_pos = []
        corr_list_neg = []
        shift_array_cross_corr = [0, 1, 2, 3, 4, 5, 6]
        for shift in shift_array_cross_corr:
            corr_list_pos.append(
                np.round(pearsonr(ds_rem_arr[shift:], ds_nrem_arr[:ds_nrem_arr.shape[0] - shift])[0], 2))
            corr_list_neg.append(
                np.round(pearsonr(ds_nrem_arr[shift:], ds_rem_arr[:ds_rem_arr.shape[0] - shift])[0], 2))
        corr_list = np.hstack((np.flip(np.array(corr_list_neg)), np.array(corr_list_pos)))

        # get mean score per event
        # --------------------------------------------------------------------------------------------------------------
        mean_nrem = []
        for i, event in enumerate(ratio_per_merged_nrem_event):
            mean_nrem.append(np.mean(event))
        mean_rem = []
        for i, event in enumerate(ratio_per_merged_rem_event):
            mean_rem.append(np.mean(event))

        # get data smoothed only across REM events and only across NREM events
        # --------------------------------------------------------------------------------------------------------------
        ratio_merged_nrem_events_arr = np.hstack(ratio_per_merged_nrem_event)
        ratio_merged_nrem_events_arr_smooth = moving_average(a=ratio_merged_nrem_events_arr, n=n_moving_average_pop_vec)
        ratio_merged_rem_events_arr = np.hstack(ratio_per_merged_rem_event)
        ratio_merged_rem_events_arr_smooth = moving_average(a=ratio_merged_rem_events_arr, n=n_moving_average_pop_vec)

        # compute oscillations of REM/NREM periods
        # --------------------------------------------------------------------------------------------------------------
        # moving window
        window_size = 100
        step_size = 100

        window_start = np.arange(0, ratio_merged_rem_events_arr_smooth.shape[0] - window_size, step_size)
        min_max_rem = []
        std_rem = []
        for w_s in window_start:
            min_max_rem.append(max(ratio_merged_rem_events_arr_smooth[w_s:w_s + step_size])
                               - min(ratio_merged_rem_events_arr_smooth[w_s:w_s + step_size]))
            std_rem.append(np.std(ratio_merged_rem_events_arr_smooth[w_s:w_s + step_size]))
        window_start = np.arange(0, ratio_merged_nrem_events_arr_smooth.shape[0] - window_size, step_size)
        min_max_nrem = []
        std_nrem = []
        for w_s in window_start:
            min_max_nrem.append(max(ratio_merged_nrem_events_arr_smooth[w_s:w_s + step_size])
                                - min(ratio_merged_nrem_events_arr_smooth[w_s:w_s + step_size]))
            std_nrem.append(np.std(ratio_merged_nrem_events_arr_smooth[w_s:w_s + step_size]))

        if not first_type == "random":
            if first_type == "nrem":
                # check which phase came first
                if rem_nrem_events_label[0] == 1:
                    # first event was a rem event
                    rem_x_axis = np.arange(0, 2 * len(ds_rem), 2)
                    nrem_x_axis = np.arange(1, 2 * len(ds_nrem) + 1, 2)
                    ds_rem_arr = ds_rem_arr[1:]
                    ds_nrem_arr = ds_nrem_arr[:-1]
                    print("First epoch: REM --> deleted first REM and last NREM epoch")
                elif rem_nrem_events_label[0] == 0:
                    # first event was a nrem event
                    nrem_x_axis = np.arange(0, 2 * len(ds_nrem), 2)
                    rem_x_axis = np.arange(1, 2 * len(ds_rem) + 1, 2)
                    print("First epoch: NREM --> nothing was done")
            elif first_type == "rem":
                # check which phase came first
                if rem_nrem_events_label[0] == 1:
                    # first event was a rem event
                    rem_x_axis = np.arange(0, 2 * len(ds_rem), 2)
                    nrem_x_axis = np.arange(1, 2 * len(ds_nrem) + 1, 2)
                    print("First epoch: REM --> nothing was done")
                elif rem_nrem_events_label[0] == 0:
                    # first event was a nrem event
                    nrem_x_axis = np.arange(0, 2 * len(ds_nrem), 2)
                    rem_x_axis = np.arange(1, 2 * len(ds_rem) + 1, 2)

                    ds_rem_arr = ds_rem_arr[:-1]
                    ds_nrem_arr = ds_nrem_arr[1:]
                    print("First epoch: NREM --> delete first NREM and last REM epoch")

        if plotting:
            def make_square_axes(ax):
                """Make an axes square in screen units.

                Should be called after plotting.
                """
                ax.set_aspect(1 / ax.get_data_ratio())

            plt.scatter(ds_rem_arr, ds_nrem_arr)
            plt.title("NEIGHBOURING PERIODS, R=" + str(np.round(pearsonr(ds_rem_arr, ds_nrem_arr)[0], 2)))
            plt.xlabel("DELTA SCORE REM")
            plt.ylabel("DELTA SCORE NREM")
            make_square_axes(plt.gca())
            plt.show()
        else:
            return ds_rem_arr, ds_nrem_arr

    # additional analysis
    # ------------------------------------------------------------------------------------------------------------------

    def memory_drift_parameter_influence(self, template_type, n_moving_average_pop_vec,
                                                   sleep_classification_method="std"):
        fig = plt.figure()
        ax = fig.add_subplot()
        for i, rem_pop_vec_threshold in enumerate([20, 50, 100, 150, 200, 300, 400, 500, 600]):
            ds_rem_sum, ds_nrem_sum, ratio_per_merged_nrem_event, ratio_per_merged_rem_event =\
                self.memory_drift(template_type=template_type, n_moving_average_pop_vec=n_moving_average_pop_vec,
                                  rem_pop_vec_threshold=rem_pop_vec_threshold,
                                  sleep_classification_method=sleep_classification_method)
            ax.scatter(rem_pop_vec_threshold, ds_nrem_sum, color="b", label="NREM", zorder=1000)
            ax.scatter(rem_pop_vec_threshold, ds_rem_sum, color="r", label="REM", zorder=1000)

        handles, labels = ax.get_legend_handles_labels()
        by_label = OrderedDict(zip(labels, handles))
        ax.legend(by_label.values(), by_label.keys())
        plt.grid()
        plt.ylabel("CUMULATIVE DELTA SCORE")
        plt.xlabel("REM THRESHOLD ( POP.VEC. < THRS. ARE DISCARDED)")
        plt.title("SMOOTHING, n="+str(n_moving_average_pop_vec))
        plt.show()

    def memory_drift_spatial_content_progression_swr(self, pHMM_file_pre, pHMM_file_post,
                                   plot_for_control=False, n_moving_average_swr=10, n_moving_average_pop_vec=40):

        print("SPATIAL CONTENT PROGRESSION \n")

        # get pre-post similarity
        pre_SWR_prob, post_SWR_prob, event_times = self.sleep_fam.content_based_memory_drift_phmm(
                        pHMM_file_pre=pHMM_file_pre,
                        pHMM_file_post=pHMM_file_post,
                        plot_for_control=plot_for_control, return_results=True)

        # get spatial information for each mode from PRE
        sparsity_pre, skaggs_pre = self.exploration_fam.phmm_mode_spatial_information_from_model(file_name=pHMM_file_pre,
                                                                                          plot_for_control=plot_for_control,
                                                                                         spatial_resolution=10)

        # get spatial information for each mode from POST
        sparsity_post, skaggs_post = self.exploration_novel.phmm_mode_spatial_information_from_model(file_name=pHMM_file_post,
                                                                                          plot_for_control=plot_for_control,
                                                                                         spatial_resolution=10)

        plt.hist(skaggs_post, label="POST MODES")
        plt.hist(skaggs_pre, color="r", alpha=0.5, label="PRE MODES")
        plt.xlabel("SKAGGS INFO")
        plt.ylabel("COUNTS")
        plt.legend()
        plt.show()

        # get model from PRE and transition matrix
        model_pre = self.exploration_fam.load_poisson_hmm(file_name=pHMM_file_pre)
        transmat_pre = model_pre.transmat_

        nr_modes_pre = pre_SWR_prob[0].shape[1]
        nr_modes_post = post_SWR_prob[0].shape[1]

        # per SWR results
        swr_pre_sparsity = []
        swr_pre_skaggs = []
        swr_post_sparsity = []
        swr_post_skaggs = []
        swr_pre_post_ratio = []
        swr_pre_prob = []
        swr_post_prob = []
        swr_seq_similarity = []
        swr_len_seq = []

        # per population vector results
        pop_vec_pre_post_ratio = []
        pre_seq_list = []
        post_seq_list = []
        pre_seq_list_prob = []
        post_seq_list_prob = []
        pop_vec_post_prob = []
        pop_vec_pre_prob = []

        # go trough all SWR
        for pre_array, post_array in zip(pre_SWR_prob, post_SWR_prob):

            # make sure that there is any data for the current SWR

            if pre_array.shape[0] > 0:
                pre_sequence = np.argmax(pre_array, axis=1)
                pre_sequence_prob = np.max(pre_array, axis=1)
                post_sequence = np.argmax(post_array, axis=1)
                post_sequence_prob = np.max(post_array, axis=1)
                pre_seq_list.extend(pre_sequence)
                post_seq_list.extend(post_sequence)
                pre_seq_list_prob.extend(pre_sequence_prob)
                post_seq_list_prob.extend(post_sequence_prob)

                # check how likely observed sequence is considering transitions from model (awake behavior)
                mode_before = pre_sequence[:-1]
                mode_after = pre_sequence[1:]
                transition_prob = 0
                # go trough each transition of the sequence
                for bef, aft in zip(mode_before, mode_after):
                    transition_prob += np.log(transmat_pre[bef, aft])

                swr_seq_similarity.append(np.exp(transition_prob))
                swr_len_seq.append(pre_sequence.shape[0])

                # per SWR computations
                # ----------------------------------------------------------------------------------------------------------
                # compute spatial information per SWR
                swr_pre_sparsity.append(np.mean(sparsity_pre[pre_sequence]))
                swr_pre_skaggs.append(np.mean(skaggs_pre[pre_sequence]))
                swr_post_sparsity.append(np.mean(sparsity_post[post_sequence]))
                swr_post_skaggs.append(np.mean(skaggs_post[post_sequence]))
                # arrays: [nr_pop_vecs_per_SWR, nr_time_spatial_time_bins]
                # get maximum value per population vector and take average across the SWR
                if pre_array.shape[0] > 0:
                    # save pre and post probabilities
                    swr_pre_prob.append(np.mean(np.max(pre_array, axis=1)))
                    swr_post_prob.append(np.mean(np.max(post_array, axis=1)))
                    # compute ratio
                    prob_pre = np.mean(np.max(pre_array, axis=1))
                    prob_post = np.mean(np.max(post_array, axis=1))
                    swr_pre_post_ratio.append((prob_post - prob_pre) / (prob_post + prob_pre))
                else:
                    swr_pre_prob.append(np.nan)
                    swr_post_prob.append(np.nan)
                    swr_pre_post_ratio.append(np.nan)

                # per population vector computations
                # ----------------------------------------------------------------------------------------------------------
                # compute per population vector similarity score
                prob_post = np.max(post_array, axis=1)
                prob_pre = np.max(pre_array, axis=1)
                pop_vec_pre_post_ratio.extend((prob_post - prob_pre) / (prob_post + prob_pre))
                pop_vec_pre_prob.extend(np.max(pre_array, axis=1))
                if pre_array.shape[0] > 0:
                    pop_vec_pre_prob.extend(np.max(pre_array, axis=1))
                    pop_vec_post_prob.extend(np.max(post_array, axis=1))
                else:
                    pop_vec_pre_prob.extend([np.nan])
                    pop_vec_post_prob.extend([np.nan])

        # smoothen per SWR info
        # --------------------------------------------------------------------------------------------------------------
        swr_pre_sparsity = moving_average(a=np.array(swr_pre_sparsity), n=n_moving_average_swr)
        swr_pre_skaggs = moving_average(a=np.array(swr_pre_skaggs), n=n_moving_average_swr)
        swr_post_skaggs = moving_average(a=np.array(swr_post_skaggs), n=n_moving_average_swr)
        swr_post_sparsity = moving_average(a=np.array(swr_post_sparsity), n=n_moving_average_swr)
        swr_pre_post_ratio = moving_average(a=np.array(swr_pre_post_ratio), n=n_moving_average_swr)

        # plot per SWR info
        # --------------------------------------------------------------------------------------------------------------
        plt.plot(swr_pre_post_ratio)
        plt.title("PRE-POST RATIO FOR EACH SWR: PHMM")
        plt.xlabel("SWR ID")
        plt.ylabel("PRE-POST SIMILARITY")
        plt.ylim(-1, 1)
        plt.grid()
        plt.show()

        plt.subplot(2,1,1)
        plt.plot(swr_pre_skaggs)
        plt.title("SPATIAL INFORMATION PER SWR: PRE, SKAGGS")
        plt.xlabel("SWR ID")
        plt.ylabel("AVG. SKAGGS INFO")
        plt.subplot(2,1,2)
        plt.plot(swr_pre_sparsity)
        plt.title("SPATIAL INFORMATION PER SWR: PRE, SPARSITY")
        plt.xlabel("SWR ID")
        plt.ylabel("AVG. SPARSITY")
        plt.show()

        plt.scatter(swr_pre_post_ratio, swr_pre_skaggs)
        plt.title("PER SWR CORRELATION: SCORE vs. SKAGGS\n"+str(pearsonr(swr_pre_post_ratio, swr_pre_skaggs)))
        plt.xlabel("SIMILARITY SCORE")
        plt.ylabel("SKAGGS INFORMATION")
        plt.show()
        plt.scatter(swr_pre_post_ratio, swr_pre_sparsity)
        plt.title("PER SWR CORRELATION: SCORE vs. SPARSITY\n"+str(pearsonr(swr_pre_post_ratio, swr_pre_sparsity)))
        plt.xlabel("SIMILARITY SCORE")
        plt.ylabel("SKAGGS SPARSITY")
        plt.show()

        plt.plot(swr_pre_skaggs, c="r", label="PRE")
        plt.plot(swr_post_skaggs, label="POST")
        plt.title("SPATIAL INFORMATION PER SWR: PRE, SKAGGS")
        plt.xlabel("SWR ID")
        plt.ylabel("AVG. SKAGGS INFO")
        plt.legend()
        plt.show()

        plt.plot(swr_pre_sparsity, c="r", label="PRE")
        plt.plot(swr_post_sparsity, label="POST")
        plt.title("SPATIAL INFORMATION PER SWR: PRE, SPARSITY")
        plt.xlabel("SWR ID")
        plt.ylabel("AVG. SPARSITY")
        plt.legend()
        plt.show()

        # sequence probability
        swr_seq_similarity = moving_average(a=np.array(swr_seq_similarity), n=n_moving_average_pop_vec)
        plt.plot(swr_seq_similarity)
        plt.title("PROBABILITY SWR PHMM MODE SEQUENCES \n USING AWAKE TRANSITION PROB. PRE")
        plt.ylabel("JOINT PROBABILITY")
        plt.xlabel("SWR ID")
        plt.show()

        # plot per population vector info
        # --------------------------------------------------------------------------------------------------------------
        pop_vec_pre_post_ratio = np.array(pop_vec_pre_post_ratio)
        # compute moving average to smooth signal
        pop_vec_pre_post_ratio_smooth = moving_average(a=pop_vec_pre_post_ratio, n=n_moving_average_pop_vec)
        plt.plot(pop_vec_pre_post_ratio_smooth)
        plt.title("PRE-POST RATIO FOR EACH POP. VECTOR: PHMM")
        plt.xlabel("POP.VEC. ID")
        plt.ylabel("PRE-POST SIMILARITY")
        plt.ylim(-1, 1)
        plt.grid()
        plt.show()

        # compute per mode info
        # --------------------------------------------------------------------------------------------------------------
        pre_seq = np.array(pre_seq_list)
        post_seq = np.array(post_seq_list)
        mode_score_mean_pre = np.zeros(pre_SWR_prob[0].shape[1])
        mode_score_std_pre = np.zeros(pre_SWR_prob[0].shape[1])
        mode_score_mean_post = np.zeros(post_SWR_prob[0].shape[1])
        mode_score_std_post = np.zeros(post_SWR_prob[0].shape[1])

        # go through all pre modes and check the average score
        for i in range(pre_SWR_prob[0].shape[1]):
            ind_sel = np.where(pre_seq == i)[0]
            if ind_sel.size == 0 or ind_sel.size == 1:
                mode_score_mean_pre[i] = np.nan
                mode_score_std_pre[i] = np.nan
            else:
                # delete all indices that are too large (becaue of moving average)
                ind_sel = ind_sel[ind_sel < pop_vec_pre_post_ratio.shape[0]]
                mode_score_mean_pre[i] = np.mean(pop_vec_pre_post_ratio[ind_sel])
                mode_score_std_pre[i] = np.std(pop_vec_pre_post_ratio[ind_sel])

        # go through all post modes and check the average score
        for i in range(post_SWR_prob[0].shape[1]):
            ind_sel = np.where(post_seq == i)[0]
            if ind_sel.size == 0 or ind_sel.size == 1:
                mode_score_mean_post[i] = np.nan
                mode_score_std_post[i] = np.nan
            else:
                # delete all indices that are too large (becaue of moving average)
                ind_sel = ind_sel[ind_sel < pop_vec_pre_post_ratio.shape[0]]
                mode_score_mean_post[i] = np.mean(pop_vec_pre_post_ratio[ind_sel])
                mode_score_std_post[i] = np.std(pop_vec_pre_post_ratio[ind_sel])

        low_score_modes = np.argsort(mode_score_mean_pre)
        # need to skip nans
        nr_nans = np.count_nonzero(np.isnan(mode_score_mean_pre))
        high_score_modes = np.flip(low_score_modes)[nr_nans:]

        plt.errorbar(range(pre_SWR_prob[0].shape[1]), mode_score_mean_pre, yerr=mode_score_std_pre, linestyle="")
        plt.scatter(range(pre_SWR_prob[0].shape[1]), mode_score_mean_pre)
        plt.title("PRE-POST SCORE PER MODE: PRE")
        plt.xlabel("MODE ID")
        plt.ylabel("PRE-POST SCORE: MEAN AND STD")
        plt.show()

        plt.errorbar(range(post_SWR_prob[0].shape[1]), mode_score_mean_post, yerr=mode_score_std_post, linestyle="")
        plt.scatter(range(post_SWR_prob[0].shape[1]), mode_score_mean_post)
        plt.title("PRE-POST SCORE PER MODE: POST")
        plt.xlabel("MODE ID")
        plt.ylabel("PRE-POST SCORE: MEAN AND STD")
        plt.show()

        plt.scatter(mode_score_mean_pre, sparsity_pre)
        plt.title("PER MODE: CORRELATION SPARSITY - PRE_POST SCORE\n"+
                  str(pearsonr(np.nan_to_num(mode_score_mean_pre), np.nan_to_num(sparsity_pre))))
        plt.xlabel("PRE_POST SCORE")
        plt.ylabel("MEAN SPARSITY")
        plt.show()
        plt.scatter(mode_score_mean_pre, skaggs_pre)
        plt.title("PER MODE: CORRELATION SKAGGS - PRE_POST SCORE\n"+
                  str(pearsonr(np.nan_to_num(mode_score_mean_pre), np.nan_to_num(skaggs_pre))))
        plt.xlabel("PRE_POST SCORE")
        plt.ylabel("MEAN SKAGGS INFO")
        plt.show()

        # check if modes get more often/less often reactivated over time
        pre_seq_list = np.array(pre_seq_list)
        nr_pop_vec = 20
        nr_windows = int(pre_seq_list.shape[0]/nr_pop_vec)
        occurence_modes = np.zeros((nr_modes_pre, nr_windows))
        for i in range(nr_windows):
            seq = pre_seq_list[i*nr_pop_vec:(i+1)*nr_pop_vec]
            mode, counts = np.unique(seq, return_counts=True)
            occurence_modes[mode,i] = counts

        plt.imshow(occurence_modes, interpolation='nearest', aspect='auto')
        plt.ylabel("MODE ID")
        plt.xlabel("WINDOW ID")
        a = plt.colorbar()
        a.set_label("#WINS/"+str(nr_pop_vec)+" POP. VEC. WINDOW")
        plt.title("OCCURENCE (#WINS) OF MODES IN WINDOWS OF FIXED LENGTH")
        plt.show()

    def memory_drift_spatial_content_progression_window(self, pHMM_file_pre, pHMM_file_post,
                                   plot_for_control=False, n_moving_average=10, sliding_window=True):

        print("SPATIAL CONTENT PROGRESSION USING WINDOWS \n")

        # get pre-post similarity
        pre_SWR_prob, post_SWR_prob, event_times = self.sleep_fam.content_based_memory_drift_phmm(
                        pHMM_file_pre=pHMM_file_pre,
                        pHMM_file_post=pHMM_file_post,
                        plot_for_control=plot_for_control, return_results=True)

        # get spatial information for each mode from PRE
        sparsity_pre, skaggs_pre = self.exploration_fam.phmm_mode_spatial_information_from_model(file_name=pHMM_file_pre,
                                                                                          plot_for_control=plot_for_control,
                                                                                         spatial_resolution=10)

        # get spatial information for each mode from POST
        sparsity_post, skaggs_post = self.exploration_novel.phmm_mode_spatial_information_from_model(file_name=pHMM_file_post,
                                                                                          plot_for_control=plot_for_control,
                                                                                         spatial_resolution=10)

        # get model from PRE and transition matrix
        model_pre = self.exploration_fam.load_poisson_hmm(file_name=pHMM_file_pre)
        transmat_pre = model_pre.transmat_


        pre_SWR_prob_arr = np.vstack(pre_SWR_prob)
        post_SWR_prob_arr = np.vstack(post_SWR_prob)

        win_pre_sparsity = []
        win_pre_skaggs = []
        win_post_sparsity = []
        win_post_skaggs = []
        win_pre_post_ratio = []
        win_seq_similarity = []


        # use sliding window to go over SWR pop. vec
        n_slid_win = 20
        overlap = 5

        if sliding_window:

            for i in np.arange(0,pre_SWR_prob_arr.shape[0] - n_slid_win +1, overlap):
                pre_array = pre_SWR_prob_arr[i:(i + n_slid_win)]
                pre_sequence = np.argmax(pre_array, axis=1)
                post_array = post_SWR_prob_arr[i:(i + n_slid_win)]
                post_sequence = np.argmax(post_array, axis=1)
                win_pre_sparsity.append(np.mean(sparsity_pre[pre_sequence]))
                win_pre_skaggs.append(np.mean(skaggs_pre[pre_sequence]))
                win_post_sparsity.append(np.mean(sparsity_post[post_sequence]))
                win_post_skaggs.append(np.mean(skaggs_post[post_sequence]))
                prob_pre = np.mean(np.max(pre_array, axis=1))
                prob_post = np.mean(np.max(post_array, axis=1))
                win_pre_post_ratio.append((prob_post - prob_pre) / (prob_post + prob_pre))

                # check how likely observed sequence is considering transitions from model (awake behavior)
                mode_before = pre_sequence[:-1]
                mode_after = pre_sequence[1:]
                transition_prob = 0
                # go trough each transition of the sequence
                for bef, aft in zip(mode_before, mode_after):
                    transition_prob += np.log(transmat_pre[bef, aft])

                win_seq_similarity.append(np.exp(transition_prob))

        else:

            for i in range(round(pre_SWR_prob_arr.shape[0]/n_slid_win)):
                pre_array = pre_SWR_prob_arr[i*n_slid_win:(i+1)*n_slid_win]
                pre_sequence = np.argmax(pre_array, axis=1)
                post_array = post_SWR_prob_arr[i*n_slid_win:(i+1)*n_slid_win]
                post_sequence = np.argmax(post_array, axis=1)
                win_pre_sparsity.append(np.mean(sparsity_pre[pre_sequence]))
                win_pre_skaggs.append(np.mean(skaggs_pre[pre_sequence]))
                win_post_sparsity.append(np.mean(sparsity_post[post_sequence]))
                win_post_skaggs.append(np.mean(skaggs_post[post_sequence]))
                prob_pre = np.mean(np.max(pre_array, axis=1))
                prob_post = np.mean(np.max(post_array, axis=1))
                win_pre_post_ratio.append((prob_post - prob_pre) / (prob_post + prob_pre))

                # check how likely observed sequence is considering transitions from model (awake behavior)
                mode_before = pre_sequence[:-1]
                mode_after = pre_sequence[1:]
                transition_prob = 0
                # go trough each transition of the sequence
                for bef, aft in zip(mode_before, mode_after):
                    transition_prob += np.log(transmat_pre[bef, aft])

                win_seq_similarity.append(np.exp(transition_prob))

        win_pre_skaggs = moving_average(a=np.array(win_pre_skaggs), n=n_moving_average)
        win_pre_sparsity = moving_average(a=np.array(win_pre_sparsity), n=n_moving_average)
        win_post_skaggs = moving_average(a=np.array(win_post_skaggs), n=n_moving_average)
        win_post_sparsity = moving_average(a=np.array(win_post_sparsity), n=n_moving_average)
        win_pre_post_ratio = moving_average(a=np.array(win_pre_post_ratio), n=n_moving_average)
        # win_seq_similarity = moving_average(a=np.array(win_seq_similarity), n=30)

        # plot per SWR info
        # --------------------------------------------------------------------------------------------------------------
        plt.plot(win_pre_post_ratio)
        plt.title("PRE-POST RATIO FOR EACH WINDOW: PHMM")
        plt.xlabel("WINDOW ID")
        plt.ylabel("PRE-POST SIMILARITY")
        plt.ylim(-1, 1)
        plt.grid()
        plt.show()

        plt.subplot(2,1,1)
        plt.plot(win_pre_skaggs)
        plt.title("SPATIAL INFORMATION PER WINDOW: PRE, SKAGGS")
        plt.xlabel("WINDOW ID")
        plt.ylabel("AVG. SKAGGS INFO")
        plt.subplot(2,1,2)
        plt.plot(win_pre_sparsity)
        plt.title("SPATIAL INFORMATION PER WINDOW: PRE, SPARSITY")
        plt.xlabel("WINDOW ID")
        plt.ylabel("AVG. SPARSITY")
        plt.show()

        plt.scatter(win_pre_post_ratio, win_pre_skaggs)
        plt.title("PER WINDOW CORRELATION: SCORE vs. SKAGGS\n"+str(pearsonr(win_pre_post_ratio, win_pre_skaggs)))
        plt.xlabel("SIMILARITY SCORE")
        plt.ylabel("SKAGGS INFORMATION")
        plt.show()
        plt.scatter(win_pre_post_ratio, win_pre_sparsity)
        plt.title("PER WINDOW CORRELATION: SCORE vs. SPARSITY\n"+str(pearsonr(win_pre_post_ratio, win_pre_sparsity)))
        plt.xlabel("SIMILARITY SCORE")
        plt.ylabel("SKAGGS SPARSITY")
        plt.show()

        plt.plot(win_pre_skaggs, c="r", label="PRE")
        plt.plot(win_post_skaggs, label="POST")
        plt.title("SPATIAL INFORMATION PER WINDOW: PRE, SKAGGS")
        plt.xlabel("WINDOW ID")
        plt.ylabel("AVG. SKAGGS INFO")
        plt.legend()
        plt.show()

        plt.plot(win_pre_sparsity, c="r", label="PRE")
        plt.plot(win_post_sparsity, label="POST")
        plt.title("SPATIAL INFORMATION PER WINDOW: PRE, SPARSITY")
        plt.xlabel("WINDOW ID")
        plt.ylabel("AVG. SPARSITY")
        plt.legend()
        plt.show()

        plt.plot(win_seq_similarity)
        plt.title("SEQUENCE PROBABILITY: PHMM")
        plt.xlabel("WINDOW ID")
        plt.ylabel("PROBABILITY")
        plt.grid()
        plt.show()

    def memory_drift_rem_nrem_and_awake_visualization(self, template_type, pre_file_name=None, post_file_name=None,
                                                      rem_pop_vec_threshold=100, log_transform=False):
        # get awake results first
        # --------------------------------------------------------------------------------------------------------------
        pre_prob_awake = self.learning_cheeseboard[0].decode_awake_activity()
        pre_prob_awake = np.vstack(pre_prob_awake)

        # get rem data
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_rem_event, event_times_rem, event_lengths_rem, event_duration_rem_in_s, pre_prob_rem, post_prob_rem = \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold)

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_nrem_event, event_times_nrem, event_lengths_nrem, event_duration_nrem_in_s, pre_prob_nrem, \
        post_prob_nrem = self.memory_drift_long_sleep_get_results(template_type=template_type,
                                                                  pre_file_name=pre_file_name,
                                                                  post_file_name=post_file_name, part_to_analyze="nrem",
                                                                  pop_vec_threshold=2)
        if log_transform:
            pre_prob_rem_log = np.log(pre_prob_rem)
            pre_prob_nrem_log = np.log(pre_prob_nrem)
            pre_prob_awake_log = np.log(pre_prob_awake)

        else:
            pre_prob_rem_log = pre_prob_rem
            pre_prob_nrem_log = pre_prob_nrem
            pre_prob_awake_log = pre_prob_awake

        pre_prob_rem_log_mean = np.mean(pre_prob_rem_log, axis=0)
        pre_prob_nrem_log_mean = np.mean(pre_prob_nrem_log, axis=0)
        pre_prob_awake_log_mean = np.mean(pre_prob_awake_log, axis=0)
        pre_prob_rem_mean = np.mean(pre_prob_rem, axis=0)
        pre_prob_nrem_mean = np.mean(pre_prob_nrem, axis=0)
        pre_prob_awake_mean = np.mean(pre_prob_awake, axis=0)

        pre_prob_rem_log_samples = pre_prob_rem_log[0::50, :]
        pre_prob_nrem_log_samples = pre_prob_nrem_log[0::50, :]
        pre_prob_awake_log_samples = pre_prob_awake_log[0::20, :]

        sep_1 = pre_prob_rem_log_samples.shape[0]
        sep_2 = pre_prob_rem_log_samples.shape[0] + pre_prob_nrem_log_samples.shape[0]
        comb = np.vstack((pre_prob_rem_log_samples, pre_prob_nrem_log_samples, pre_prob_awake_log_samples)).T
        sep_3 = comb.shape[1]

        result = multi_dim_scaling(act_mat=comb, param_dic=self.params)
        # result = perform_isomap(act_mat=comb, param_dic=self.params)
        # result = perform_PCA(act_mat=comb, param_dic=self.params)[0]
        # result = perform_TSNE(act_mat=comb, param_dic=self.params)


        # split again into REM/NREM/AWAKE
        result_rem = result[:sep_1,:]
        result_nrem = result[sep_1:sep_2,:]
        result_awake = result[sep_2:,:]

        fig, axs = plt.subplots(2, 2)
        axs[0, 0].scatter(result_rem[:,0], result_rem[:,1], color="r", s=1)
        axs[0, 0].set_title("REM")
        axs[0, 1].scatter(result_nrem[:,0], result_nrem[:,1], color="b", s=1)
        axs[0, 1].set_title("NREM")
        axs[1, 0].scatter(result_awake[:,0], result_awake[:,1], color="y", s=1)
        axs[1, 0].set_title("AWAKE")
        axs[1, 1].scatter(result_rem[:,0], result_rem[:,1], color="r", s=1)
        axs[1, 1].scatter(result_nrem[:, 0], result_nrem[:, 1], color="b", s=1)
        axs[1, 1].scatter(result_awake[:, 0], result_awake[:, 1], color="y", s=1)

        axs[1, 1].set_title("ALL")
        plt.show()
        exit()



        if self.params.dr_method_p2 == 3:
            # create figure instance
            fig = plt.figure()
            ax = fig.add_subplot(111, projection='3d')
            plot_3D_scatter(ax=ax, mds=result, params=self.params, data_sep=np.array([sep_1, sep_2]), data_sep2=sep_2)
        else:
            fig = plt.figure()
            ax = fig.add_subplot(111)
            plot_2D_scatter(ax=ax, mds=result, params=self.params, data_sep=np.array([sep_1, sep_2, sep_3]),
                            labels=["REM",
                                    "NREM",
                                    "AWAKE"])
        handles, labels = ax.get_legend_handles_labels()
        by_label = OrderedDict(zip(labels, handles))
        ax.legend(by_label.values(), by_label.keys())
        plt.show()

        exit()
        comb_min = np.min(np.hstack((pre_prob_nrem_log_mean, pre_prob_rem_log_mean, pre_prob_awake_log_mean)))
        comb_max = np.max(np.hstack((pre_prob_nrem_log_mean, pre_prob_rem_log_mean, pre_prob_awake_log_mean)))
        plt.subplot(1, 3, 1)
        plt.imshow(np.expand_dims(pre_prob_nrem_log_mean, 1), vmin=comb_min, vmax=comb_max, interpolation='nearest',
                   aspect='auto')
        plt.ylabel("MODE ID")
        plt.title("NREM")
        plt.gca().set_xticklabels([])
        plt.subplot(1, 3, 2)
        plt.imshow(np.expand_dims(pre_prob_rem_log_mean, 1), vmin=comb_min, vmax=comb_max, interpolation='nearest',
                   aspect='auto')
        plt.title("REM")
        plt.gca().set_xticklabels([])
        plt.subplot(1, 3, 3)
        plt.imshow(np.expand_dims(pre_prob_awake_log_mean, 1), vmin=comb_min, vmax=comb_max, interpolation='nearest',
                   aspect='auto')
        a = plt.colorbar()
        a.set_label("MEAN LOG-PROB.")
        plt.title("AWAKE")
        plt.gca().set_xticklabels([])
        plt.show()

        comb_min = np.min(np.hstack((pre_prob_nrem_mean, pre_prob_rem_mean, pre_prob_awake_mean)))
        comb_max = np.max(np.hstack((pre_prob_nrem_mean, pre_prob_rem_mean, pre_prob_awake_mean)))
        plt.subplot(1, 3, 1)
        plt.imshow(np.expand_dims(pre_prob_nrem_mean, 1), vmin=comb_min, vmax=comb_max, interpolation='nearest',
                   aspect='auto')
        plt.ylabel("MODE ID")
        plt.title("NREM")
        plt.gca().set_xticklabels([])
        plt.subplot(1, 3, 2)
        plt.imshow(np.expand_dims(pre_prob_rem_mean, 1), vmin=comb_min, vmax=comb_max, interpolation='nearest',
                   aspect='auto')
        plt.title("REM")
        plt.gca().set_xticklabels([])
        plt.subplot(1, 3, 3)
        plt.imshow(np.expand_dims(pre_prob_awake_mean, 1), vmin=comb_min, vmax=comb_max, interpolation='nearest',
                   aspect='auto')
        a = plt.colorbar()
        a.set_label("MEAN PROB.")
        plt.title("AWAKE")
        plt.gca().set_xticklabels([])
        plt.show()

        plt.imshow(pre_prob_rem_log.T, interpolation='nearest', aspect='auto')
        a = plt.colorbar()
        a.set_label("LOG(PROB)")
        plt.xlabel("POP.VEC.ID")
        plt.ylabel("MODE ID")
        plt.title("REM")
        plt.show()

        plt.imshow(pre_prob_nrem_log.T, interpolation='nearest', aspect='auto')
        plt.xlabel("POP.VEC.ID")
        plt.ylabel("MODE ID")
        a = plt.colorbar()
        a.set_label("LOG(PROB)")
        plt.title("NREM")
        plt.show()

        plt.imshow(pre_prob_awake_log.T, interpolation='nearest', aspect='auto')
        plt.xlabel("POP.VEC.ID")
        plt.ylabel("MODE ID")
        a = plt.colorbar()
        a.set_label("LOG(PROB)")
        plt.title("AWAKE")
        plt.show()

    def memory_drift_rem_nrem_and_awake_fluctuations(self, template_type, pre_file_name=None, post_file_name=None,
                                                     rem_pop_vec_threshold=100):
        # get awake results first
        # --------------------------------------------------------------------------------------------------------------
        pre_prob_awake = self.learning_cheeseboard[0].decode_awake_activity()
        pre_prob_awake = np.vstack(pre_prob_awake)
        pre_prob_awake_log = np.log(pre_prob_awake).T

        # get rem data
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_rem_event, event_times_rem, event_lengths_rem, event_duration_rem_in_s, pre_prob_rem, post_prob_rem = \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold)

        pre_prob_rem = np.vstack(pre_prob_rem).T
        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_nrem_event, event_times_nrem, event_lengths_nrem, event_duration_nrem_in_s, pre_prob_nrem, \
        post_prob_nrem = self.memory_drift_long_sleep_get_results(template_type=template_type,
                                                                  pre_file_name=pre_file_name,
                                                                  post_file_name=post_file_name, part_to_analyze="nrem",
                                                                  pop_vec_threshold=2)

        pre_prob_nrem = np.vstack(pre_prob_nrem).T

        pre_prob_rem_log = np.log(pre_prob_rem)
        pre_prob_nrem_log = np.log(pre_prob_nrem)
        pre_prob_awake_log = np.log(pre_prob_awake)

        ab_angle, rel_angle = angle_between_col_vectors(pre_prob_nrem_log)
        plt.plot(rel_angle)
        plt.show()

    def memory_drift_rem_nrem_decoding_similarity(self, template_type, pre_file_name=None, post_file_name=None,
                                                  rem_pop_vec_threshold=100, cells_to_use="all", plotting=True,
                                                  control=False, nr_shuffles=20, pre_or_post="pre"):

        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        _, _, _, _, pre_prob_rem, post_prob_rem= \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold,
                                                     cells_to_use=cells_to_use)

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        _, _, _, _, pre_prob_nrem, \
        post_prob_nrem= self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="nrem",
                                                     pop_vec_threshold=2, cells_to_use=cells_to_use)

        # need to compute control if want to plot
        if plotting:
            control=True
        if control is False:
            rem_mode_freq_norm_odd = None
            rem_mode_freq_norm_even = None
            nrem_mode_freq_norm_odd = None
            nrem_mode_freq_norm_even = None


        # pre_prob_rem_norm = pre_prob_rem / np.sum(pre_prob_rem, axis=1, keepdims=True)
        # pre_prob_nrem_norm = pre_prob_nrem / np.sum(pre_prob_nrem, axis=1, keepdims=True)

        # use PRE or POST results --> REM
        if pre_or_post=="pre":
            # rem pre map results and normalize
            rem_mode_freq = np.zeros(pre_prob_rem.shape[1])
            map_result_rem = np.argmax(pre_prob_rem, axis=1)
            prob_rem = pre_prob_rem

        elif pre_or_post=="post":
            # rem post map results and normalize
            rem_mode_freq = np.zeros(post_prob_rem.shape[1])
            map_result_rem = np.argmax(post_prob_rem, axis=1)
            prob_rem = post_prob_rem

        rem_mode_id, rem_mode_count = np.unique(map_result_rem, return_counts=True)
        rem_mode_freq[rem_mode_id] = rem_mode_count
        rem_mode_freq_norm = rem_mode_freq / np.sum(rem_mode_freq)

        if control:
            corr_within_rem = np.zeros(nr_shuffles)
            # check rem reactivation correlation with itself
            for shuffle_id in range(nr_shuffles):
                shuffled_ind = np.random.permutation(np.arange(map_result_rem.shape[0]))
                map_result_rem_even = map_result_rem[shuffled_ind[:int(0.5*shuffled_ind.shape[0])]]
                map_result_rem_odd = map_result_rem[shuffled_ind[int(0.5*shuffled_ind.shape[0]):]]
                # pre_map_result_rem_even = pre_map_result_rem[::2]
                # pre_map_result_rem_odd = pre_map_result_rem[1:][::2]
                rem_mode_id_even, rem_mode_count_even = np.unique(map_result_rem_even, return_counts=True)
                rem_mode_id_odd, rem_mode_count_odd = np.unique(map_result_rem_odd, return_counts=True)
                rem_mode_freq_even = np.zeros(prob_rem.shape[1])
                rem_mode_freq_even[rem_mode_id_even] = rem_mode_count_even
                rem_mode_freq_norm_even = rem_mode_freq_even / np.sum(rem_mode_freq_even)

                rem_mode_freq_odd = np.zeros(prob_rem.shape[1])
                rem_mode_freq_odd[rem_mode_id_odd] = rem_mode_count_odd
                rem_mode_freq_norm_odd = rem_mode_freq_odd / np.sum(rem_mode_freq_odd)

                corr_within_rem[shuffle_id] = pearsonr(rem_mode_freq_norm_even , rem_mode_freq_norm_odd)[0]
            #
            # plt.scatter(pre_rem_mode_freq_norm_even, pre_rem_mode_freq_norm_odd)
            # plt.title("REM: even vs. odd\n Pearson:" + str(
            #     pearsonr(pre_rem_mode_freq_norm_even, pre_rem_mode_freq_norm_odd)[0]) + "\n Spear:" +
            #           str(spearmanr(pre_rem_mode_freq_norm_even, pre_rem_mode_freq_norm_odd)[0]))
            # plt.show()

            # get nrem pre map results and normalize

        # use PRE or POST results --> REM
        if pre_or_post == "pre":
            # rem pre map results and normalize
            nrem_mode_freq = np.zeros(pre_prob_nrem.shape[1])
            map_result_nrem = np.argmax(pre_prob_nrem, axis=1)
            prob_nrem = pre_prob_nrem

        elif pre_or_post == "post":
            # rem post map results and normalize
            nrem_mode_freq = np.zeros(post_prob_nrem.shape[1])
            map_result_nrem = np.argmax(post_prob_nrem, axis=1)
            prob_nrem = post_prob_nrem

        nrem_mode_id, nrem_mode_count = np.unique(map_result_nrem, return_counts=True)
        nrem_mode_freq[nrem_mode_id] = nrem_mode_count
        nrem_mode_freq_norm = nrem_mode_freq / np.sum(nrem_mode_freq)

        if control:
            # check nrem reactivation correlation with itself
            corr_within_nrem = np.zeros(nr_shuffles)
            # check rem reactivation correlation with itself
            for shuffle_id in range(nr_shuffles):
                shuffled_ind = np.random.permutation(np.arange(map_result_nrem.shape[0]))
                map_result_nrem_even = map_result_nrem[shuffled_ind[:int(0.5*shuffled_ind.shape[0])]]
                map_result_nrem_odd = map_result_nrem[shuffled_ind[int(0.5*shuffled_ind.shape[0]):]]
                # pre_map_result_nrem_even = pre_map_result_nrem[::2]
                # pre_map_result_nrem_odd = pre_map_result_nrem[1:][::2]
                nrem_mode_id_even, nrem_mode_count_even = np.unique(map_result_nrem_even, return_counts=True)
                nrem_mode_id_odd, nrem_mode_count_odd = np.unique(map_result_nrem_odd, return_counts=True)
                nrem_mode_freq_even = np.zeros(prob_nrem.shape[1])
                nrem_mode_freq_even[nrem_mode_id_even] = nrem_mode_count_even
                nrem_mode_freq_norm_even = nrem_mode_freq_even / np.sum(nrem_mode_freq_even)

                nrem_mode_freq_odd = np.zeros(prob_nrem.shape[1])
                nrem_mode_freq_odd[nrem_mode_id_odd] = nrem_mode_count_odd
                nrem_mode_freq_norm_odd = nrem_mode_freq_odd / np.sum(nrem_mode_freq_odd)

                corr_within_nrem[shuffle_id] = pearsonr(nrem_mode_freq_norm_even, nrem_mode_freq_norm_odd)[0]
            # plt.scatter(pre_nrem_mode_freq_norm_even, pre_nrem_mode_freq_norm_odd)
            # plt.title("NREM: even vs. odd\n Pearson:" + str(
            #     pearsonr(pre_nrem_mode_freq_norm_even, pre_nrem_mode_freq_norm_odd)[0]) + "\n Spear:" +
            #           str(spearmanr(pre_nrem_mode_freq_norm_even, pre_nrem_mode_freq_norm_odd)[0]))
            # plt.show()

            # pre_rem_mode_freq_norm = pre_rem_mode_freq_norm[ pre_rem_mode_freq_norm<0.2]
            # pre_nrem_mode_freq_norm = pre_nrem_mode_freq_norm[pre_nrem_mode_freq_norm < 0.2]
        if plotting:
            binwidth=0.0001

            y1,x1,_ = plt.hist(corr_within_nrem, bins=np.arange(min(corr_within_nrem), max(corr_within_nrem) + binwidth, binwidth),
                     color="blue", label="within nrem", density=True)
            y2,x2,_ = plt.hist(corr_within_rem, bins=np.arange(min(corr_within_rem), max(corr_within_rem) + binwidth, binwidth),
                     color="red", label="within rem", density=True)

            r_across = pearsonr(rem_mode_freq_norm, nrem_mode_freq_norm)[0]
            plt.title(template_type+", Pearson R across = " +str(r_across))
            plt.legend()
            plt.xlabel("Pearson R")
            plt.show()

            plt.scatter(rem_mode_freq_norm, nrem_mode_freq_norm)
            plt.xlabel("REM")
            plt.ylabel("NREM")
            plt.title(template_type+", REM vs. NREM\n Pearson:"+str(pearsonr(rem_mode_freq_norm, nrem_mode_freq_norm)[0])+"\n Spear:"+
                      str(spearmanr(rem_mode_freq_norm, nrem_mode_freq_norm)[0]))

            plt.show()
        else:
            return rem_mode_freq_norm, nrem_mode_freq_norm, rem_mode_freq_norm_odd, rem_mode_freq_norm_even, \
                   nrem_mode_freq_norm_odd, nrem_mode_freq_norm_even

    def memory_drift_rem_nrem_decoding_cleanliness_per_mode(self, template_type, pre_file_name=None, post_file_name=None,
                                                  rem_pop_vec_threshold=100, cells_to_use="all", plotting=True,
                                                  nr_shuffles=500, control_data="rem"):

        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        _, _, _, _, pre_prob_rem, post_prob_rem= \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold,
                                                     cells_to_use=cells_to_use)

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        _, _, _, _, pre_prob_nrem, \
        post_prob_nrem= self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="nrem",
                                                     pop_vec_threshold=2, cells_to_use=cells_to_use)

        pre_prob_rem_norm = pre_prob_rem / np.sum(pre_prob_rem, axis=1, keepdims=True)
        pre_prob_nrem_norm = pre_prob_nrem / np.sum(pre_prob_nrem, axis=1, keepdims=True)

        pre_prob_rem_map_mode = np.argmax(pre_prob_rem, axis=1)
        pre_prob_nrem_map_mode = np.argmax(pre_prob_nrem, axis=1)

        pre_prob_rem_map_val = np.max(pre_prob_rem_norm, axis=1)
        pre_prob_nrem_map_val = np.max(pre_prob_nrem_norm, axis=1)

        mean_ratio_per_mode = np.zeros(pre_prob_rem.shape[1])
        mean_ratio_per_mode[:] = np.nan
        mean_post_prob_rem = np.zeros(pre_prob_rem.shape[1])
        mean_post_prob_nrem = np.zeros(pre_prob_rem.shape[1])
        post_prob_rem = []
        post_prob_nrem = []
        for mode_id in range(pre_prob_rem.shape[1]):
            if np.count_nonzero(pre_prob_rem_map_val[pre_prob_rem_map_mode == mode_id]) and \
                np.count_nonzero(pre_prob_nrem_map_val[pre_prob_nrem_map_mode == mode_id]):
                mean_ratio_per_mode[mode_id] = np.mean(pre_prob_rem_map_val[pre_prob_rem_map_mode == mode_id])/\
                                           np.mean(pre_prob_nrem_map_val[pre_prob_nrem_map_mode == mode_id])

                mean_post_prob_rem[mode_id] = np.mean(pre_prob_rem_map_val[pre_prob_rem_map_mode == mode_id])
                mean_post_prob_nrem[mode_id] = np.mean(pre_prob_nrem_map_val[pre_prob_nrem_map_mode == mode_id])
                post_prob_rem.append(pre_prob_rem_map_val[pre_prob_rem_map_mode == mode_id])
                post_prob_nrem.append(pre_prob_nrem_map_val[pre_prob_nrem_map_mode == mode_id])

        # mean_ratio_per_mode = mean_ratio_per_mode[~np.isnan(mean_ratio_per_mode)]

        shuffled_rem_per_mode = []
        shuffled_nrem_per_mode = []

        for mode_id in range(pre_prob_rem.shape[1]):
            mean_ratio_per_mode_rem = np.zeros(nr_shuffles)
            mean_ratio_per_mode_rem[:] = np.nan
            mean_ratio_per_mode_nrem = np.zeros(nr_shuffles)
            mean_ratio_per_mode_nrem[:] = np.nan
            all_rem = pre_prob_rem_map_val[pre_prob_rem_map_mode == mode_id]
            all_nrem = pre_prob_nrem_map_val[pre_prob_nrem_map_mode == mode_id]

            for shuffle_id in range(nr_shuffles):

                all_rem_shuffled = np.random.permutation(all_rem)
                mean_ratio_per_mode_rem[shuffle_id] = np.mean(all_rem_shuffled[:int(all_rem_shuffled.shape[0]*0.5)])/\
                                                   np.mean(all_rem_shuffled[int(all_rem_shuffled.shape[0]*0.5):])

                all_nrem_shuffled = np.random.permutation(all_nrem)
                mean_ratio_per_mode_nrem[shuffle_id] = np.mean(all_nrem_shuffled[:int(all_nrem_shuffled.shape[0]*0.5)])/\
                                                   np.mean(all_nrem_shuffled[int(all_nrem_shuffled.shape[0]*0.5):])

            shuffled_rem_per_mode.append(mean_ratio_per_mode_rem)
            shuffled_nrem_per_mode.append(mean_ratio_per_mode_nrem)

        # go through all modes and check if they are signficantly greater than within
        sign_diff_modes = np.zeros(pre_prob_rem.shape[1])

        for mode_id, (mode_ratio, nrem_control, rem_control) in enumerate(zip(mean_ratio_per_mode, shuffled_nrem_per_mode,
                                                                    shuffled_rem_per_mode)):

            if control_data == "nrem":
                control = nrem_control
            elif control_data == "rem":
                control = rem_control

            if np.count_nonzero(np.isnan(control)) > 0:
                continue

            if mode_ratio < np.mean(control) - 2 * np.std(control) or \
                np.mean(control)+2*np.std(control) < mode_ratio:

                sign_diff_modes[mode_id] = 1

                # plt.hist(nrem_control, color="gray")
                # plt.vlines(mode_ratio, 0, 20, color="red", label="data")
                # plt.vlines(np.mean(nrem_control) + 2 * np.std(nrem_control), 0, 20, color="yellow", label="2std")
                # plt.vlines(np.mean(nrem_control) - 2 * np.std(nrem_control), 0, 20, color="yellow")
                # plt.title("mode" + str(mode_id))
                # plt.legend()
                # plt.show()

        # only select modes that are significantly different
        mean_ratio_per_mode_significant = mean_ratio_per_mode[sign_diff_modes.astype(bool)]

        mean_post_prob_rem_significant = mean_post_prob_rem[sign_diff_modes.astype(bool)]
        mean_post_prob_nrem_significant = mean_post_prob_nrem[sign_diff_modes.astype(bool)]

        post_prob_rem_significant = []
        post_prob_nrem_significant = []

        for rem_dat, nrem_dat, signif_ind in zip(post_prob_rem, post_prob_nrem, sign_diff_modes):
            if signif_ind:
                post_prob_rem_significant.extend(rem_dat)
                post_prob_nrem_significant.extend(nrem_dat)

        if plotting:
            g = sns.kdeplot(mean_post_prob_rem_significant, fill=True, color="red", label="REM")
            max_y = g.viewLim.bounds[3]
            g = sns.kdeplot(mean_post_prob_nrem_significant, fill=True, color="blue", label="NREM")
            max_y = g.viewLim.bounds[3]
            plt.show()
            plt.hist(mean_ratio_per_mode_significant)
            plt.show()
        else:
            return mean_ratio_per_mode_significant, mean_ratio_per_mode, mean_post_prob_rem_significant, \
                   mean_post_prob_nrem_significant, post_prob_rem_significant, post_prob_nrem_significant

    def memory_drift_rem_nrem_decoding_cleanliness(self, template_type, pre_file_name=None, post_file_name=None,
                                                  rem_pop_vec_threshold=100, cells_to_use="all", plotting=True):

        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        _, _, _, _, pre_prob_rem, post_prob_rem= \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold,
                                                     cells_to_use=cells_to_use)

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        _, _, _, _, pre_prob_nrem, \
        post_prob_nrem= self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="nrem",
                                                     pop_vec_threshold=2, cells_to_use=cells_to_use)

        # compute mean of all likelihoods to threshold
        mean_likeli_nrem = np.mean(pre_prob_nrem.flatten())
        pre_prob_nrem = pre_prob_nrem[np.max(pre_prob_nrem, axis=1) > mean_likeli_nrem]

        # compute ratio of highest and second highest likelihood NREM
        max_likeli_ind_nrem = np.argmax(pre_prob_nrem, axis=1)
        max_likeli_nrem = np.max(pre_prob_nrem, axis=1)

        m, n = pre_prob_nrem.shape
        pre_prob_nrem_wo_max = pre_prob_nrem[np.arange(n) != np.array(max_likeli_ind_nrem)[:, None]].reshape(m, -1)
        second_max_likeli_nrem = np.max(pre_prob_nrem_wo_max, axis=1)

        nrem_first_second_max_ratio = max_likeli_nrem / second_max_likeli_nrem
        

        # compute mean of likelihoods
        mean_likeli_rem = np.mean(pre_prob_rem.flatten())
        pre_prob_rem = pre_prob_rem[np.max(pre_prob_rem, axis=1) > mean_likeli_rem]


        # compute ratio of highest and second highest likelihood REM
        max_likeli_ind_rem = np.argmax(pre_prob_rem, axis=1)
        max_likeli_rem = np.max(pre_prob_rem, axis=1)

        m, n = pre_prob_rem.shape
        pre_prob_rem_wo_max = pre_prob_rem[np.arange(n) != np.array(max_likeli_ind_rem)[:, None]].reshape(m, -1)
        second_max_likeli_rem = np.max(pre_prob_rem_wo_max, axis=1)

        rem_first_second_max_ratio = max_likeli_rem / second_max_likeli_rem

        p_ratio_rem = 1. * np.arange(rem_first_second_max_ratio.shape[0]) / (rem_first_second_max_ratio.shape[0] - 1)
        p_ratio_nrem = 1. * np.arange(nrem_first_second_max_ratio.shape[0]) / (nrem_first_second_max_ratio.shape[0] - 1)

        pre_prob_nrem_norm = pre_prob_nrem / np.sum(pre_prob_nrem, axis=1, keepdims=True)
        pre_prob_rem_norm = pre_prob_rem / np.sum(pre_prob_rem, axis=1, keepdims=True)

        max_prob_nrem = np.max(pre_prob_nrem_norm, axis=1)
        max_prob_rem = np.max(pre_prob_rem_norm, axis=1)

        p_prob_rem = 1. * np.arange(max_prob_rem.shape[0]) / (max_prob_rem.shape[0] - 1)
        p_prob_nrem = 1. * np.arange(max_prob_nrem.shape[0]) / (max_prob_nrem.shape[0] - 1)

        if plotting:

            plt.plot(np.sort(rem_first_second_max_ratio), p_ratio_rem, label="REM")
            plt.plot(np.sort(nrem_first_second_max_ratio), p_ratio_nrem, label="NREM")
            plt.xscale("log")
            plt.legend()
            plt.xlabel("max. likeli / second largest likeli")
            plt.ylabel("cdf")
            plt.show()
            print(mannwhitneyu(rem_first_second_max_ratio, nrem_first_second_max_ratio, alternative="greater"))

            plt.plot(np.sort(max_prob_rem), p_prob_rem, label="REM")
            plt.plot(np.sort(max_prob_nrem), p_prob_nrem, label="NREM")
            plt.ylabel("cdf")
            plt.xlabel("max. prob.")
            plt.legend()
            plt.show()

            print(mannwhitneyu(max_prob_rem, max_prob_nrem, alternative="greater"))

        else:
            return rem_first_second_max_ratio, nrem_first_second_max_ratio, max_prob_rem, max_prob_nrem

    def memory_drift_rem_nrem_decoding_visualization(self, template_type, pre_file_name=None, post_file_name=None,
                                                  rem_pop_vec_threshold=100, only_stable_cells=False):

        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_rem_event, event_times_rem, event_lengths_rem, event_duration_rem_in_s, pre_prob_rem, post_prob_rem= \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold,
                                                     only_stable_cells=only_stable_cells)


        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_nrem_event, event_times_nrem, event_lengths_nrem, event_duration_nrem_in_s, pre_prob_nrem, \
        post_prob_nrem= self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="nrem",
                                                     pop_vec_threshold=2, only_stable_cells=only_stable_cells)

        pre_prob_rem_log = np.log(pre_prob_rem)
        pre_prob_nrem_log = np.log(pre_prob_nrem)

        pre_prob_rem_log_mean = np.mean(pre_prob_rem_log, axis=0)
        pre_prob_nrem_log_mean = np.mean(pre_prob_nrem_log, axis=0)
        pre_prob_rem_mean = np.mean(pre_prob_rem, axis=0)
        pre_prob_nrem_mean = np.mean(pre_prob_nrem, axis=0)

        pre_prob_rem = pre_prob_rem_log[0::50,:]
        pre_prob_nrem = pre_prob_nrem_log[0::50,:]

        comb_mean = np.vstack((pre_prob_rem, pre_prob_nrem)).T
        sep = np.array([pre_prob_rem.shape[0], comb_mean.shape[1]])

        result = multi_dim_scaling(act_mat=comb_mean, param_dic=self.params)

        if self.params.dr_method_p2 == 3:
            # create figure instance
            fig = plt.figure()
            ax = fig.add_subplot(111, projection='3d')
            plot_3D_scatter(ax=ax, mds=result, params=self.params, data_sep=sep)
        else:
            fig = plt.figure()
            ax = fig.add_subplot(111)
            plot_2D_scatter(ax=ax, mds=result, params=self.params, data_sep=sep, labels=["REM", "NREM"])
        handles, labels = ax.get_legend_handles_labels()
        by_label = OrderedDict(zip(labels, handles))
        ax.legend(by_label.values(), by_label.keys())
        plt.show()

        comb_min = np.min(np.hstack((pre_prob_nrem_log_mean, pre_prob_rem_log_mean)))
        comb_max = np.max(np.hstack((pre_prob_nrem_log_mean, pre_prob_rem_log_mean)))
        plt.subplot(1,2,1)
        plt.imshow(np.expand_dims(pre_prob_nrem_log_mean, 1), vmin=comb_min, vmax=comb_max, interpolation='nearest',
                   aspect='auto')
        a=plt.colorbar()
        a.set_label("MEAN LOG-PROB.")
        plt.ylabel("MODE ID")
        plt.title("NREM")
        plt.gca().set_xticklabels([])
        plt.subplot(1,2,2)
        plt.imshow(np.expand_dims(pre_prob_rem_log_mean, 1), vmin=comb_min, vmax=comb_max, interpolation='nearest',
                   aspect='auto')
        a=plt.colorbar()
        a.set_label("MEAN LOG-PROB.")
        plt.title("REM")
        plt.gca().set_xticklabels([])
        plt.show()

        comb_min = np.min(np.hstack((pre_prob_nrem_mean, pre_prob_rem_mean)))
        comb_max = np.max(np.hstack((pre_prob_nrem_mean, pre_prob_rem_mean)))
        plt.subplot(1,2,1)
        plt.imshow(np.expand_dims(pre_prob_nrem_mean, 1), vmin=comb_min, vmax=comb_max, interpolation='nearest',
                   aspect='auto')
        a=plt.colorbar()
        a.set_label("MEAN PROB.")
        plt.ylabel("MODE ID")
        plt.title("NREM")
        plt.gca().set_xticklabels([])
        plt.subplot(1,2,2)
        plt.imshow(np.expand_dims(pre_prob_rem_mean, 1), vmin=comb_min, vmax=comb_max, interpolation='nearest',
                   aspect='auto')
        a=plt.colorbar()
        a.set_label("MEAN PROB.")
        plt.title("REM")
        plt.gca().set_xticklabels([])
        plt.show()

        plt.imshow(pre_prob_rem_log.T, interpolation='nearest', aspect='auto')
        a = plt.colorbar()
        a.set_label("LOG(PROB)")
        plt.xlabel("POP.VEC.ID")
        plt.ylabel("MODE ID")
        plt.title("REM")
        plt.show()

        plt.imshow(pre_prob_nrem_log.T, interpolation='nearest', aspect='auto')
        plt.xlabel("POP.VEC.ID")
        plt.ylabel("MODE ID")
        a = plt.colorbar()
        a.set_label("LOG(PROB)")
        plt.title("NREM")
        plt.show()

    def memory_drift_rem_nrem_decoding_temporal_visualization(self, template_type, pre_file_name=None,
                                                              post_file_name=None, only_stable_cells=False):

        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        pre_prob_list_rem, post_prob_list_rem, event_times_list_rem= \
            self.memory_drift_long_sleep_get_raw_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                         only_stable_cells=only_stable_cells)

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        pre_prob_list_nrem, post_prob_list_nrem, event_times_list_nrem= self.memory_drift_long_sleep_get_raw_results(
            template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="nrem",
                                                     only_stable_cells=only_stable_cells)


        time_per_rem_vec = []
        # assign temporal label to all vectors
        for prob_l, e_t in zip(pre_prob_list_rem, event_times_list_rem):
            time_per_rem_vec.append(np.linspace(e_t[0],e_t[1], prob_l.shape[0]))

        time_per_rem_vec = np.hstack(time_per_rem_vec)
        pre_prob_rem = np.vstack(pre_prob_list_rem)


        time_per_nrem_vec = []
        # assign temporal label to all vectors
        for prob_l, e_t in zip(pre_prob_list_nrem, event_times_list_nrem):
            time_per_nrem_vec.append(np.linspace(e_t[0],e_t[1], prob_l.shape[0]))

        time_per_nrem_vec = np.hstack(time_per_nrem_vec)
        pre_prob_nrem = np.vstack(pre_prob_list_nrem)

        sampling = 50

        pre_prob_rem_log = np.log(pre_prob_rem)
        pre_prob_nrem_log = np.log(pre_prob_nrem)

        pre_prob_rem_log = pre_prob_rem_log[0::sampling,:]
        time_per_rem_vec = time_per_rem_vec[0::sampling]
        pre_prob_nrem_log = pre_prob_nrem_log[0::sampling,:]
        time_per_nrem_vec = time_per_nrem_vec[0::sampling]

        comb = np.vstack((pre_prob_rem_log, pre_prob_nrem_log)).T
        # sep = np.array([pre_prob_rem_log.shape[0], comb.shape[1]])
        sep = pre_prob_rem_log.shape[0]

        result = multi_dim_scaling(act_mat=comb, param_dic=self.params)

        rem_res = result[:sep,:]
        nrem_res = result[sep:, :]

        fig = plt.figure()
        ax = fig.add_subplot(111, projection='3d')

        ax.scatter(rem_res[:,0], rem_res[:,1], time_per_rem_vec / 60, color="r", alpha=0.5)
        ax.scatter(nrem_res[:, 0], nrem_res[:, 1], time_per_nrem_vec / 60, color="b", alpha=0.5)
        for plot_c in range(rem_res.shape[0]-1):
            ax.plot(rem_res[plot_c:plot_c+2,0], rem_res[plot_c:plot_c+2,1], time_per_rem_vec[plot_c:plot_c+2] / 60,
                    c="lightcoral", alpha=0.5)

        for plot_c in range(nrem_res.shape[0]-1):
            ax.plot(nrem_res[plot_c:plot_c+2,0], nrem_res[plot_c:plot_c+2,1], time_per_nrem_vec[plot_c:plot_c+2] / 60,
                    c="royalblue", alpha=0.8)

        # hide labels
        ax.set_yticklabels([])
        ax.set_xticklabels([])
        ax.set_zlabel("Time / min")
        # set pane alpha value to zero --> transparent
        ax.w_xaxis.set_pane_color((0.8, 0.8, 0.8, 0.0))
        ax.w_yaxis.set_pane_color((0.8, 0.8, 0.8, 0.0))
        ax.w_zaxis.set_pane_color((0.8, 0.8, 0.8, 0.0))
        plt.show()

    def memory_drift_decoding_similarity_temporal(self, template_type, pre_file_name=None, samples_per_epoch = 40,
                                                  compare_with_previous=True, post_file_name=None,
                                                  only_stable_cells=False):

        # with open(self.params.pre_proc_dir+"temp_data/"+"test", 'rb') as f:
        #     per_event_max_corr = pickle.load(f)
        # start = 0
        # for i, event in enumerate(per_event_max_corr):
        #     length_event = event.shape[0]
        #     plt.plot(range(start, start+length_event),event)
        #     start += length_event
        #     plt.xlabel("Pop.Vec.ID (REM)")
        #     plt.ylabel("Max. corr. with previous NREM")
        #     plt.title("REM EPOCH "+str(i))
        #     plt.show()
        # exit()
        #
        #
        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        pre_prob_list_rem, post_prob_list_rem, event_times_list_rem= \
            self.memory_drift_long_sleep_get_raw_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                         pop_vec_threshold=10, only_stable_cells=only_stable_cells)

        # pre_prob_arr_rem = np.vstack(pre_prob_list_rem)
        length_per_event_rem = [x.shape[0] for x in pre_prob_list_rem]
        event_times_rem = np.vstack(event_times_list_rem)

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        pre_prob_list_nrem, post_prob_list_nrem, event_times_list_nrem= self.memory_drift_long_sleep_get_raw_results(
            template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="nrem",
            only_stable_cells=only_stable_cells)

        length_per_event_nrem = [x.shape[0] for x in pre_prob_list_nrem]
        event_times_nrem = np.vstack(event_times_list_nrem)

        # combine rem and nrem data to order it in the right way (chronologically)
        # --------------------------------------------------------------------------------------------------------------

        all_events_pre_prob = pre_prob_list_rem + pre_prob_list_nrem
        all_events_length = length_per_event_rem + length_per_event_nrem
        labels_events = np.zeros(len(pre_prob_list_rem) + len(pre_prob_list_nrem))
        labels_events[:len(pre_prob_list_rem)] = 1
        all_times = np.hstack((event_times_rem[:, 0], event_times_nrem[:, 0]))
        all_end_times = np.hstack((event_times_rem[:, 1], event_times_nrem[:, 2]))

        # sort events according to time
        sorted_events_pre_prob_list = [x for _, x in sorted(zip(all_times, all_events_pre_prob))]
        sorted_labels_events = [x for _, x in sorted(zip(all_times, labels_events))]
        sorted_lengths = [x for _, x in sorted(zip(all_times, all_events_length))]
        sorted_pre_prob = np.vstack(sorted_events_pre_prob_list)
        sorted_times = [x for x in sorted(all_times)]
        sorted_end_times = [x for _, x in sorted(zip(all_times, all_end_times))]

        # compute labels per population vector --> rem events = 1, nrem events = 0
        # --------------------------------------------------------------------------------------------------------------
        labels_per_pop_vec = []
        for len_event, label in zip(sorted_lengths, sorted_labels_events):
            if label:
                labels_per_pop_vec.extend(np.ones(len_event))
            elif label == 0:
                labels_per_pop_vec.extend(np.zeros(len_event))

        labels_per_pop_vec = np.array(labels_per_pop_vec)

        # concatenate events that have the same label
        # --------------------------------------------------------------------------------------------------------------
        sorted_labels_events = np.array(sorted_labels_events)
        label_change = np.diff(sorted_labels_events)
        merged_events_labels = []
        merged_events_times = []
        first = 0
        while True:
            dat = label_change[first:]
            if first > label_change.shape[0]:
                break
            if sorted_labels_events[first] == 0:
                # nrem event
                # check if this is the last event
                if np.where(dat == 1)[0].shape[0] == 0:
                    trans = label_change.shape[0] - first + 1
                else:
                    trans = np.where(dat == 1)[0][0] + 1
                merged_events_labels.append(np.unique(sorted_labels_events[first:first+trans]))
                merged_events_times.append([sorted_times[first], sorted_end_times[first+trans-1]])
            elif sorted_labels_events[first] == 1:
                # rem event
                # check if this is the last event
                if np.where(dat == -1)[0].shape[0] == 0:
                    trans = label_change.shape[0] - first + 1
                else:
                    trans = np.where(dat == -1)[0][0] + 1
                merged_events_labels.append(np.unique(sorted_labels_events[first:first+trans]))
                merged_events_times.append([sorted_times[first], sorted_end_times[first+trans-1]])
            first += trans

        merged_events_labels = np.squeeze(np.array(merged_events_labels))

        # concatenate population vectors that have the same label: TODO: there might be a better way to combine this
        # step and the previous step
        # --------------------------------------------------------------------------------------------------------------
        new_event_time_stamps = np.abs(np.diff(labels_per_pop_vec))

        # need to add one element at the beginning
        new_event_time_stamps = np.insert(new_event_time_stamps, 0, 1)
        start = np.nonzero(new_event_time_stamps == 1)[0]
        end = start[1:]
        end = np.append(end, new_event_time_stamps.shape[0])

        # only select data (pre_post ratio) from population vectors that belong to selected rem/nrem events
        # --> results per merged event
        # --------------------------------------------------------------------------------------------------------------
        pre_prob_per_merged_rem_event = []
        pre_prob_per_merged_nrem_event = []
        pre_prob_rem_nrem_events = []
        rem_nrem_events_label = []
        pre_prob_rem_nrem_pop_vec = []
        rem_nrem_pop_vec_label = []

        for start_event, end_event in zip(start, end):
            # rem event
            if labels_per_pop_vec[start_event + 1] == 1:
                pre_prob_per_merged_rem_event.append(sorted_pre_prob[start_event:end_event])
            # nrem event
            else:
                pre_prob_per_merged_nrem_event.append(sorted_pre_prob[start_event:end_event])

            pre_prob_rem_nrem_events.append(sorted_pre_prob[start_event:end_event])
            rem_nrem_events_label.extend([labels_per_pop_vec[start_event + 1]])

            pre_prob_rem_nrem_pop_vec.extend(sorted_pre_prob[start_event:end_event])
            rem_nrem_pop_vec_label.extend(labels_per_pop_vec[start_event:end_event])

        merged_events_times = np.vstack(merged_events_times)

        merged_rem_event_times = merged_events_times[merged_events_labels == 1]
        merged_nrem_event_times = merged_events_times[merged_events_labels == 0]

        # take samples from each epoch --> otherwise MDS won't work (too much data)
        pre_prob_per_merged_rem_event_samples = []
        for epoch in pre_prob_per_merged_rem_event:
            sample_ind = np.random.randint(0, epoch.shape[0], size=samples_per_epoch)
            pre_prob_per_merged_rem_event_samples.append(epoch[sample_ind, :])

        pre_prob_per_merged_nrem_event_samples = []
        for epoch in pre_prob_per_merged_nrem_event:
            sample_ind = np.random.randint(0, epoch.shape[0], size=samples_per_epoch)
            pre_prob_per_merged_nrem_event_samples.append(epoch[sample_ind, :])

        nr_rem_epochs = len(pre_prob_per_merged_rem_event_samples)
        nr_nrem_epochs = len(pre_prob_per_merged_nrem_event_samples)

        merged_data = np.vstack((np.vstack(pre_prob_per_merged_rem_event_samples),
                                 np.vstack(pre_prob_per_merged_nrem_event_samples)))

        merged_data = np.log(merged_data)
        # apply multidimensional scaling using correlations
        D = pairwise_distances(merged_data, metric="correlation")

        model = MDS(n_components=2, dissimilarity='precomputed', random_state=1)
        results = model.fit_transform(D)

        plt.scatter(results[:(samples_per_epoch*nr_rem_epochs),0], results[:(samples_per_epoch*nr_rem_epochs),1], color="r")
        plt.scatter(results[(samples_per_epoch * nr_rem_epochs):, 0], results[(samples_per_epoch * nr_rem_epochs):, 1], color="b")
        plt.show()

        # split again into REM and NREM
        split_results = np.split(results, nr_rem_epochs+nr_nrem_epochs)
        rem_results = split_results[:nr_rem_epochs]
        nrem_results = split_results[nr_rem_epochs:]

        # go through all epochs and compute spread
        rem_area = []
        rem_centers = []
        for rem_epoch in rem_results:
            # find mean
            m = np.mean(rem_epoch, axis=0)
            # apply pca to find ellipse that spans the data
            centered = rem_epoch - m
            # covariance
            c = centered.transpose() @ centered
            ev = np.linalg.eig(c)
            trans = centered @ ev[1]
            ellipse_wh = np.max(np.abs(trans), axis=0)
            area = np.round(np.pi*ellipse_wh[0]*ellipse_wh[1],3)
            # fig, ax = plt.subplots()
            # from matplotlib.patches import Ellipse
            # ax.scatter(trans[:,0], trans[:,1], color="red", label="LIKELIHOOD VECTORS, MDS")
            # e = Ellipse(xy=[0,0], width=2*ellipse_wh[0], height=2*ellipse_wh[1], facecolor="None", edgecolor="r")
            # ax.add_artist(e)
            # plt.xlim(-1.5, 1.5)
            # plt.ylim(-1.5, 1.5)
            # plt.xlabel("FIRST PC")
            # plt.ylabel("SECOND PC")
            # plt.legend()
            # plt.title("AREA ELLIPSE = "+str(area))
            # plt.show()

            rem_area.append(area)
            rem_centers.append(m)

        # go through all epochs and compute spread
        nrem_area = []
        nrem_centers = []
        for nrem_epoch in nrem_results:
            # find mean
            m = np.mean(nrem_epoch, axis=0)
            # apply pca to find ellipse that spans the data
            centered = nrem_epoch - m
            # covariance
            c = centered.transpose() @ centered
            ev = np.linalg.eig(c)
            trans = centered @ ev[1]
            ellipse_wh = np.max(np.abs(trans), axis=0)
            area = np.round(np.pi*ellipse_wh[0]*ellipse_wh[1],3)
            # fig, ax = plt.subplots()
            # from matplotlib.patches import Ellipse
            # ax.scatter(trans[:,0], trans[:,1], color="blue", label="LIKELIHOOD VECTORS, MDS")
            # e = Ellipse(xy=[0,0], width=2*ellipse_wh[0], height=2*ellipse_wh[1], facecolor="None", edgecolor="b")
            # ax.add_artist(e)
            # plt.xlim(-1.5, 1.5)
            # plt.ylim(-1.5, 1.5)
            # plt.xlabel("FIRST PC")
            # plt.ylabel("SECOND PC")
            # plt.legend()
            # plt.title("AREA ELLIPSE = "+str(area))
            # plt.show()
            nrem_area.append(area)
            nrem_centers.append(m)

        # Create figure and axes
        fig, ax = plt.subplots()
        prev_center = np.array([0,0])
        for i, (area, center, event_time) in enumerate(zip(nrem_area, nrem_centers, merged_nrem_event_times)):
            # center nrem around zero
            dist_from_prev = 0
            ax.hlines(dist_from_prev, event_time[0], event_time[1], colors="b", zorder=1000, label="CENTER NREM")
            rect = patches.Rectangle((event_time[0], dist_from_prev-0.5*area), event_time[1]-event_time[0], area, linewidth=1,
                                     edgecolor='None', facecolor='lightblue', zorder=800, label="SPREAD NREM")
            ax.add_patch(rect)

        nrem_centers = np.array(nrem_centers)
        rem_centers = np.array(rem_centers)
        # compute distance between rem and previous nrem
        if merged_events_labels[0] == 1:
            # first event is a REM event
            rem_centers = rem_centers[1:,:]
            rem_area = rem_area[1:]
            merged_rem_event_times = merged_rem_event_times[1:,:]
            nrem_centers = nrem_centers[:rem_centers.shape[0],:]
        elif merged_events_labels[0] == 0:
            nrem_centers = nrem_centers[:rem_centers.shape[0],:]

        dist = np.linalg.norm(nrem_centers-rem_centers, axis=1)

        for i, (area, d, event_time) in enumerate(zip(rem_area, dist, merged_rem_event_times)):
            # compute correlations within
            ax.hlines(d, event_time[0], event_time[1], colors="r", zorder=1000, label="REM DIST. TO PREV. NREM")
            rect = patches.Rectangle((event_time[0], d-0.5*area), event_time[1]-event_time[0], area, linewidth=1,
                                     edgecolor='None', facecolor='mistyrose', zorder=800, label="SPREAD REM")
            ax.add_patch(rect)

        handles, labels = plt.gca().get_legend_handles_labels()
        by_label = OrderedDict(zip(labels, handles))
        plt.legend(by_label.values(), by_label.keys())
        plt.xlabel("TIME / s")
        plt.ylabel("CENTER & SPREAD OF ELLIPSE")
        plt.show()

        per_event_max_corr = []

        exit()

        if part_to_analyze == "nrem":
            data = pre_prob_per_merged_nrem_event
            template = pre_prob_per_merged_rem_event
            data_identifier = 0
        elif part_to_analyze == "rem":
            data = pre_prob_per_merged_rem_event
            template = pre_prob_per_merged_nrem_event
            data_identifier = 1

        if compare_with_previous:
            if merged_events_labels[0] == data_identifier:
                # first event is NREM --> skip this one and compute for the next
                for i in np.arange(1, len(data)):
                    # get likelihoods from previous
                    prev_likeli = np.vstack(template[i-1])
                    # go through single likelhood vectors of current NREM and compare with previous REM
                    max_corr = []
                    for likeli_vec in data[i]:
                        # compute correlations
                        corr_mat = np.corrcoef(np.vstack((np.expand_dims(likeli_vec,0), prev_likeli)))
                        # want maximum correlation
                        max_corr.append(np.max(corr_mat[1:,0]))
                    per_event_max_corr.append(np.array(max_corr))
            else:
                # first event is REM
                for i in np.arange(0, len(data)):
                    # get likelihoods from previous REM
                    prev_likeli = np.vstack(template[i])
                    # go through single likelhood vectors of current NREM and compare with previous REM
                    max_corr = []
                    for likeli_vec in data[i]:
                        # compute correlations
                        corr_mat = np.corrcoef(np.log(np.vstack((np.expand_dims(likeli_vec, 0), prev_likeli))))
                        # want maximum correlation
                        max_corr.append(np.max(corr_mat[1:, 0]))
                    per_event_max_corr.append(np.array(max_corr))
        else:
            if merged_events_labels[0] == data_identifier:
                # first event is NREM
                for i in np.arange(0, len(data)):
                    # check if there is still a REM that follows
                    if i >= len(template):
                        break
                    # get likelihoods from next REM
                    next_likeli = np.vstack(template[i])
                    # go through single likelhood vectors of current NREM and compare with previous REM
                    max_corr = []
                    for likeli_vec in data[i]:
                        # compute correlations
                        corr_mat = np.corrcoef(np.vstack((np.expand_dims(likeli_vec,0), next_likeli)))
                        # want maximum correlation
                        max_corr.append(np.max(corr_mat[1:,0]))
                    per_event_max_corr.append(np.array(max_corr))
            else:
                # first event is REM
                for i in np.arange(0, len(data)):
                    # check if there is still a REM that follows
                    if (i+1) >= len(template):
                        break
                    # get likelihoods from previous REM
                    next_likeli = np.vstack(template[i+1])
                    # go through single likelhood vectors of current NREM and compare with previous REM
                    max_corr = []
                    for likeli_vec in data[i]:
                        # compute correlations
                        corr_mat = np.corrcoef(np.log(np.vstack((np.expand_dims(likeli_vec, 0), next_likeli))))
                        # want maximum correlation
                        max_corr.append(np.max(corr_mat[1:, 0]))
                    per_event_max_corr.append(np.array(max_corr))

        with open(self.params.pre_proc_dir+"temp_data/"+"test", "wb") as fp:  # Pickling
            pickle.dump(per_event_max_corr, fp)

        exit()


        time_per_rem_vec = []
        # assign temporal label to all vectors
        for prob_l, e_t in zip(pre_prob_list_rem, event_times_list_rem):
            time_per_rem_vec.append(np.linspace(e_t[0],e_t[1], prob_l.shape[0]))

        time_per_rem_vec = np.hstack(time_per_rem_vec)
        pre_prob_rem = np.vstack(pre_prob_list_rem)


        time_per_nrem_vec = []
        # assign temporal label to all vectors
        for prob_l, e_t in zip(pre_prob_list_nrem, event_times_list_nrem):
            time_per_nrem_vec.append(np.linspace(e_t[0],e_t[1], prob_l.shape[0]))

        time_per_nrem_vec = np.hstack(time_per_nrem_vec)
        pre_prob_nrem = np.vstack(pre_prob_list_nrem)

        sampling = 50

        pre_prob_rem_log = np.log(pre_prob_rem)
        pre_prob_nrem_log = np.log(pre_prob_nrem)

        pre_prob_rem_log = pre_prob_rem_log[0::sampling,:]
        time_per_rem_vec = time_per_rem_vec[0::sampling]
        pre_prob_nrem_log = pre_prob_nrem_log[0::sampling,:]
        time_per_nrem_vec = time_per_nrem_vec[0::sampling]

        comb = np.vstack((pre_prob_rem_log, pre_prob_nrem_log)).T
        # sep = np.array([pre_prob_rem_log.shape[0], comb.shape[1]])
        sep = pre_prob_rem_log.shape[0]

        result = multi_dim_scaling(act_mat=comb, param_dic=self.params)

        rem_res = result[:sep,:]
        nrem_res = result[sep:, :]

        fig = plt.figure()
        ax = fig.add_subplot(111, projection='3d')

        ax.scatter(rem_res[:,0], rem_res[:,1], time_per_rem_vec / 60, color="r", alpha=0.5)
        # ax.scatter(nrem_res[:, 0], nrem_res[:, 1], time_per_nrem_vec / 60, color="b", alpha=0.5)
        for plot_c in range(rem_res.shape[0]-1):
            ax.plot(rem_res[plot_c:plot_c+2,0], rem_res[plot_c:plot_c+2,1], time_per_rem_vec[plot_c:plot_c+2] / 60,
                    c="lightcoral", alpha=0.5)

        # for plot_c in range(nrem_res.shape[0]-1):
        #     ax.plot(nrem_res[plot_c:plot_c+2,0], nrem_res[plot_c:plot_c+2,1], time_per_nrem_vec[plot_c:plot_c+2] / 60,
        #             c="royalblue", alpha=0.8)

        # hide labels
        ax.set_yticklabels([])
        ax.set_xticklabels([])
        ax.set_zlabel("Time / min")
        # set pane alpha value to zero --> transparent
        ax.w_xaxis.set_pane_color((0.8, 0.8, 0.8, 0.0))
        ax.w_yaxis.set_pane_color((0.8, 0.8, 0.8, 0.0))
        ax.w_zaxis.set_pane_color((0.8, 0.8, 0.8, 0.0))
        plt.show()

    def memory_drift_rem_nrem_decoding_linear_separability(self, template_type, pre_file_name=None, post_file_name=None,
                                                  rem_pop_vec_threshold=100, log_transform=False):

        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_rem_event, event_times_rem, event_lengths_rem, event_duration_rem_in_s, pre_prob_rem, post_prob_rem= \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold)
        ratio_rem = np.hstack(ratio_per_rem_event)

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_nrem_event, event_times_nrem, event_lengths_nrem, event_duration_nrem_in_s, pre_prob_nrem, \
        post_prob_nrem= self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="nrem",
                                                     pop_vec_threshold=2)
        ratio_nrem = np.hstack(ratio_per_nrem_event)

        # dat_1 = np.log(pre_prob_rem[0::50, :])
        # dat_2 = np.log(pre_prob_nrem[0::50, :])

        nr_samples = 2000

        ind_rem = np.random.choice(pre_prob_rem.shape[0], nr_samples, replace=False)
        ind_nrem = np.random.choice(pre_prob_nrem.shape[0], nr_samples, replace=False)

        dat_1 = pre_prob_rem[ind_rem, :]
        dat_2 = pre_prob_nrem[ind_nrem, :]

        if log_transform:
            dat_1 = np.log(dat_1)
            dat_2 = np.log(dat_2)

        # dat_1 = pre_prob_rem[0::20, :]
        # dat_2 = pre_prob_nrem[0::20, :]

        # linear separability of probability vectors
        data = np.vstack((dat_1, dat_2)).T
        labels = np.zeros(dat_1.shape[0]+dat_2.shape[0])
        # rem --> 1
        labels[:dat_1.shape[0]+1] = 1

        acc_rem_list = []
        acc_nrem_list = []
        acc_overal_list = []
        # try linear separability for different test/training set
        for iter in range(10):
            acc_rem, acc_nrem, acc = MlMethodsOnePopulation(params=self.params).linear_separability(
                input_data=data, input_labels=labels)
            acc_rem_list.append(acc_rem)
            acc_nrem_list.append(acc_nrem)
            acc_overal_list.append(acc)

        c = "white"

        acc_nrem = np.array(acc_nrem_list)
        acc_rem = np.array(acc_rem_list)
        acc_overal = np.array(acc_overal_list)
        res = np.vstack((acc_nrem, acc_rem, acc_overal)).T

        bplot=plt.boxplot(res, positions=[1, 2, 3], patch_artist=True,
                    labels=["NREM", "REM", "ALL"],
                    boxprops=dict(color=c),
                    capprops=dict(color=c),
                    whiskerprops=dict(color=c),
                    flierprops=dict(color=c, markeredgecolor=c),
                    medianprops=dict(color=c),
                    )
        colors = ['blue', 'red', "grey"]
        for patch, color in zip(bplot['boxes'], colors):
            patch.set_facecolor(color)
        if log_transform:
            plt.title("LINEAR SEPARABILITY OF LOG-LIKELIHOOD VECTORS")
        else:
            plt.title("LINEAR SEPARABILITY OF LIKELIHOOD VECTORS")
        plt.ylabel("ACCURACY (10 SPLITS)")
        plt.grid(color="grey", axis="y")
        plt.show()

    def memory_drift_rem_nrem_likelihoods(self, template_type, pre_file_name=None, post_file_name=None,
                                                  rem_pop_vec_threshold=100, plotting=True, save_fig=False):


        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_rem_event, event_times_rem, event_lengths_rem, event_duration_rem_in_s, pre_prob_rem, post_prob_rem= \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold)

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_nrem_event, event_times_nrem, event_lengths_nrem, event_duration_nrem_in_s, pre_prob_nrem, \
        post_prob_nrem= self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="nrem",
                                                     pop_vec_threshold=2)

        pre_likeli_rem_max = np.max(pre_prob_rem, axis=1)
        pre_likeli_nrem_max = np.max(pre_prob_nrem, axis=1)

        # normalize --> maximal posterior probability
        pre_posterior_prob_rem = pre_prob_rem / np.sum(pre_prob_rem, axis=1, keepdims=True)
        pre_posterior_prob_rem_max = np.max(pre_posterior_prob_rem, axis=1)

        pre_posterior_prob_nrem = pre_prob_nrem / np.sum(pre_prob_nrem, axis=1, keepdims=True)
        pre_posterior_prob_nrem_max = np.max(pre_posterior_prob_nrem, axis=1)

        pre_likeli_rem_flat = pre_prob_rem.flatten()
        pre_likeli_nrem_flat = pre_prob_nrem.flatten()

        if plotting or save_fig:

            if save_fig:
                plt.style.use('default')

            p_mwu = mannwhitneyu(pre_prob_rem_max, pre_prob_nrem_max, alternative="greater")
            print("Max. likelihoods, MWU-test: p-value = " + str(p_mwu))


            pre_prob_rem_max_sorted = np.sort(pre_prob_rem_max)
            pre_prob_nrem_max_sorted = np.sort(pre_prob_nrem_max)

            p_rem = 1. * np.arange(pre_prob_rem_max.shape[0]) / (pre_prob_rem_max.shape[0] - 1)
            p_nrem = 1. * np.arange(pre_prob_nrem_max.shape[0]) / (pre_prob_nrem_max.shape[0] - 1)
            plt.plot(pre_prob_rem_max_sorted, p_rem, color="red", label="REM")
            plt.plot(pre_prob_nrem_max_sorted, p_nrem, color="blue", label="NREM")
            plt.gca().set_xscale("log")
            plt.xlabel("max. likelihood per PV")
            plt.ylabel("CDF")
            plt.legend()
            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig("decoding_max_likelihoods"+self.session_name+".svg", transparent="True")
                plt.close()
            else:
                plt.show()

            pre_prob_rem_flat_sorted = np.sort(pre_prob_rem_flat)
            pre_prob_nrem_flat_sorted = np.sort(pre_prob_nrem_flat)

            p_rem_flat = 1. * np.arange(pre_prob_rem_flat.shape[0]) / (pre_prob_rem_flat.shape[0] - 1)
            p_nrem_flat = 1. * np.arange(pre_prob_nrem_flat.shape[0]) / (pre_prob_nrem_flat.shape[0] - 1)
            plt.plot(pre_prob_rem_flat_sorted, p_rem_flat, color="red", label="REM")
            plt.plot(pre_prob_nrem_flat_sorted, p_nrem_flat, color="blue", label="NREM")
            plt.gca().set_xscale("log")
            plt.xlabel("Likelihoods per PV")
            plt.ylabel("CDF")
            plt.legend()
            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig("decoding_likelihoods"+self.session_name+".svg", transparent="True")
            else:
                plt.show()

        else:
            return pre_likeli_rem_max, pre_likeli_nrem_max, pre_likeli_rem_flat, pre_likeli_nrem_flat, \
                   pre_posterior_prob_nrem_max, pre_posterior_prob_rem_max

    def memory_drift_rem_nrem_autocorrelation_temporal(self, template_type, pre_file_name=None, post_file_name=None,
                                              rem_pop_vec_threshold=100, plot_for_control=True, plotting=True,
                                              bootstrapping=False, duration_for_autocorrelation_rem=10,
                                              duration_for_autocorrelation_nrem=10, save_fig=False):

        # get median bin duration of constant spike bins
        nrem_bin_dur, rem_bin_dur = self.get_constant_spike_bin_length(plotting=False, return_median=True)

        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_rem_event, event_times_rem, event_lengths_rem, event_duration_rem_in_s, pre_prob_rem, post_prob_rem = \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold)
        ratio_rem = np.hstack(ratio_per_rem_event)

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_nrem_event, event_times_nrem, event_lengths_nrem, event_duration_nrem_in_s, pre_prob_nrem, \
        post_prob_nrem = self.memory_drift_long_sleep_get_results(template_type=template_type,
                                                                  pre_file_name=pre_file_name,
                                                                  post_file_name=post_file_name, part_to_analyze="nrem",
                                                                  pop_vec_threshold=2)
        ratio_nrem = np.hstack(ratio_per_nrem_event)

        auto_corr_rem = []
        shift_array_nrem = np.arange(int(-duration_for_autocorrelation_nrem/nrem_bin_dur),
                                     int(duration_for_autocorrelation_nrem/nrem_bin_dur))
        shift_array_rem = np.arange(int(-duration_for_autocorrelation_rem/rem_bin_dur),
                                    int(duration_for_autocorrelation_rem/rem_bin_dur))
        for mode in range(pre_prob_rem.shape[1]):
            ac, _ = cross_correlate(pre_prob_rem[:, mode], pre_prob_rem[:, mode], shift_array=shift_array_rem)
            auto_corr_rem.append(ac)
        auto_corr_rem = np.vstack(auto_corr_rem)
        mean_auto_corr_rem = np.mean(auto_corr_rem, axis=0)

        auto_corr_nrem = []
        for mode in range(pre_prob_rem.shape[1]):
            ac, _ = cross_correlate(pre_prob_nrem[:, mode], pre_prob_nrem[:, mode], shift_array=shift_array_nrem)
            auto_corr_nrem.append(ac)
        auto_corr_nrem = np.vstack(auto_corr_nrem)
        mean_auto_corr_nrem = np.mean(auto_corr_nrem, axis=0)

        # auto_corr_raw = np.correlate(ratio_all, ratio_all, mode="full")
        auto_corr_raw_rem, shift_array = cross_correlate(ratio_rem, ratio_rem, shift_array=shift_array_rem)
        auto_corr_raw_nrem, _ = cross_correlate(ratio_nrem, ratio_nrem, shift_array=shift_array_nrem)

        # z-score data
        auto_corr_raw_nrem_z = (auto_corr_raw_nrem - np.mean(auto_corr_raw_nrem[:50]))/np.std(auto_corr_raw_nrem[:50])
        auto_corr_raw_rem_z = (auto_corr_raw_rem - np.mean(auto_corr_raw_rem[:50])) / np.std(auto_corr_raw_rem[:50])

        auto_corr_raw_nrem_z[int(auto_corr_raw_nrem.shape[0] / 2)] = np.nan
        auto_corr_raw_rem_z[int(auto_corr_raw_rem.shape[0] / 2)] = np.nan

        if plotting or save_fig:

            if save_fig:
                plt.style.use('default')
            plt.plot(shift_array_nrem*nrem_bin_dur, auto_corr_raw_nrem_z, c="b", label="NREM")
            plt.plot(shift_array_rem*rem_bin_dur, auto_corr_raw_rem_z, c="r", label="REM")
            plt.title("Auto-correlation of sim_ratio")
            plt.xlabel("Time (s)")
            plt.ylabel("z-scored Pearson correlation of sim_ratio")
            plt.xscale("symlog")
            plt.legend()
            # plt.xlim(-4, 4)
            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig("sim_ratio_autocorr_temporal.svg", transparent="True")
                plt.close()
            else:
                plt.show()

        nrem_test_data = auto_corr_raw_nrem_z[int(auto_corr_raw_nrem.shape[0] / 2)+1:]
        rem_test_data = auto_corr_raw_rem_z[int(auto_corr_raw_rem.shape[0] / 2) + 1:]

        def exponential(x, a, k, b):
            return a * np.exp(x * k) + b

        popt_exponential_rem, pcov_exponential_rem = optimize.curve_fit(exponential, np.arange(rem_test_data.shape[0])*rem_bin_dur,
                                                                        rem_test_data, p0=[1, -0.5, 1])
        popt_exponential_nrem, pcov_exponential_nrem = optimize.curve_fit(exponential, np.arange(nrem_test_data.shape[0])*nrem_bin_dur,
                                                                        nrem_test_data, p0=[1, -0.5, 1])
        if plotting or save_fig:

            if save_fig:
                plt.style.use('default')
            # plot fits
            plt.text(3, 10, "k = " +str(np.round(popt_exponential_rem[1], 2)), c="red" )
            plt.scatter(np.arange(rem_test_data.shape[0])*rem_bin_dur, rem_test_data, c="salmon", label="REM data")
            plt.plot((np.arange(rem_test_data.shape[0])*rem_bin_dur)[1:], exponential((np.arange(rem_test_data.shape[0])*rem_bin_dur)[1:],
                                                                    a=popt_exponential_rem[0], k=popt_exponential_rem[1],
                                                                    b=popt_exponential_rem[2]), c="red", label="REM fit")
            plt.text(0.05, 5, "k = " +str(np.round(popt_exponential_nrem[1], 2)), c="blue" )
            plt.scatter(np.arange(nrem_test_data.shape[0])*nrem_bin_dur, nrem_test_data, c="lightblue", label="NREM data")
            plt.plot((np.arange(nrem_test_data.shape[0])*nrem_bin_dur)[1:], exponential((np.arange(nrem_test_data.shape[0])*nrem_bin_dur)[1:],
                                                                    a=popt_exponential_nrem[0], k=popt_exponential_nrem[1],
                                                                    b=popt_exponential_nrem[2]), c="blue", label="NREM fit")

            plt.legend(loc=2)
            plt.xscale("log")
            plt.ylabel("Pearson R (z-scored)")
            plt.xlabel("Time (s)")
            plt.yticks([0, 5, 10, 15])
            plt.ylim(-3, 18)
            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig("exponential_fit_example.svg", transparent="True")
                plt.close()
            else:
                plt.show()

        if bootstrapping:

            # bootstrapping
            n_boots = 500
            n_samples_perc = 0.8

            nrem_exp = []
            rem_exp = []

            for boots_id in range(n_boots):
                per_ind = np.random.permutation(np.arange(rem_test_data.shape[0]))
                sel_ind = per_ind[:int(n_samples_perc*per_ind.shape[0])]
                # select subset
                x_rem = np.arange(nrem_test_data.shape[0])[sel_ind]*rem_bin_dur
                x_nrem = np.arange(nrem_test_data.shape[0])[sel_ind]*nrem_bin_dur
                y_rem = rem_test_data[sel_ind]
                y_nrem = nrem_test_data[sel_ind]
                try:
                    popt_exponential_rem, _ = optimize.curve_fit(exponential,x_rem, y_rem, p0=[1, -0.5, 1])
                    popt_exponential_nrem, _ = optimize.curve_fit(exponential, x_nrem, y_nrem, p0=[1, -0.5, 1])
                except:
                    continue

                rem_exp.append(popt_exponential_rem[1])
                nrem_exp.append(popt_exponential_nrem[1])

            if plotting:
                plt.hist(rem_exp, label="rem", color="red", bins=10, density=True)
                plt.xlabel("k from exp. function")
                plt.ylabel("density")
                plt.legend()
                plt.show()
                plt.hist(nrem_exp, label="nrem", color="blue", alpha=0.8, bins=10, density=True)
                # plt.xlim(-2,0.1)
                # plt.title("k from exponential fit (bootstrapped)\n"+"Ttest one-sided: p="+\
                #           str(ttest_ind(rem_exp, nrem_exp, alternative="greater")[1]))
                # plt.xscale("log")
                plt.show()
            else:
                return np.median(np.array(rem_exp)), np.median(np.array(nrem_exp))
        else:
            return popt_exponential_rem[1], popt_exponential_nrem[1]

    def memory_drift_rem_nrem_autocorrelation_spikes_sim_ratio(self, template_type, pre_file_name=None, post_file_name=None,
                                              rem_pop_vec_threshold=100, plot_for_control=True, plotting=True,
                                              bootstrapping=False, nr_pop_vecs=100, save_fig=False):

        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_rem_event, event_times_rem, event_lengths_rem, event_duration_rem_in_s, pre_prob_rem, post_prob_rem = \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold)
        ratio_rem = np.hstack(ratio_per_rem_event)

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_nrem_event, event_times_nrem, event_lengths_nrem, event_duration_nrem_in_s, pre_prob_nrem, \
        post_prob_nrem = self.memory_drift_long_sleep_get_results(template_type=template_type,
                                                                  pre_file_name=pre_file_name,
                                                                  post_file_name=post_file_name, part_to_analyze="nrem",
                                                                  pop_vec_threshold=2)
        ratio_nrem = np.hstack(ratio_per_nrem_event)

        auto_corr_rem = []
        shift_array = np.arange(-1*int(nr_pop_vecs),
                                     int(nr_pop_vecs))

        # for mode in range(pre_prob_rem.shape[1]):
        #     ac, _ = cross_correlate(pre_prob_rem[:, mode], pre_prob_rem[:, mode], shift_array=shift_array)
        #     auto_corr_rem.append(ac)
        # auto_corr_rem = np.vstack(auto_corr_rem)
        # mean_auto_corr_rem = np.mean(auto_corr_rem, axis=0)
        #
        # auto_corr_nrem = []
        # for mode in range(pre_prob_rem.shape[1]):
        #     ac, _ = cross_correlate(pre_prob_nrem[:, mode], pre_prob_nrem[:, mode], shift_array=shift_array)
        #     auto_corr_nrem.append(ac)
        # auto_corr_nrem = np.vstack(auto_corr_nrem)
        # mean_auto_corr_nrem = np.mean(auto_corr_nrem, axis=0)

        # auto_corr_raw = np.correlate(ratio_all, ratio_all, mode="full")
        auto_corr_raw_rem, shift_array = cross_correlate(ratio_rem, ratio_rem, shift_array=shift_array)
        auto_corr_raw_nrem, _ = cross_correlate(ratio_nrem, ratio_nrem, shift_array=shift_array)

        # z-score data
        auto_corr_raw_nrem_z = (auto_corr_raw_nrem - np.mean(auto_corr_raw_nrem[:50]))/np.std(auto_corr_raw_nrem[:50])
        auto_corr_raw_rem_z = (auto_corr_raw_rem - np.mean(auto_corr_raw_rem[:50])) / np.std(auto_corr_raw_rem[:50])

        auto_corr_raw_nrem_z[int(auto_corr_raw_nrem.shape[0] / 2)] = np.nan
        auto_corr_raw_rem_z[int(auto_corr_raw_rem.shape[0] / 2)] = np.nan

        if plotting or save_fig:

            if save_fig:
                plt.style.use('default')
            plt.plot(shift_array, auto_corr_raw_nrem_z, c="b", label="NREM")
            plt.plot(shift_array, auto_corr_raw_rem_z, c="r", label="REM")
            plt.xlabel("Shift (#spikes)")
            plt.ylabel("z-scored Pearson correlation of sim_ratio")
            plt.legend()
            plt.xticks([-100, -75, -50, -25, 0, 25, 50, 75, 100], np.array([-100, -75, -50, -25, 0, 25, 50, 75, 100]) * 12)
            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig("sim_ratio_autocorr_spikes.svg", transparent="True")
                plt.close()
            else:
                plt.show()

        nrem_test_data = auto_corr_raw_nrem_z[int(auto_corr_raw_nrem.shape[0] / 2)+1:]
        rem_test_data = auto_corr_raw_rem_z[int(auto_corr_raw_rem.shape[0] / 2) + 1:]

        def exponential(x, a, k, b):
            return a * np.exp(x * k) + b

        popt_exponential_rem, pcov_exponential_rem = optimize.curve_fit(exponential, np.arange(rem_test_data.shape[0]),
                                                                        rem_test_data, p0=[1, -0.5, 1])
        popt_exponential_nrem, pcov_exponential_nrem = optimize.curve_fit(exponential, np.arange(nrem_test_data.shape[0]),
                                                                        nrem_test_data, p0=[1, -0.5, 1])
        if plotting or save_fig:

            if save_fig:
                plt.style.use('default')
            # plot fits
            plt.text(3, 10, "k = " +str(np.round(popt_exponential_rem[1], 2)), c="red" )
            plt.scatter(np.arange(rem_test_data.shape[0]), rem_test_data, c="salmon", label="REM data")
            plt.plot((np.arange(rem_test_data.shape[0]))[1:], exponential((np.arange(rem_test_data.shape[0]))[1:],
                                                                    a=popt_exponential_rem[0], k=popt_exponential_rem[1],
                                                                    b=popt_exponential_rem[2]), c="red", label="REM fit")
            plt.text(0.05, 5, "k = " +str(np.round(popt_exponential_nrem[1], 2)), c="blue" )
            plt.scatter(np.arange(nrem_test_data.shape[0]), nrem_test_data, c="lightblue", label="NREM data")
            plt.plot((np.arange(nrem_test_data.shape[0]))[1:], exponential((np.arange(nrem_test_data.shape[0]))[1:],
                                                                    a=popt_exponential_nrem[0], k=popt_exponential_nrem[1],
                                                                    b=popt_exponential_nrem[2]), c="blue", label="NREM fit")

            plt.legend(loc=2)
            plt.ylabel("Pearson R (z-scored)")
            plt.xlabel("nr. spikes")
            plt.ylim(-3, 18)
            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig("exponential_fit_spikes.svg", transparent="True")
                plt.close()
            else:
                plt.show()

        if bootstrapping:

            # bootstrapping
            n_boots = 500
            n_samples_perc = 0.8

            nrem_exp = []
            rem_exp = []

            for boots_id in range(n_boots):
                per_ind = np.random.permutation(np.arange(rem_test_data.shape[0]))
                sel_ind = per_ind[:int(n_samples_perc*per_ind.shape[0])]
                # select subset
                x_rem = np.arange(nrem_test_data.shape[0])[sel_ind]*rem_bin_dur
                x_nrem = np.arange(nrem_test_data.shape[0])[sel_ind]*nrem_bin_dur
                y_rem = rem_test_data[sel_ind]
                y_nrem = nrem_test_data[sel_ind]
                try:
                    popt_exponential_rem, _ = optimize.curve_fit(exponential,x_rem, y_rem, p0=[1, -0.5, 1])
                    popt_exponential_nrem, _ = optimize.curve_fit(exponential, x_nrem, y_nrem, p0=[1, -0.5, 1])
                except:
                    continue

                rem_exp.append(popt_exponential_rem[1])
                nrem_exp.append(popt_exponential_nrem[1])

            if plotting:
                plt.hist(rem_exp, label="rem", color="red", bins=10, density=True)
                plt.xlabel("k from exp. function")
                plt.ylabel("density")
                plt.legend()
                plt.show()
                plt.hist(nrem_exp, label="nrem", color="blue", alpha=0.8, bins=10, density=True)
                # plt.xlim(-2,0.1)
                # plt.title("k from exponential fit (bootstrapped)\n"+"Ttest one-sided: p="+\
                #           str(ttest_ind(rem_exp, nrem_exp, alternative="greater")[1]))
                # plt.xscale("log")
                plt.show()
            else:
                return np.median(np.array(rem_exp)), np.median(np.array(nrem_exp))
        else:
            return popt_exponential_rem[1], popt_exponential_nrem[1]

    def memory_drift_rem_nrem_autocorrelation_spikes_likelihood_vectors(self, template_type, pre_file_name=None,
                                                                        post_file_name=None, rem_pop_vec_threshold=100,
                                                                        plot_for_control=True, plotting=True,
                                                                        bootstrapping=False, nr_pop_vecs=10, save_fig=False):

        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        pre_prob_list_rem, post_prob_list_rem, _ = \
            self.memory_drift_long_sleep_get_raw_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold)
        pre_prob_rem = np.vstack(pre_prob_list_rem)

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        pre_prob_list_nrem, post_prob_list_nrem, _ = self.memory_drift_long_sleep_get_raw_results(template_type=template_type,
                                                                  pre_file_name=pre_file_name,
                                                                  post_file_name=post_file_name, part_to_analyze="nrem",
                                                                  pop_vec_threshold=2)
        pre_prob_nrem = np.vstack(pre_prob_list_nrem)


        # compute correlations
        # --------------------------------------------------------------------------------------------------------------
        shift_array = np.arange(-1*int(nr_pop_vecs), int(nr_pop_vecs)+1)
        print("Computing rem autocorrelation ...")
        auto_corr_rem, _ = cross_correlate_matrices(pre_prob_rem.T, pre_prob_rem.T, shift_array=shift_array)
        print("... done!")
        print("Computing nrem autocorrelation ...")
        auto_corr_nrem, _ = cross_correlate_matrices(pre_prob_nrem.T, pre_prob_nrem.T, shift_array=shift_array)
        print("... done!")

        auto_corr_nrem_norm = (auto_corr_nrem-auto_corr_nrem[-1])/(1-auto_corr_nrem[-1])
        auto_corr_rem_norm = (auto_corr_rem-auto_corr_rem[-1])/(1-auto_corr_rem[-1])

        if plotting or save_fig:
            if save_fig:
                plt.style.use('default')
            plt.plot(shift_array, (auto_corr_nrem-auto_corr_nrem[-1])/(1-auto_corr_nrem[-1]), c="b", label="NREM")
            plt.plot(shift_array, (auto_corr_rem-auto_corr_rem[-1])/(1-auto_corr_rem[-1]), c="r", label="REM")
            plt.xlabel("Shift (#spikes)")
            plt.ylabel("Avg. Pearson correlation of likelihood vectors")
            plt.legend()
            # plt.xticks([-100, -75, -50, -25, 0, 25, 50, 75, 100], np.array([-100, -75, -50, -25, 0, 25, 50, 75, 100]) * 12)
            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig("sim_ratio_autocorr_spikes.svg", transparent="True")
                plt.close()
            else:
                plt.show()

        # fitting exponential
        # --------------------------------------------------------------------------------------------------------------
        # only take positive part (symmetric) --> exclude first data point
        nrem_test_data = auto_corr_nrem[int(auto_corr_nrem.shape[0] / 2):][1:]
        rem_test_data = auto_corr_rem[int(auto_corr_rem.shape[0] / 2):][1:]

        def exponential(x, a, k, b):
            return a * np.exp(x * k) + b

        popt_exponential_rem, pcov_exponential_rem = optimize.curve_fit(exponential, np.arange(rem_test_data.shape[0]),
                                                                        rem_test_data, p0=[1, -0.5, 1])
        popt_exponential_nrem, pcov_exponential_nrem = optimize.curve_fit(exponential, np.arange(nrem_test_data.shape[0]),
                                                                        nrem_test_data, p0=[1, -0.5, 1])
        if plotting or save_fig:

            if save_fig:
                plt.style.use('default')
            # plot fits
            plt.text(3, 10, "k = " +str(np.round(popt_exponential_rem[1], 2)), c="red" )
            plt.scatter(np.arange(rem_test_data.shape[0]), rem_test_data, c="salmon", label="REM data")
            plt.plot((np.arange(rem_test_data.shape[0]))[1:], exponential((np.arange(rem_test_data.shape[0]))[1:],
                                                                    a=popt_exponential_rem[0], k=popt_exponential_rem[1],
                                                                    b=popt_exponential_rem[2]), c="red", label="REM fit")
            plt.text(0.05, 5, "k = " +str(np.round(popt_exponential_nrem[1], 2)), c="blue" )
            plt.scatter(np.arange(nrem_test_data.shape[0]), nrem_test_data, c="lightblue", label="NREM data")
            plt.plot((np.arange(nrem_test_data.shape[0]))[1:], exponential((np.arange(nrem_test_data.shape[0]))[1:],
                                                                    a=popt_exponential_nrem[0], k=popt_exponential_nrem[1],
                                                                    b=popt_exponential_nrem[2]), c="blue", label="NREM fit")

            plt.legend(loc=2)
            plt.ylabel("Pearson R (z-scored)")
            plt.xlabel("nr. spikes")
            plt.ylim(-3, 18)
            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig("exponential_fit_spikes.svg", transparent="True")
                plt.close()
            else:
                plt.show()

        if bootstrapping:

            # bootstrapping
            n_boots = 500
            n_samples_perc = 0.8

            nrem_exp = []
            rem_exp = []

            for boots_id in range(n_boots):
                per_ind = np.random.permutation(np.arange(rem_test_data.shape[0]))
                sel_ind = per_ind[:int(n_samples_perc*per_ind.shape[0])]
                # select subset
                x_rem = np.arange(nrem_test_data.shape[0])[sel_ind]
                x_nrem = np.arange(nrem_test_data.shape[0])[sel_ind]
                y_rem = rem_test_data[sel_ind]
                y_nrem = nrem_test_data[sel_ind]
                try:
                    popt_exponential_rem, _ = optimize.curve_fit(exponential,x_rem, y_rem, p0=[1, -0.5, 1])
                    popt_exponential_nrem, _ = optimize.curve_fit(exponential, x_nrem, y_nrem, p0=[1, -0.5, 1])
                except:
                    continue

                rem_exp.append(popt_exponential_rem[1])
                nrem_exp.append(popt_exponential_nrem[1])

            if plotting:
                plt.hist(rem_exp, label="rem", color="red", bins=10, density=True)
                plt.xlabel("k from exp. function")
                plt.ylabel("density")
                plt.legend()
                plt.show()
                plt.hist(nrem_exp, label="nrem", color="blue", alpha=0.8, bins=10, density=True)
                # plt.xlim(-2,0.1)
                # plt.title("k from exponential fit (bootstrapped)\n"+"Ttest one-sided: p="+\
                #           str(ttest_ind(rem_exp, nrem_exp, alternative="greater")[1]))
                # plt.xscale("log")
                plt.show()
            else:
                return np.median(np.array(rem_exp)), np.median(np.array(nrem_exp))
        else:
            return auto_corr_rem_norm, auto_corr_nrem_norm, popt_exponential_rem[1], popt_exponential_nrem[1]

    def memory_drift_rem_nrem_spatial_decoding_autocorrelation(self, rem_pop_vec_threshold=100, plot_for_control=False,
                                                               save_fig=False, duration_for_autocorrelation_rem=22,
                                                               duration_for_autocorrelation_nrem=22):

        # get median bin duration of constant spike bins
        nrem_bin_dur, rem_bin_dur = self.get_constant_spike_bin_length(plotting=False)

        shift_array_nrem = np.arange(int(-duration_for_autocorrelation_nrem/nrem_bin_dur),
                                     int(duration_for_autocorrelation_nrem/nrem_bin_dur))
        shift_array_rem = np.arange(int(-duration_for_autocorrelation_rem/rem_bin_dur),
                                    int(duration_for_autocorrelation_rem/rem_bin_dur))

        # load model from PRE
        with open(self.params.pre_proc_dir + 'awake_ising_maps/' + self.session_params.default_pre_ising_model + '.pkl',
                  'rb') as f:
            model_dic = pickle.load(f)

        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        pre_prob_list_rem, post_prob_list_rem, event_times_list_rem = \
            self.memory_drift_long_sleep_get_raw_results(template_type="ising", pre_file_name=None,
                                                         post_file_name=None, part_to_analyze="rem",
                                                         pop_vec_threshold=rem_pop_vec_threshold)

        pre_likeli_rem = np.vstack(pre_prob_list_rem)

        # reshape likelihoods to be spatially like the pre model
        pre_likeli_rem_spatial = pre_likeli_rem.reshape(-1, model_dic["occ_map"].shape[0], model_dic["occ_map"].shape[1])

        if plot_for_control:
            # plot 5 random likelihood maps
            for i in range(5):
                plt.imshow(pre_likeli_rem_spatial[np.random.choice(np.arange(pre_likeli_rem_spatial.shape[0])),:,:]), \
                plt.show()

        dec_loc = np.zeros((pre_likeli_rem_spatial.shape[0], 2))
        for time_bin_id, time_bin in enumerate(pre_likeli_rem_spatial):
            # for each time bin --> get location of max
            dec_loc[time_bin_id] = np.unravel_index(time_bin.argmax(), time_bin.shape)


        dist_rem = np.zeros(shift_array_rem.shape[0])

        for i, shift in enumerate(shift_array_rem):
            if shift == 0:
                dist_rem[i] = np.nan
            elif shift > 0:
                dist_rem[i] = np.mean(np.linalg.norm(dec_loc[:-1*shift,:] - dec_loc[shift:,:], axis=1))
            else:
                shift = np.abs(shift)
                dist_rem[i] = np.mean(np.linalg.norm(dec_loc[shift:,:] - dec_loc[:-1*shift,:], axis=1))


        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        pre_prob_list_nrem, post_prob_list_nrem, event_times_list_nrem = \
            self.memory_drift_long_sleep_get_raw_results(template_type="ising", pre_file_name=None,
                                                         post_file_name=None, part_to_analyze="nrem",
                                                         pop_vec_threshold=2)

        pre_likeli_nrem = np.vstack(pre_prob_list_nrem)

        # reshape likelihoods to be spatially like the pre model
        pre_likeli_nrem_spatial = pre_likeli_nrem.reshape(-1, model_dic["occ_map"].shape[0], model_dic["occ_map"].shape[1])

        if plot_for_control:
            # plot 5 random likelihood maps
            for i in range(5):
                plt.imshow(pre_likeli_nrem_spatial[np.random.choice(np.arange(pre_likeli_nrem_spatial.shape[0])),:,:]), \
                plt.show()

        dec_loc = np.zeros((pre_likeli_nrem_spatial.shape[0], 2))
        for time_bin_id, time_bin in enumerate(pre_likeli_nrem_spatial):
            # for each time bin --> get location of max
            dec_loc[time_bin_id] = np.unravel_index(time_bin.argmax(), time_bin.shape)

        dist_nrem = np.zeros(shift_array_nrem.shape[0])

        for i, shift in enumerate(shift_array_nrem):
            if shift == 0:
                dist_nrem[i] = np.nan
            elif shift > 0:
                dist_nrem[i] = np.mean(np.linalg.norm(dec_loc[:-1*shift,:] - dec_loc[shift:,:], axis=1))
            else:
                shift = np.abs(shift)
                dist_nrem[i] = np.mean(np.linalg.norm(dec_loc[shift:,:] - dec_loc[:-1*shift,:], axis=1))

        if save_fig:
            plt.style.use('default')
        # bins are 5 a.u. --> 2.25 cm
        plt.plot(shift_array_nrem * nrem_bin_dur, dist_nrem * 2.25, c="b", label="NREM")
        plt.plot(shift_array_rem * rem_bin_dur, dist_rem *2.25, c="r", label="REM")
        plt.ylabel("Distance between decoded locations (cm)")
        plt.xlabel("Time (s)")
        plt.xscale("symlog")
        plt.legend()
        plt.xlim(-duration_for_autocorrelation_rem/2, duration_for_autocorrelation_rem/2)
        if save_fig:
            plt.rcParams['svg.fonttype'] = 'none'
            plt.savefig("autocorr_distance.svg", transparent="True")
            plt.close()
        else:
            plt.show()

    # decoding content
    # ------------------------------------------------------------------------------------------------------------------

    def memory_drift_analyze_nrem(self, template_type, pre_file_name=None, post_file_name=None,
                                                      rem_pop_vec_threshold=100, log_transform=False):
        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_nrem_event, event_times_nrem, event_lengths_nrem, event_duration_nrem_in_s, pre_prob_nrem, \
        post_prob_nrem = self.memory_drift_long_sleep_get_results(template_type=template_type,
                                                                  pre_file_name=pre_file_name,
                                                                  post_file_name=post_file_name, part_to_analyze="nrem",
                                                                  pop_vec_threshold=2)

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        _, _, _, _, pre_prob_rem, \
        post_prob_rem = self.memory_drift_long_sleep_get_results(template_type=template_type,
                                                                  pre_file_name=pre_file_name,
                                                                  post_file_name=post_file_name, part_to_analyze="rem",
                                                                  pop_vec_threshold=2)

        # get most likely mode
        ml_mode_nrem = np.argmax(pre_prob_nrem, axis=1)
        ml_mode_rem = np.argmax(pre_prob_rem, axis=1)

        with open("nrem_spikes","rb") as fp:
            nrem_spikes = pickle.load(fp)

        nrem_spikes = np.hstack(nrem_spikes)

        with open("rem_spikes","rb") as fp:
            rem_spikes = pickle.load(fp)

        rem_spikes = np.hstack(rem_spikes)

        mode_to_plot_1 = 5
        mode_to_plot_2 = 2
        # select only one mode
        nrem_spikes_sel_1 = nrem_spikes[:, ml_mode_nrem==mode_to_plot_1]
        rem_spikes_sel_1 = rem_spikes[:, ml_mode_rem == mode_to_plot_1]
        nrem_spikes_sel_2= nrem_spikes[:, ml_mode_nrem==mode_to_plot_2]
        rem_spikes_sel_2 = rem_spikes[:, ml_mode_rem == mode_to_plot_2]


        comb = np.hstack((rem_spikes_sel_1, rem_spikes_sel_2,
                          nrem_spikes_sel_1, nrem_spikes_sel_2))

        # active and non active set

        comb[comb>0] = 1

        print(comb.shape)
        DD = pairwise_distances(comb.T, metric="jaccard")


        seed = np.random.RandomState(seed=3)
        mds = MDS(n_components=2, max_iter=3000, eps=1e-9, random_state=seed,
                           dissimilarity="precomputed", n_jobs = 1)
        pos = mds.fit(DD).embedding_
        nmds = MDS(n_components=2, metric=False, max_iter=3000, eps=1e-12,
                            dissimilarity="precomputed", random_state=seed, n_jobs=1,
                                                                                       n_init = 1)
        npos = nmds.fit_transform(DD, init=pos)

        # npos = TSNE(n_components=2, perplexity=5).fit_transform(comb.T)

        sep_1 = rem_spikes_sel_1.shape[1]
        sep_2 = sep_1 + rem_spikes_sel_2.shape[1]
        sep_3 = sep_2 + nrem_spikes_sel_1.shape[1]

        rem_res_1 = npos[:sep_1,:]
        rem_res_2 = npos[sep_1:sep_2, :]
        nrem_res_1 = npos[sep_2:sep_3, :]
        nrem_res_2 = npos[sep_3:, :]

        plt.scatter(rem_res_1[:,0], rem_res_1[:,1], color="lightcoral", marker="*")
        plt.scatter(rem_res_2[:, 0], rem_res_2[:, 1], edgecolors="red", facecolors="none")
        plt.scatter(nrem_res_1[:,0], nrem_res_1[:,1], color="cornflowerblue", marker="*", alpha=0.5)
        plt.scatter(nrem_res_2[:, 0], nrem_res_2[:, 1], edgecolors="blue", facecolors="none")
        plt.show()

        # compute transition matrix
        # trans_mat = transition_matrix(ml_mode)
        print("HERE")

    def phmm_mode_occurrence(self, part_to_analyze="rem", n_smoothing=2000, data_length=1):

        pre_prob_list, _, _ = \
            self.memory_drift_long_sleep_get_raw_results(template_type="phmm", part_to_analyze=part_to_analyze)
        pre_prob = np.vstack(pre_prob_list)
        pre_prob = pre_prob[:int(data_length*pre_prob.shape[0]), :]

        active_mode = np.argmax(pre_prob, axis=1)
        mode, mode_occ = np.unique(active_mode, return_counts=True)
        mode_occurrence = np.zeros(pre_prob.shape[1])
        mode_occurrence[mode] = mode_occ
        smooth_post_prob = []
        m = []
        # compute probabilites in moving window
        for mode_post_prob in pre_prob.T:
            mode_post_prob_smooth = moving_average(a=mode_post_prob, n=n_smoothing)
            mode_post_prob_smooth_norm = mode_post_prob_smooth / np.max(mode_post_prob_smooth)
            smooth_post_prob.append(mode_post_prob_smooth_norm)
            coef = np.polyfit(np.linspace(0, 1, mode_post_prob_smooth_norm.shape[0]), mode_post_prob_smooth_norm, 1)
            m.append(coef[0])
            # plt.plot(mode_post_prob_smooth_norm)
            # poly1d_fn = np.poly1d(coef)
            # plt.plot(np.arange(mode_post_prob_smooth_norm.shape[0]),
            # poly1d_fn(np.linspace(0,1,mode_post_prob_smooth_norm.shape[0])), '--w')
            # plt.title(coef[0])
            # plt.show()

        m = np.array(m)

        return m, mode_occurrence

    """#################################################################################################################
    #  non-stationarity
    #################################################################################################################"""

    # regression analysis
    # ------------------------------------------------------------------------------------------------------------------

    # population vectors

    def predict_time_progression_const_spikes(self, part_to_analyze="all"):

        if part_to_analyze == "all":
            raster = []
            times = []
            first = 0
            for l_s in self.long_sleep:
                duration = l_s.get_duration_sec()
                r, t = l_s.get_spike_binned_raster(return_estimated_times=True)
                raster.append(r)
                times.append(t + first)
                first += duration

            raster = np.hstack(raster)
            times = np.hstack(times)

            # load only stable cells
            with open(self.params.pre_proc_dir + "cell_classification/" + self.params.session_name + "_k_means.pickle",
                      "rb") as f:
                class_dic = pickle.load(f)

            stable_ids = class_dic["stable_cell_ids"]
            inc_ids = class_dic["increase_cell_ids"]
            raster_stable = raster[stable_ids, :]
            raster_wo_stable = np.delete(raster, stable_ids, axis=0)

            new_ml = MlMethodsOnePopulation()
            all_r2 = []
            stable_r2 = []
            wo_stable = []

            r2 = new_ml.ridge_time_bin_progress(x=raster, y=times, new_time_bin_size=" 50 SPIKE", alpha=100,
                                                alpha_fitting=False, plotting=True)

            for i in range(15):
                r2 = new_ml.ridge_time_bin_progress(x=raster, y=times, new_time_bin_size="CONST #SPIKES", alpha=100,
                                                    alpha_fitting=False, plotting=False)
                all_r2.append(r2)

            for i in range(15):
                r2 = new_ml.ridge_time_bin_progress(x=raster_wo_stable, y=times, new_time_bin_size="CONST #SPIKES",
                                                    alpha=100,
                                                    alpha_fitting=False, plotting=False)
                wo_stable.append(r2)

            for i in range(15):
                r2 = new_ml.ridge_time_bin_progress(x=raster_stable, y=times, new_time_bin_size="CONST #SPIKES",
                                                    alpha=100,
                                                    alpha_fitting=False, plotting=False)
                stable_r2.append(r2)

            c = "white"

            stable_r2 = np.array(stable_r2)
            wo_stable = np.array(wo_stable)
            all_r2 = np.array(all_r2)

            res = np.vstack((all_r2, wo_stable, stable_r2)).T

            bplot = plt.boxplot(res, positions=[1, 2, 3], patch_artist=True,
                                labels=["ALL", "W/O STABLE", "ONLY STABLE"],
                                boxprops=dict(color=c),
                                capprops=dict(color=c),
                                whiskerprops=dict(color=c),
                                flierprops=dict(color=c, markeredgecolor=c),
                                medianprops=dict(color=c),
                                )
            colors = ["yellow", 'blue', 'red']
            for patch, color in zip(bplot['boxes'], colors):
                patch.set_facecolor(color)
            plt.title("R2 VALUES OF RIDGE REGRESSION")
            plt.ylabel("R2 (15 SPLITS)")
            plt.grid(color="grey", axis="y")
            plt.show()



        else:
            # check how many cells
            nr_cells = self.long_sleep[0].get_raster().shape[0]
            all_event_rasters = []
            all_raster_lengths = []
            start_times = []
            end_times = []

            # need to offset each sleep file by duration of previous sleep files
            first = 0
            for l_s in self.long_sleep:
                duration = l_s.get_duration_sec()
                # all_event_rasters = all_event_rasters + l_s.get_event_spike_rasters(part_to_analyze=part_to_analyze)[0]
                new_event_raster, start_time, end_time = l_s.get_event_time_bin_rasters(part_to_analyze=part_to_analyze,
                                                                                        time_bin_size=0.01)
                all_event_rasters = all_event_rasters + new_event_raster
                start_times.append(start_time + first)
                end_times.append(end_time + first)
                first += duration

            start_times = np.hstack(start_times)
            end_times = np.hstack(end_times)

            new_time_stamps = []

            for event, start, end in zip(all_event_rasters, start_times, end_times):
                new_time_stamps.extend(np.linspace(start, end, event.shape[1]))

            new_time_stamps = np.expand_dims(np.array(new_time_stamps), 0)
            all_event_rasters = np.hstack(all_event_rasters)

            scaler = int(time_bin_size / 0.01)

            down_sampled = down_sample_array_sum(x=all_event_rasters, chunk_size=scaler)
            times_down_sampled = down_sample_array_mean(x=new_time_stamps, chunk_size=scaler)
            times_down_sampled = np.squeeze(times_down_sampled)

            new_ml = MlMethodsOnePopulation()
            new_ml.ridge_time_bin_progress(x=down_sampled, y=times_down_sampled,
                                           new_time_bin_size=0.5, alpha_fitting=True)

    def predict_time_progression_time_bin(self, part_to_analyze="all", time_bin_size=0.1, nr_fits=5):

        if part_to_analyze == "all":
            raster = []
            first = 0
            for l_s in self.long_sleep:
                duration = l_s.get_duration_sec()
                r = l_s.get_raster()
                raster.append(r)
                first += duration

            raster = np.hstack(raster)
            scaler = int(time_bin_size / self.params.time_bin_size)
            raster = down_sample_array_sum(x=raster, chunk_size=scaler)

            print(raster.shape)
            #
            times = np.arange(0, raster.shape[1]) * time_bin_size

            # load only stable cells
            with open(self.params.pre_proc_dir + "cell_classification/" + self.params.session_name + "_k_means.pickle",
                      "rb") as f:
                class_dic = pickle.load(f)

            stable_ids = class_dic["stable_cell_ids"]
            inc_ids = class_dic["increase_cell_ids"]
            raster_stable = raster[stable_ids, :]
            raster_wo_stable = np.delete(raster, stable_ids, axis=0)

            new_ml = MlMethodsOnePopulation()
            all_r2 = []
            stable_r2 = []
            wo_stable = []
            for i in range(nr_fits):
                r2 = new_ml.ridge_time_bin_progress(x=raster, y=times, new_time_bin_size=time_bin_size, alpha=100,
                                                    alpha_fitting=False, plotting=False)
                all_r2.append(r2)

            for i in range(nr_fits):
                r2 = new_ml.ridge_time_bin_progress(x=raster_wo_stable, y=times, new_time_bin_size="CONST #SPIKES",
                                                    alpha=100,
                                                    alpha_fitting=False, plotting=False)
                wo_stable.append(r2)

            for i in range(nr_fits):
                r2 = new_ml.ridge_time_bin_progress(x=raster_stable, y=times, new_time_bin_size="CONST #SPIKES",
                                                    alpha=100,
                                                    alpha_fitting=False, plotting=False)
                stable_r2.append(r2)

            c = "white"

            stable_r2 = np.array(stable_r2)
            wo_stable = np.array(wo_stable)
            all_r2 = np.array(all_r2)

            res = np.vstack((all_r2, wo_stable, stable_r2)).T

            bplot = plt.boxplot(res, positions=[1, 2, 3], patch_artist=True,
                                labels=["ALL", "W/O STABLE", "ONLY STABLE"],
                                boxprops=dict(color=c),
                                capprops=dict(color=c),
                                whiskerprops=dict(color=c),
                                flierprops=dict(color=c, markeredgecolor=c),
                                medianprops=dict(color=c),
                                )
            colors = ["yellow", 'blue', 'red']
            for patch, color in zip(bplot['boxes'], colors):
                patch.set_facecolor(color)
            plt.title("R2 VALUES OF RIDGE REGRESSION")
            plt.ylabel("R2 (15 SPLITS)")
            plt.grid(color="grey", axis="y")
            plt.show()

        else:
            # check how many cells
            nr_cells = self.long_sleep[0].get_raster().shape[0]
            all_event_rasters = []
            all_raster_lengths = []
            start_times = []
            end_times = []

            # need to offset each sleep file by duration of previous sleep files
            first = 0
            for l_s in self.long_sleep:
                duration = l_s.get_duration_sec()
                # all_event_rasters = all_event_rasters + l_s.get_event_spike_rasters(part_to_analyze=part_to_analyze)[0]
                new_event_raster, start_time, end_time = l_s.get_event_time_bin_rasters(part_to_analyze=part_to_analyze,
                                                                                        time_bin_size=0.01)
                all_event_rasters = all_event_rasters + new_event_raster
                start_times.append(start_time + first)
                end_times.append(end_time + first)
                first += duration

            start_times = np.hstack(start_times)
            end_times = np.hstack(end_times)

            new_time_stamps = []

            for event, start, end in zip(all_event_rasters, start_times, end_times):
                new_time_stamps.extend(np.linspace(start, end, event.shape[1]))

            new_time_stamps = np.expand_dims(np.array(new_time_stamps), 0)
            all_event_rasters = np.hstack(all_event_rasters)

            scaler = int(time_bin_size / 0.01)

            down_sampled = down_sample_array_sum(x=all_event_rasters, chunk_size=scaler)
            times_down_sampled = down_sample_array_mean(x=new_time_stamps, chunk_size=scaler)
            times_down_sampled = np.squeeze(times_down_sampled)

            new_ml = MlMethodsOnePopulation()
            new_ml.ridge_time_bin_progress(x=down_sampled, y=times_down_sampled,
                                           new_time_bin_size=0.5, alpha_fitting=True)

    def optimal_time_bin_size_to_predict_time_progression(self, part_to_analyze="all", nr_fits=100):

        if part_to_analyze == "all":
            raster = []
            first = 0
            for l_s in self.long_sleep:
                duration = l_s.get_duration_sec()
                r = l_s.get_raster()
                raster.append(r)
                first += duration

            raster_orig = np.hstack(raster)

            time_bin_size_array = [5, 10, 20, 30, 60, 80, 100, 120, 140, 160, 180]
            # time_bin_size_array = [0.1, 1]

            res_mean = []
            res_std = []
            for time_bin_size in time_bin_size_array:
                print("COMPUTING RESULTS FOR TIME BIN SIZE: " + str(time_bin_size))

                scaler = int(time_bin_size / self.params.time_bin_size)
                raster = down_sample_array_sum(x=raster_orig, chunk_size=scaler)

                #
                times = np.arange(0, raster.shape[1]) * time_bin_size

                new_ml = MlMethodsOnePopulation()

                res_temp = []
                for i in range(nr_fits):
                    r2 = new_ml.ridge_time_bin_progress(x=raster, y=times, new_time_bin_size=time_bin_size, alpha=100,
                                                        alpha_fitting=False, plotting=False)
                    res_temp.append(r2)

                c = "white"

                res_mean.append(np.mean(np.array(res_temp)))
                res_std.append(np.std(np.array(res_temp)))

            plt.scatter(x=time_bin_size_array, y=res_mean, color="red")
            plt.errorbar(x=time_bin_size_array, y=res_mean, yerr=res_std, ls="none", color="red")
            plt.title("R2 VALUES OF RIDGE REGRESSION")
            plt.ylabel("R2: MEAN +- STD (" + str(nr_fits) + " FITS)")
            plt.xlabel("TIME BIN SIZE (s)")
            plt.grid(color="grey")
            plt.show()

        else:
            # check how many cells
            nr_cells = self.long_sleep[0].get_raster().shape[0]
            all_event_rasters = []
            all_raster_lengths = []
            start_times = []
            end_times = []

            # need to offset each sleep file by duration of previous sleep files
            first = 0
            for l_s in self.long_sleep:
                duration = l_s.get_duration_sec()
                # all_event_rasters = all_event_rasters + l_s.get_event_spike_rasters(part_to_analyze=part_to_analyze)[0]
                new_event_raster, start_time, end_time = l_s.get_event_time_bin_rasters(part_to_analyze=part_to_analyze,
                                                                                        time_bin_size=0.01)
                all_event_rasters = all_event_rasters + new_event_raster
                start_times.append(start_time + first)
                end_times.append(end_time + first)
                first += duration

            start_times = np.hstack(start_times)
            end_times = np.hstack(end_times)

            new_time_stamps = []

            for event, start, end in zip(all_event_rasters, start_times, end_times):
                new_time_stamps.extend(np.linspace(start, end, event.shape[1]))

            new_time_stamps = np.expand_dims(np.array(new_time_stamps), 0)
            all_event_rasters = np.hstack(all_event_rasters)

            scaler = int(time_bin_size / 0.01)

            down_sampled = down_sample_array_sum(x=all_event_rasters, chunk_size=scaler)
            times_down_sampled = down_sample_array_mean(x=new_time_stamps, chunk_size=scaler)
            times_down_sampled = np.squeeze(times_down_sampled)

            new_ml = MlMethodsOnePopulation()
            new_ml.ridge_time_bin_progress(x=down_sampled, y=times_down_sampled,
                                           new_time_bin_size=0.5, alpha_fitting=True)

    # correlation matrices

    def predict_time_progression_time_bin_correlations(self, plot_file_name="test_1", only_upper_triangle=True,
                                                       part_to_analyze="all",   use_pca=True,
                                                       bins_per_corr_matrix=20, only_stable_cells=False):

        if part_to_analyze == "all":

            # load only stable cells
            if only_stable_cells:
                with open(
                        self.params.pre_proc_dir + "cell_classification/" + self.params.session_name + "_k_means.pickle",
                        "rb") as f:
                    class_dic = pickle.load(f)

                stable_ids = class_dic["stable_cell_ids"]
                print("ONLY STABLE CELLS!!!")

                nr_cells = stable_ids.shape[0]

                if only_upper_triangle is True:
                    corr_mat = np.zeros((int(nr_cells * (nr_cells - 1) / 2), 0))
                else:
                    corr_mat = np.zeros((nr_cells ** 2, 0))

                for l_s in self.long_sleep:
                    c_m = l_s.get_correlation_matrices(bins_per_corr_matrix=bins_per_corr_matrix,
                                                       only_upper_triangle=only_upper_triangle, cell_selection=stable_ids)
                    corr_mat = np.hstack((corr_mat, c_m))

            else:

                nr_cells = self.long_sleep[0].get_nr_cells()

                if only_upper_triangle is True:
                    corr_mat = np.zeros((int(nr_cells * (nr_cells - 1) / 2), 0))
                else:
                    corr_mat = np.zeros((nr_cells ** 2, 0))

                for l_s in self.long_sleep:
                    c_m = l_s.get_correlation_matrices(bins_per_corr_matrix=bins_per_corr_matrix,
                                                       only_upper_triangle=only_upper_triangle)
                    corr_mat = np.hstack((corr_mat, c_m))

            times = np.arange(0, corr_mat.shape[1]) * self.params.time_bin_size * bins_per_corr_matrix

            # apply PCA to only use first n-th principal components
            if use_pca == True:
                res_mean = []
                res_std = []
                n_comp_array = np.arange(1,30,2)
                for n_components in n_comp_array:
                    pca = PCA(n_components=n_components)
                    pca_data = pca.fit_transform(X=corr_mat.T).T
                    new_ml = MlMethodsOnePopulation()
                    print("\nSTARTING RIDGE ... \n")
                    fit_res = []
                    for nr_fits in range(10):
                        r2 = new_ml.ridge_time_bin_progress(x=pca_data, y=times, new_time_bin_size="CONST #SPIKES",
                                                        alpha_fitting=True, plotting=False)
                        fit_res.append(r2)
                    res_mean.append(np.mean(np.array(fit_res)))
                    res_std.append(np.std(np.array(fit_res)))
                plt.scatter(x=n_comp_array, y=res_mean, color="red", label= "R2")
                plt.errorbar(x=n_comp_array, y=res_mean, yerr=res_std, ls="none", color="red")
                plt.title("PREDICTIVE POWER USING PC OF CORRELATIONS")
                plt.ylabel("R2 (MEAN +- STD, 10 SPLITS)")
                plt.xlabel("n-th PRINCIPAL COMPONENTS")
                plt.grid(color="grey")
                plt.show()
                pca = PCA(n_components=30)
                pca.fit_transform(X=corr_mat.T).T
                v_ex = pca.explained_variance_ratio_
                plt.plot(v_ex)
                plt.ylabel("VARIANCE EXPLAINED")
                plt.xlabel("PC")
                plt.title("PCA OF CORRELATIONS (UPPER TRIANGLE): VARIANCE EXPLAINED")
                plt.show()

            else:
                new_ml = MlMethodsOnePopulation()
                print("\nSTARTING RIDGE ... \n")
                r2 = new_ml.ridge_time_bin_progress(x=corr_mat, y=times, new_time_bin_size="CONST #SPIKES",
                                                    alpha_fitting=True, plotting=True)

                plt.savefig(self.params.pre_proc_dir + plot_file_name)

        else:
            # check how many cells
            nr_cells = self.long_sleep[0].get_raster().shape[0]
            all_event_rasters = []
            all_raster_lengths = []
            start_times = []
            end_times = []

            # need to offset each sleep file by duration of previous sleep files
            first = 0
            for l_s in self.long_sleep:
                duration = l_s.get_duration_sec()
                # all_event_rasters = all_event_rasters + l_s.get_event_spike_rasters(part_to_analyze=part_to_analyze)[0]
                new_event_raster, start_time, end_time = l_s.get_event_time_bin_rasters(part_to_analyze=part_to_analyze,
                                                                                        time_bin_size=0.01)
                all_event_rasters = all_event_rasters + new_event_raster
                start_times.append(start_time + first)
                end_times.append(end_time + first)
                first += duration

            start_times = np.hstack(start_times)
            end_times = np.hstack(end_times)

            new_time_stamps = []

            for event, start, end in zip(all_event_rasters, start_times, end_times):
                new_time_stamps.extend(np.linspace(start, end, event.shape[1]))

            new_time_stamps = np.expand_dims(np.array(new_time_stamps), 0)
            all_event_rasters = np.hstack(all_event_rasters)

            scaler = int(time_bin_size / 0.01)

            down_sampled = down_sample_array_sum(x=all_event_rasters, chunk_size=scaler)
            times_down_sampled = down_sample_array_mean(x=new_time_stamps, chunk_size=scaler)
            times_down_sampled = np.squeeze(times_down_sampled)

            new_ml = MlMethodsOnePopulation()
            new_ml.ridge_time_bin_progress(x=down_sampled, y=times_down_sampled,
                                           new_time_bin_size=0.5, alpha_fitting=True)

    def compute_rank_correlation_matrices(self):

        ranks_mean = []
        ranks_std = []
        window_size_list = [20, 50, 100, 200, 400, 600, 800, 1000, 1200]
        for bins_per_corr_matrix in window_size_list:
            nr_cells = self.long_sleep[0].get_nr_cells()

            ranks_per_window_size = []

            for l_s in self.long_sleep:
                c_m = l_s.get_correlation_matrices(bins_per_corr_matrix=bins_per_corr_matrix,
                                                   exclude_diagonal=False)
                for corr_mat in c_m.T:
                    corr_mat = np.reshape(corr_mat, (nr_cells, nr_cells))
                    ranks_per_window_size.append(np.linalg.matrix_rank(corr_mat))

            ranks_mean.append(np.mean(np.array(ranks_per_window_size)))
            ranks_std.append(np.std(np.array(ranks_per_window_size)))

        plt.scatter(x=np.array(window_size_list)*0.1, y=ranks_mean, color="red")
        plt.errorbar(x=np.array(window_size_list)*0.1, y=ranks_mean, yerr=ranks_std, ls="none", color="red")
        plt.title("RANK OF CORRELATION MATRICES")
        plt.ylabel("RANK (MEAN +- STD)")
        plt.xlabel("WINDOW SIZE (s)")
        plt.hlines(nr_cells, 0, max(np.array(window_size_list)*0.1), color="red", linestyle="--", label="FULL RANK")
        plt.legend(loc="lower right")
        plt.grid(color="grey")
        plt.show()

    def predict_time_progression_pop_vec_and_corr(self, part_to_analyze="all", time_bin_and_window_size_s=60, nr_pcs=15,
                                                  only_stable_cells=False):

        if part_to_analyze == "all":

            # load only stable cells
            if only_stable_cells:
                bins_per_corr_matrix = int(time_bin_and_window_size_s / self.params.time_bin_size)

                if self.params.stable_cell_method == "k_means":
                    # load only stable cells
                    with open(self.params.pre_proc_dir + "cell_classification/" +
                              self.params.session_name + "_k_means.pickle", "rb") as f:
                        class_dic = pickle.load(f)
                    stable_ids = class_dic["stable_cell_ids"].flatten()

                elif self.params.stable_cell_method == "mean_firing_awake":
                    # load only stable cells
                    with open(self.params.pre_proc_dir + "cell_classification/" +
                              self.params.session_name + "_mean_firing_awake.pickle", "rb") as f:
                        class_dic = pickle.load(f)

                    stable_ids = class_dic["stable_cell_ids"].flatten()

                print("ONLY STABLE CELLS!!!")

                nr_cells = stable_ids.shape[0]

                corr_mat = np.zeros((int(nr_cells * (nr_cells - 1) / 2), 0))

                # get population vectors and correlations
                raster = []
                first = 0
                for l_s in self.long_sleep:
                    duration = l_s.get_duration_sec()
                    r = l_s.get_raster()
                    raster.append(r)
                    first += duration
                    c_m = l_s.get_correlation_matrices(bins_per_corr_matrix=bins_per_corr_matrix,
                                                       only_upper_triangle=True, cell_selection=stable_ids)
                    corr_mat = np.hstack((corr_mat, c_m))

                # apply PCA to correlation matrices
                pca = PCA(n_components=nr_pcs)
                pca_data = pca.fit_transform(X=corr_mat.T).T

                raster = np.hstack(raster)
                scaler = int(time_bin_and_window_size_s / self.params.time_bin_size)
                raster = down_sample_array_sum(x=raster, chunk_size=scaler)

                # only select stable cells
                raster = raster[stable_ids,:]

                times = np.arange(0, raster.shape[1]) * time_bin_and_window_size_s
                new_ml = MlMethodsOnePopulation()

                # trim both to same length
                if raster.shape[1] > pca_data.shape[1]:
                    raster = raster[:, :pca_data.shape[1]]
                elif raster.shape[1] < pca_data.shape[1]:
                    pca_data = pca_data[:, :raster.shape[1]]

                # compute using only
                res_only_pop_vecs = []
                for nr_fits in range(100):
                    r2 = new_ml.ridge_time_bin_progress(x=raster, y=times, new_time_bin_size="CONST #SPIKES",
                                                        alpha_fitting=True, plotting=False)
                    res_only_pop_vecs.append(r2)

                raster_and_corr = np.vstack((pca_data, raster))
                res_pop_vec_and_corr = []
                for nr_fits in range(100):
                    r2 = new_ml.ridge_time_bin_progress(x=raster_and_corr, y=times, new_time_bin_size="CONST #SPIKES",
                                                        alpha_fitting=True, plotting=False)
                    res_pop_vec_and_corr.append(r2)

                res = np.vstack((np.array(res_only_pop_vecs), np.array(res_pop_vec_and_corr))).T

            else:
                bins_per_corr_matrix = int(time_bin_and_window_size_s / self.params.time_bin_size)

                nr_cells = self.long_sleep[0].get_nr_cells()

                corr_mat = np.zeros((int(nr_cells * (nr_cells - 1) / 2), 0))

                # get population vectors and correlations
                raster = []
                first = 0
                for l_s in self.long_sleep:
                    duration = l_s.get_duration_sec()
                    r = l_s.get_raster()
                    raster.append(r)
                    first += duration
                    c_m = l_s.get_correlation_matrices(bins_per_corr_matrix=bins_per_corr_matrix,
                                                       only_upper_triangle=True)
                    corr_mat = np.hstack((corr_mat, c_m))

                # apply PCA to correlation matrices
                pca = PCA(n_components=nr_pcs)
                pca_data = pca.fit_transform(X=corr_mat.T).T


                raster = np.hstack(raster)
                scaler = int(time_bin_and_window_size_s / self.params.time_bin_size)
                raster = down_sample_array_sum(x=raster, chunk_size=scaler)

                times = np.arange(0, raster.shape[1]) * time_bin_and_window_size_s
                new_ml = MlMethodsOnePopulation()

                # trim both to same length
                if raster.shape[1] > pca_data.shape[1]:
                    raster = raster[:,:pca_data.shape[1]]
                elif raster.shape[1] < pca_data.shape[1]:
                    pca_data = pca_data[:,:raster.shape[1]]

                # compute using only
                res_only_pop_vecs = []
                for nr_fits in range(100):
                    r2 = new_ml.ridge_time_bin_progress(x=raster, y=times, new_time_bin_size="CONST #SPIKES",
                                                    alpha_fitting=True, plotting=False)
                    res_only_pop_vecs.append(r2)

                raster_and_corr = np.vstack((pca_data, raster))
                res_pop_vec_and_corr = []
                for nr_fits in range(100):
                    r2 = new_ml.ridge_time_bin_progress(x=raster_and_corr, y=times, new_time_bin_size="CONST #SPIKES",
                                                    alpha_fitting=True, plotting=False)
                    res_pop_vec_and_corr.append(r2)

                res = np.vstack((np.array(res_only_pop_vecs), np.array(res_pop_vec_and_corr))).T

            c = "white"

            bplot = plt.boxplot(res, positions=[1, 2], patch_artist=True,
                                labels=["ONLY POP. VEC", "POP. VEC. AND CORR."],
                                boxprops=dict(color=c),
                                capprops=dict(color=c),
                                whiskerprops=dict(color=c),
                                flierprops=dict(color=c, markeredgecolor=c),
                                medianprops=dict(color=c),
                                )
            plt.ylabel("R2 (100 SPLITS)")
            plt.show()

    def drift_correlation_structure(self, plot_file_name="test", bins_per_corr_matrix=600,
                                    only_stable_cells=False, n_smoothing=40):
        # --------------------------------------------------------------------------------------------------------------
        # analyzes memory drift using correlation structure. Computes correlation matrix of awake behavior before and
        # correlation matrix of behavior after -> compares correlation matrix computed from sliding window during sleep
        # with before/after correlation matrix using Pearson correlation value
        #
        # parameters:   - correlation_window_size, int: size of sliding window (nr. time bins) to compute correlations
        #                 during sleep
        #
        # returns:      -
        # --------------------------------------------------------------------------------------------------------------

        nr_cells = self.long_sleep[0].get_nr_cells()

        if only_stable_cells:
            # load only stable cells
            with open(
                    self.params.pre_proc_dir + "cell_classification/" + self.params.session_name + "_k_means.pickle",
                    "rb") as f:
                class_dic = pickle.load(f)

            stable_ids = class_dic["stable_cell_ids"]

            nr_cells = stable_ids.shape[0]

            corr_sleep = np.zeros((int(nr_cells * (nr_cells - 1) / 2), 0))

            for l_s in self.long_sleep:
                c_m = l_s.get_correlation_matrices(bins_per_corr_matrix=bins_per_corr_matrix,
                                                   cell_selection=stable_ids, only_upper_triangle=True)
                corr_sleep = np.hstack((corr_sleep, c_m))

        else:

            # corr_sleep = np.zeros((nr_cells ** 2, 0))
            # only off-diagonal elements
            corr_sleep = np.zeros((int(nr_cells * (nr_cells - 1) / 2), 0))

            for l_s in self.long_sleep:
                c_m = l_s.get_correlation_matrices(bins_per_corr_matrix=bins_per_corr_matrix, only_upper_triangle=True)
                corr_sleep = np.hstack((corr_sleep, c_m))

        # define first 10% and last 10% as template
        first_10_template = np.mean(corr_sleep[:,:int(corr_sleep.shape[1]*0.1)], axis=1)
        last_10_template = np.mean(corr_sleep[:,-int(corr_sleep.shape[1]*0.1):], axis=1)

        sleep_data = corr_sleep[:,int(corr_sleep.shape[1]*0.1):-int(corr_sleep.shape[1]*0.1)]

        sim_pearson = []

        # for each sliding window compute similarity with behavior before/after
        for corr in sleep_data.T:
            sim_post = abs(pearsonr(corr, last_10_template)[0])
            sim_pre = abs(pearsonr(corr, first_10_template)[0])
            sim_pearson.append((sim_post - sim_pre) / (sim_post + sim_pre))
            # TODO: how to deal with negative correlation values

        x_axis = (np.arange(len(sim_pearson)) * bins_per_corr_matrix * self.params.time_bin_size / 60) / 60
        fig = plt.figure()
        ax = fig.add_subplot()
        ax.plot(x_axis, sim_pearson, color="red", label="PEARSON")
        plt.title("CORRELATION STRUCTURE SIMILARITY: FIRST 10% - LAST 10%\n #BINS PER WINDOW: " + str(bins_per_corr_matrix))
        plt.xlabel("TIME (hours)")
        plt.ylabel("SIMILARITY FIRST 10% - LAST 10% / PEARSON")
        plt.ylim(-1, 1)
        plt.show()

        sim_pearson = np.array(sim_pearson)

        s = sim_pearson.copy()
        control = []
        # control --> do 50 shuffles
        for i in range(50):
            np.random.shuffle(s)
            s_smooth = moving_average(a=s, n=n_smoothing)
            control.append(s_smooth)

        x_axis = (np.arange(s_smooth.shape[0]) * bins_per_corr_matrix * self.params.time_bin_size / 60) / 60
        control = np.array(control)
        con_mean = np.mean(control, axis=0)
        con_std = np.std(control, axis=0)
        fig = plt.figure()
        ax = fig.add_subplot()
        # smoothing
        sim_pearson_s = moving_average(a=np.array(sim_pearson), n=n_smoothing)
        ax.plot(x_axis, con_mean, color="grey", label="CONTROL (MEAN +- STD), 50 SHUFFLES")
        ax.plot(x_axis, con_mean + con_std, color="grey", linestyle="dashed")
        ax.plot(x_axis, con_mean - con_std, color="grey", linestyle="dashed")

        ax.plot(x_axis, sim_pearson_s, color="red", label="DATA")
        plt.title("CORRELATION STRUCTURE SIMILARITY: FIRST 10% - LAST 10%\n #BINS PER WINDOW: " + str(bins_per_corr_matrix))
        plt.xlabel("TIME (hours)")
        plt.ylabel("FIRST 10% - LAST 10% SIMILARITY")
        plt.ylim(-0.33, 0.33)
        # plt.ylim(min(sim_pearson_s), -1*min(sim_pearson_s))
        plt.legend()
        plt.show()

        # speed_smooth = moving_average(a=speed, n=n_smoothing)
        # plt.plot(x_axis, speed_smooth)
        # plt.xlabel("TIME (hours)")
        # plt.ylabel("SPEED (cm/s)")
        # plt.show()

        exit()
        plt.savefig(self.params.pre_proc_dir + plot_file_name)

    def decode_sleep_using_phmm(self, file_name, use_full_model=False, part_to_analyze="rem"):
        for l_s in self.long_sleep:
            l_s.decode_poisson_hmm_sleep(part_to_analyze=part_to_analyze, template_file_name=file_name,
                                                            use_full_model=use_full_model, return_results=False)


"""#####################################################################################################################
#   PRE AND POST CHEESEBOARD TASK
#####################################################################################################################"""


class PrePostCheeseboard:
    """Class to compare PRE and POST"""

    def __init__(self, pre, post, params, session_params=None):
        self.params = params
        self.session_params = session_params
        self.cell_type = self.params.cell_type
        self.session_name = session_params.session_name

        # initialize each phase
        self.pre = pre
        self.post = post

        # get default models for pre and post
        self.default_pre_phmm_model = self.session_params.default_pre_phmm_model
        self.default_post_phmm_model = self.session_params.default_post_phmm_model

    # cell classification
    # ------------------------------------------------------------------------------------------------------------------

    def classify_cells_firing_rate_distribution(self, alpha=0.01, test="mwu"):
        # check if cells have similar firing rates looking at before and after sleep sessions

        lcb_1_raster = self.pre.get_raster()
        lcb_2_raster = self.post.get_raster()

        stable_cell_ids = []
        increase_cell_ids = []
        decrease_cell_ids = []
        for cell_id, (cell_fir_bef, cell_fir_aft) in enumerate(zip(lcb_1_raster, lcb_2_raster)):
            if test == "ks":
                if ks_2samp(data1=cell_fir_aft, data2=cell_fir_bef, alternative="less")[1] < alpha:
                    increase_cell_ids.append(cell_id)
                elif ks_2samp(data1=cell_fir_aft, data2=cell_fir_bef, alternative="greater")[1] < alpha:
                    decrease_cell_ids.append(cell_id)
                else:
                    stable_cell_ids.append(cell_id)
            elif test == "mwu":
                if mannwhitneyu(x=cell_fir_aft, y=cell_fir_bef, alternative="less")[1] < alpha:
                    decrease_cell_ids.append(cell_id)
                elif mannwhitneyu(x=cell_fir_aft, y=cell_fir_bef, alternative="greater")[1] < alpha:
                    increase_cell_ids.append(cell_id)
                else:
                    stable_cell_ids.append(cell_id)

        print("#stable: " + str(len(stable_cell_ids)) + " ,#inc: " +
              str(len(increase_cell_ids)) + " ,#dec:" + str(len(decrease_cell_ids)))

        cell_class_dic = {
            "stable_cell_ids": np.array(stable_cell_ids),
            "decrease_cell_ids": np.array(decrease_cell_ids),
            "increase_cell_ids": np.array(increase_cell_ids)
        }

        with open(self.params.pre_proc_dir + "cell_classification/" + self.session_name + "_" + test +
                  "_awake.pickle", "wb") as f:
            pickle.dump(cell_class_dic, f, pickle.HIGHEST_PROTOCOL)

    def classify_cells_firing_rate_average_rates(self, plotting=False):
        """
        classifies cells into 3 groups (stable, increasing, decreasing) based on the change in their average firing
        rates from PRE to POST

        """

        pre_raster_mean = np.mean(self.pre.get_raster(), axis=1)
        post_raster_mean = np.mean(self.post.get_raster(), axis=1)

        norm_diff = (post_raster_mean-pre_raster_mean)/(post_raster_mean+pre_raster_mean)

        plt.hist(norm_diff, bins=30, density=True)
        plt.xlabel("PRE-POST DIFF. NORMALIZED")
        plt.ylabel("DENSITY")
        plt.show()

        stable_cell_ids = np.argwhere((norm_diff<0.33) & (-0.33<norm_diff))
        decrease_cell_ids = np.argwhere(norm_diff < -0.33)
        increase_cell_ids = np.argwhere(norm_diff > 0.33)

        if plotting:
            plt.subplot(1,2,1)
            plt.imshow(np.expand_dims(pre_raster_mean,1), interpolation='nearest', aspect='auto')
            plt.subplot(1,2,2)
            plt.imshow(np.expand_dims(post_raster_mean,1), interpolation='nearest', aspect='auto')
            plt.show()

        else:
            # create dictionary with labels
            cell_class_dic = {
                "stable_cell_ids": stable_cell_ids,
                "decrease_cell_ids": decrease_cell_ids,
                "increase_cell_ids": increase_cell_ids
            }

            with open(self.params.pre_proc_dir+"cell_classification/"+
                      self.params.session_name+"_mean_firing_awake.pickle", "wb") as f:
                pickle.dump(cell_class_dic, f, pickle.HIGHEST_PROTOCOL)

    def classify_cells_remapping(self, plotting=False, spatial_resolution=5):
        """
        classifies cells into 3 groups (stable, increasing, decreasing) based on the change in remapping properties

        """
        # get rate map from PRE
        rate_maps_pre = self.pre.get_rate_maps(spatial_resolution=spatial_resolution)
        env_dim_pre = self.pre.get_env_dim()
        occ_map_pre = self.pre.get_occ_map(spatial_resolution=spatial_resolution)
        # need to adjust dimensions of post rate map to compute overlap
        rate_maps_post = self.post.get_rate_maps(spatial_resolution=spatial_resolution, env_dim=env_dim_pre)
        occ_map_post = self.post.get_occ_map(spatial_resolution=spatial_resolution, env_dim=env_dim_pre)
        # --------------------------------------------------------------------------------------------------------------
        # compute remapping (correlation of PRE and POST rate maps)
        remapping = []

        for pre, post in zip(rate_maps_pre.T, rate_maps_post.T):
            if np.count_nonzero(pre) > 0 and np.count_nonzero(post) > 0:
                remapping.append(pearsonr(pre.flatten(), post.flatten())[0])
            else:
                remapping.append(0)

        remapping = np.array(remapping)

        # plt.hist(remapping, bins=30, density=True)
        # plt.xlabel("PRE-POST DIFF. NORMALIZED")
        # plt.ylabel("DENSITY")
        # plt.show()

        # stable cells: everything above median
        stable_cell_ids = np.argwhere(remapping > np.median(remapping)).flatten()

        # check if decreasing or increasing
        pre_raster_mean = np.mean(self.pre.get_raster(), axis=1)
        post_raster_mean = np.mean(self.post.get_raster(), axis=1)

        norm_diff = (post_raster_mean-pre_raster_mean)/(post_raster_mean+pre_raster_mean)

        decrease_cell_ids = np.argwhere((remapping < np.median(remapping)) & (norm_diff < 0)).flatten()
        increase_cell_ids = np.argwhere((remapping < np.median(remapping)) & (norm_diff > 0)).flatten()

        # create dictionary with labels
        cell_class_dic = {
            "stable_cell_ids": stable_cell_ids,
            "decrease_cell_ids": decrease_cell_ids,
            "increase_cell_ids": increase_cell_ids
        }

        with open(self.params.pre_proc_dir+"cell_classification/"+
                  self.session_name+"_remapping.pickle", "wb") as f:
            pickle.dump(cell_class_dic, f, pickle.HIGHEST_PROTOCOL)

    # remapping, goal coding & firing rate changes
    # ------------------------------------------------------------------------------------------------------------------

    def remapping_pre_post_stable(self, spatial_resolution=5, nr_shuffles=500, plot_results=True,
                                  return_distribution=True, nr_trials_to_use=None):
        """
        Estimates remapping between PRE/POST using correlation of spatial
        maps: per population vector (for each spatial bin) and per cell (for entire rate map)

        :param spatial_resolution: resolution of spatial bins in cm
        :type spatial_resolution: int
        :param nr_shuffles: how many shuffles to use for the control
        :type nr_shuffles: in
        :param plot_results: whether to plot the results
        :type plot_results: bool
        :return: percent_stable_place, p --> how many cells have a stable place field (>2std above mean of shuffle),
                 p-value of Kolmogorov-Smirnov test (data vs. shuffle)
        :rtype: float, float
        """
        if nr_trials_to_use is None:
            # get rate map from PRE
            rate_maps_pre = self.pre.get_rate_maps(spatial_resolution=spatial_resolution)
            env_dim_pre = self.pre.get_env_dim()
            occ_map_pre = self.pre.get_occ_map(spatial_resolution=spatial_resolution)
            # need to adjust dimensions of post rate map to compute overlap
            rate_maps_post = self.post.get_rate_maps(spatial_resolution=spatial_resolution, env_dim=env_dim_pre)
            occ_map_post = self.post.get_occ_map(spatial_resolution=spatial_resolution, env_dim=env_dim_pre)
        else:
            nr_trials_pre = self.pre.get_nr_of_trials()
            rate_maps_pre = self.pre.get_rate_maps(spatial_resolution=spatial_resolution,
                                                   trials_to_use=range(nr_trials_pre - nr_trials_to_use, nr_trials_pre))
            env_dim_pre = self.pre.get_env_dim()
            occ_map_pre = self.pre.get_occ_map(spatial_resolution=spatial_resolution,
                                               trials_to_use=range(nr_trials_pre - nr_trials_to_use, nr_trials_pre))

            # need to adjust dimensions of post rate map to compute overlap
            rate_maps_post = self.post.get_rate_maps(spatial_resolution=spatial_resolution, env_dim=env_dim_pre,
                                                     trials_to_use=range(0, nr_trials_to_use))
            occ_map_post = self.post.get_occ_map(spatial_resolution=spatial_resolution, env_dim=env_dim_pre,
                                                 trials_to_use=range(0, nr_trials_to_use))

        # load cell labels
        with open(self.params.pre_proc_dir + "cell_classification/" +
                  self.session_name + "_"+self.params.stable_cell_method+".pickle", "rb") as f:
            class_dic = pickle.load(f)

        stable_cells = class_dic["stable_cell_ids"].flatten()
        nr_stable_cells = stable_cells.shape[0]

        # compute remapping based on population vectors per bin
        # --------------------------------------------------------------------------------------------------------------
        remapping_pv = []

        pop_vec_pre = np.reshape(rate_maps_pre, (rate_maps_pre.shape[0]*rate_maps_pre.shape[1], rate_maps_pre.shape[2]))
        pop_vec_post = np.reshape(rate_maps_post, (rate_maps_post.shape[0] * rate_maps_post.shape[1],
                                                  rate_maps_post.shape[2]))

        # only select spatial bins that were visited in PRE and POST
        comb_occ_map = np.logical_and(occ_map_pre.flatten() > 0, occ_map_post.flatten() > 0)
        pop_vec_pre = pop_vec_pre[comb_occ_map, :]
        pop_vec_post = pop_vec_post[comb_occ_map, :]

        pop_vec_pre_stable = pop_vec_pre[:, stable_cells]
        pop_vec_post_stable = pop_vec_post[:, stable_cells]

        for pre, post in zip(pop_vec_pre_stable, pop_vec_post_stable):
            remapping_pv.append(pearsonr(pre.flatten(), post.flatten())[0])

        remapping_pv = np.nan_to_num(np.array(remapping_pv))

        remapping_pv_shuffle = []
        for i in range(nr_shuffles):
            shuffle_res = []
            per_ind = np.random.permutation(np.arange(pop_vec_post_stable.shape[0]))
            shuffled_pop_vec_post = pop_vec_post_stable[per_ind,:]
            for pre, post in zip(pop_vec_pre_stable, shuffled_pop_vec_post):
                shuffle_res.append(pearsonr(pre.flatten(), post.flatten())[0])
            remapping_pv_shuffle.append(shuffle_res)

        remapping_pv_shuffle = np.array(remapping_pv_shuffle)
        remapping_pv_shuffle_flat = remapping_pv_shuffle.flatten()

        data_sorted = np.sort(remapping_pv)
        shuffle_sorted = np.sort(remapping_pv_shuffle_flat)

        # compute statistics
        _, p = ks_2samp(remapping_pv, remapping_pv_shuffle_flat)

        # --------------------------------------------------------------------------------------------------------------
        # compute remapping (correlation of PRE and POST rate maps)
        remapping = []

        for pre, post in zip(rate_maps_pre.T, rate_maps_post.T):
            remapping.append(pearsonr(pre.flatten(), post.flatten())[0])

        remapping = np.array(remapping)
        # compute shuffled data
        remapping_shuffle = []
        for pre, post in zip(rate_maps_pre.T, rate_maps_post.T):
            shuffle_list = []
            post_flat = post.flatten()
            for i in range(nr_shuffles):
                np.random.shuffle(post_flat)
                shuffle_list.append(pearsonr(pre.flatten(), post_flat)[0])
            remapping_shuffle.append(shuffle_list)
        remapping_shuffle = np.vstack(remapping_shuffle)

        remapping_stable = remapping[stable_cells]
        remapping_shuffle_stable = remapping_shuffle[stable_cells,:]

        # check how many cells did not remapped
        const = 0
        for data, control in zip(remapping_stable, remapping_shuffle_stable):
            # if data is 2 std above the mean of control --> no significant remapping
            if data > np.mean(control)+2*np.std(control):
                const += 1

        percent_stable_place = np.round(const / nr_stable_cells * 100, 2)

        binwidth = 0.05
        min_val = min(np.hstack((remapping_shuffle[stable_cells,:].flatten(), remapping[stable_cells].flatten())))
        max_val = max(np.hstack((remapping_shuffle[stable_cells, :].flatten(), remapping[stable_cells].flatten())))

        if plot_results:
            plt.hist(remapping_shuffle[stable_cells,:].flatten(), color="gray", density=True,
                     label="SHUFFLE (n="+str(nr_shuffles)+")", bins=np.arange(min_val, max_val + binwidth, binwidth))
            plt.hist(remapping[stable_cells], label="STABLE CELLS", density=True,
                     bins=np.arange(min_val, max_val + binwidth, binwidth), color="#ffdba1", alpha=0.8)
            plt.legend()
            plt.xlabel("PEARSON R: RATE MAP PRE - RATE MAP POST")
            plt.ylabel("DENSITY")
            plt.title("% CELLS WITH CONSTANT PLACE FIELDS: "+str(percent_stable_place))
            plt.show()

            # calculate the proportional values of samples
            p_data = 1. * np.arange(data_sorted.shape[0]) / (data_sorted.shape[0] - 1)
            p_shuffle = 1. * np.arange(shuffle_sorted.shape[0]) / (shuffle_sorted.shape[0] - 1)

            plt.plot(data_sorted, p_data, label="DATA")
            plt.plot(shuffle_sorted, p_shuffle, label="SHUFFLE")
            plt.legend()
            plt.ylabel("CDF")
            plt.xlabel("PEARSON R")
            plt.title("KS, p-value = " + str(np.round(p, 0)))
            plt.show()

        else:
            if return_distribution:
                return remapping_stable, remapping_shuffle_stable, remapping_pv, remapping_pv_shuffle_flat
            else:
                return percent_stable_place, p

    def remapping_learning_vs_drift(self, spatial_resolution=5, plotting=True, nr_trials_to_use=5):
        """
        Estimates remapping between PRE/POST and between first trials/last trials in PRE using correlation of spatial
        maps: per population vector (for each spatial bin) and per cell (for entire rate map)

        :param spatial_resolution: resolution of spatial bins in cm
        :type spatial_resolution: int
        :param plotting: whether to plot the results
        :type plotting: bool
        :param nr_trials_to_use: trials for analysis (first n and last n trials in PRE)
        :type nr_trials_to_use: int
        :return: remapping_pv_learning, remapping_pv_drift, remapping_rm_learning, remapping_rm_drift --> correlation
                 values for population vectors and rate maps
        :rtype: arrays
        """

        # get rate maps from PRE: beginning and end
        nr_trials_pre = self.pre.get_nr_of_trials()
        rate_maps_pre_start = self.pre.get_rate_maps(spatial_resolution=spatial_resolution,
                                               trials_to_use=range(nr_trials_to_use))
        rate_maps_pre_end = self.pre.get_rate_maps(spatial_resolution=spatial_resolution,
                                               trials_to_use=range(nr_trials_pre - nr_trials_to_use, nr_trials_pre))
        env_dim_pre = self.pre.get_env_dim()
        occ_map_pre_start = self.pre.get_occ_map(spatial_resolution=spatial_resolution,
                                           trials_to_use=range(nr_trials_to_use))
        occ_map_pre_end = self.pre.get_occ_map(spatial_resolution=spatial_resolution,
                                           trials_to_use=range(nr_trials_pre - nr_trials_to_use, nr_trials_pre))

        # get rate maps from POST: need to adjust dimensions of post rate map to compute overlap
        rate_maps_post = self.post.get_rate_maps(spatial_resolution=spatial_resolution, env_dim=env_dim_pre,
                                                 trials_to_use=range(0, nr_trials_to_use))
        occ_map_post = self.post.get_occ_map(spatial_resolution=spatial_resolution, env_dim=env_dim_pre,
                                             trials_to_use=range(0, nr_trials_to_use))

        # compute remapping based on population vectors per bin
        # --------------------------------------------------------------------------------------------------------------

        pop_vec_pre_start = np.reshape(rate_maps_pre_start, (rate_maps_pre_start.shape[0]*rate_maps_pre_start.shape[1],
                                                     rate_maps_pre_start.shape[2]))
        pop_vec_pre_end = np.reshape(rate_maps_pre_end, (rate_maps_pre_end.shape[0]*rate_maps_pre_end.shape[1],
                                                     rate_maps_pre_end.shape[2]))
        pop_vec_post = np.reshape(rate_maps_post, (rate_maps_post.shape[0] * rate_maps_post.shape[1],
                                                  rate_maps_post.shape[2]))

        # only select spatial bins that were visited in PRE and POST
        comb_occ_map = np.logical_and(np.logical_and(occ_map_pre_start.flatten() > 0, occ_map_pre_end.flatten() > 0),
                                      occ_map_post.flatten() > 0)
        pop_vec_pre_start = pop_vec_pre_start[comb_occ_map, :]
        pop_vec_pre_end = pop_vec_pre_end[comb_occ_map, :]
        pop_vec_post = pop_vec_post[comb_occ_map, :]

        # compute remapping during learning
        remapping_pv_learning = []
        for start, end in zip(pop_vec_pre_start, pop_vec_pre_end):
            remapping_pv_learning.append(pearsonr(start.flatten(), end.flatten())[0])

        remapping_pv_learning = np.nan_to_num(np.array(remapping_pv_learning))

        # compute remapping due to drift (PRE-POST)
        remapping_pv_drift = []
        for pre, post in zip(pop_vec_pre_end, pop_vec_post):
            remapping_pv_drift.append(pearsonr(pre.flatten(), post.flatten())[0])

        remapping_pv_drift = np.nan_to_num(np.array(remapping_pv_drift))

        # compute remapping based on correlation of rate maps per cell
        # --------------------------------------------------------------------------------------------------------------

        # learning
        remapping_rm_learning = []
        for start, end in zip(rate_maps_pre_start.T, rate_maps_pre_end.T):
            if np.count_nonzero(start) and np.count_nonzero(end):
                remapping_rm_learning.append(pearsonr(start.flatten(), end.flatten())[0])

        remapping_rm_learning = np.array(remapping_rm_learning)

        # PRE-POST
        remapping_rm_drift = []
        for pre, post in zip(rate_maps_pre_end.T, rate_maps_post.T):
            if np.count_nonzero(pre) and np.count_nonzero(post):
                remapping_rm_drift.append(pearsonr(pre.flatten(), post.flatten())[0])

        remapping_rm_drift = np.array(remapping_rm_drift)

        if plotting:
            c = "white"
            bplot = plt.boxplot([remapping_pv_learning, remapping_pv_drift], positions=[1, 2], patch_artist=True,
                                labels=["Learning", "PRE-POST"],
                                boxprops=dict(color=c),
                                capprops=dict(color=c),
                                whiskerprops=dict(color=c),
                                flierprops=dict(color=c, markeredgecolor=c),
                                medianprops=dict(color=c),
                                )
            colors = ["yellow", 'blue']
            for patch, color in zip(bplot['boxes'], colors):
                patch.set_facecolor(color)
            plt.ylabel("Pop. vec. correlations (Pearson R)")
            plt.grid(color="grey", axis="y")
            plt.ylim(0,1)
            plt.show()


            c = "white"
            bplot = plt.boxplot([remapping_rm_learning, remapping_rm_drift], positions=[1, 2], patch_artist=True,
                                labels=["Learning", "PRE-POST"],
                                boxprops=dict(color=c),
                                capprops=dict(color=c),
                                whiskerprops=dict(color=c),
                                flierprops=dict(color=c, markeredgecolor=c),
                                medianprops=dict(color=c),
                                )
            colors = ["yellow", 'blue']
            for patch, color in zip(bplot['boxes'], colors):
                patch.set_facecolor(color)
            plt.ylabel("Rate map correlations (Pearson R)")
            plt.grid(color="grey", axis="y")
            plt.ylim(0,1)
            plt.show()

        else:
            return remapping_pv_learning, remapping_pv_drift, remapping_rm_learning, remapping_rm_drift

    def remapping_correlates(self, spatial_resolution=5, nr_trials_to_use=None):
        """
        Check which feature correlates with remapping from PRE to POST

        :param spatial_resolution: resolution of spatial bins in cm
        :type spatial_resolution: int
        """
        if nr_trials_to_use is None:
            # get rate map from PRE
            rate_maps_pre = self.pre.get_rate_maps(spatial_resolution=spatial_resolution)
            env_dim_pre = self.pre.get_env_dim()
            # need to adjust dimensions of post rate map to compute overlap
            rate_maps_post = self.post.get_rate_maps(spatial_resolution=spatial_resolution, env_dim=env_dim_pre)
            raster_pre = self.pre.get_raster()
            raster_post = self.post.get_raster()
        else:
            nr_trials_pre = self.pre.get_nr_of_trials()
            rate_maps_pre = self.pre.get_rate_maps(spatial_resolution=spatial_resolution,
                                                   trials_to_use=range(nr_trials_pre - nr_trials_to_use, nr_trials_pre))
            env_dim_pre = self.pre.get_env_dim()

            # need to adjust dimensions of post rate map to compute overlap
            rate_maps_post = self.post.get_rate_maps(spatial_resolution=spatial_resolution, env_dim=env_dim_pre,
                                                     trials_to_use=range(0, nr_trials_to_use))
            raster_pre = self.pre.get_raster(trials_to_use=range(nr_trials_pre - nr_trials_to_use, nr_trials_pre))
            raster_post = self.post.get_raster(trials_to_use=range(0, nr_trials_to_use))

        # --------------------------------------------------------------------------------------------------------------
        # compute remapping (correlation of PRE and POST rate maps)
        remapping = []

        for pre, post in zip(rate_maps_pre.T, rate_maps_post.T):
            if np.count_nonzero(pre) > 0 and np.count_nonzero(post) > 0:
                remapping.append(pearsonr(pre.flatten(), post.flatten())[0])
            else:
                remapping.append(np.nan)

        remapping = np.array(remapping)

        p_value_mwu = []

        for pre, post in zip(raster_pre, raster_post):
            p_value_mwu.append(mannwhitneyu(pre, post)[1])
            # p_value_mwu.append((np.mean(post)-np.mean(pre))/(np.mean(post)+np.mean(pre)))
        p_value_mwu = np.array(p_value_mwu)
        
        p_value_mwu =p_value_mwu[~np.isnan(remapping)]
        remapping = remapping[~np.isnan(remapping)]
        # plt.figure(figsize=(5,1))
        plt.scatter(p_value_mwu, remapping)
        plt.xlabel("p-value MWU")
        plt.ylabel("remapping (R rate map)")
        plt.xscale("log")
        plt.xlim(10e-75, 1)
        plt.title(pearsonr(p_value_mwu, remapping))
        plt.show()

    def firing_rate_changes(self, plotting=True, mean_or_max="mean"):
        """
        Compares average firing rates in PRE and post
        """

        # get raster
        lcb_1_raster = self.pre.get_raster()
        lcb_2_raster = self.post.get_raster()

        # load cell labels
        with open(self.params.pre_proc_dir + "cell_classification/" +
                  self.session_name + "_"+self.params.stable_cell_method+".pickle", "rb") as f:
            class_dic = pickle.load(f)

        stable_cells = class_dic["stable_cell_ids"].flatten()
        inc_cells = class_dic["increase_cell_ids"].flatten()
        dec_cells = class_dic["decrease_cell_ids"].flatten()

        if mean_or_max == "mean":
            # compute mean firing rates
            mean_pre = np.mean(lcb_1_raster, axis=1)
            mean_post = np.mean(lcb_2_raster, axis=1)
        elif mean_or_max == "max":
            mean_pre = np.max(lcb_1_raster, axis=1)
            mean_post = np.max(lcb_2_raster, axis=1)


        # plot results
        # --------------------------------------------------------------------------------------------------------------
        if plotting:
            plt.scatter(mean_pre[stable_cells], mean_post[stable_cells], c="#ffdba1", label="STABLE")
            plt.scatter(mean_pre[inc_cells], mean_post[inc_cells], c="#f7959c", label="INCREASING")
            plt.scatter(mean_pre[dec_cells], mean_post[dec_cells], c="#a0c4e4", label="DECREASING")
            plt.xlabel("MEAN FIRING PRE")
            plt.ylabel("MEAN FIRING POST")
            plt.legend()
            plt.show()

            plt.hist((mean_post[dec_cells] - mean_pre[dec_cells]) / (mean_post[dec_cells] + mean_pre[dec_cells]),
                     color="#a0c4e4", label="DECREASING", density=True, edgecolor='blue')
            plt.hist((mean_post[inc_cells] - mean_pre[inc_cells]) / (mean_post[inc_cells] + mean_pre[inc_cells]),
                     color="#f7959c", label="INCREASING", density=True, alpha=0.8, edgecolor='red')
            plt.hist((mean_post[stable_cells] - mean_pre[stable_cells]) / (mean_post[stable_cells] +
                                                                           mean_pre[stable_cells]), color="#ffdba1",
                     label="STABLE", density=True, alpha=0.7, edgecolor='yellow')

            matplotlib.rcParams['mathtext.default'] = 'regular'

            matplotlib.rcParams['text.usetex'] = True
            plt.xlabel(r'$\frac{\overline{firing\_rate}_{POST} - '
                       r'\overline{firing\_rate}_{PRE}}{\overline{firing\_rate}_{POST} + \overline{firing\_rate}_{PRE}}$')
            plt.ylabel("DENSITY")
            plt.ylim(0, 1.4)
            plt.legend(loc="upper left")
            plt.show()
        else:
            return mean_pre[stable_cells], mean_pre[dec_cells], mean_post[stable_cells], mean_post[inc_cells]

    def firing_rates(self, plotting=True, mean_or_max="mean", cells_to_use="all"):
        """
        Comutes mean or max firing rates in pre and post
        """
        # load cell labels
        with open(self.params.pre_proc_dir + "cell_classification/" +
                  self.session_name + "_" + self.params.stable_cell_method + ".pickle", "rb") as f:
            class_dic = pickle.load(f)

        # get raster
        pre_raster = self.pre.get_raster()
        post_raster = self.post.get_raster()

        if mean_or_max == "mean":
            firing_pre = np.mean(pre_raster, axis=1)/self.params.time_bin_size
            firing_post = np.mean(post_raster, axis=1)/self.params.time_bin_size
        elif mean_or_max == "max":
            firing_pre = np.max(pre_raster, axis=1)/self.params.time_bin_size
            firing_post = np.max(post_raster, axis=1)/self.params.time_bin_size

        if cells_to_use == "stable":
            cell_ids = class_dic["stable_cell_ids"].flatten()
        elif cells_to_use == "increasing":
            cell_ids = class_dic["increase_cell_ids"].flatten()
        elif cells_to_use == "decreasing":
            cell_ids = class_dic["decrease_cell_ids"].flatten()

        if cells_to_use is not None:
            firing_pre = firing_pre[cell_ids]
            firing_post = firing_post[cell_ids]

        return np.nan_to_num(firing_pre), np.nan_to_num(firing_post)

    def nr_goals_coded(self, spatial_resolution=1, plotting=True, radius=20, single_cells=True, mean_firing_thresh=None):

        # load cell labels (stable, increasing, decreasing)
        with open(self.params.pre_proc_dir + "cell_classification/" +
                  self.session_name + "_"+self.params.stable_cell_method+".pickle", "rb") as f:
            class_dic = pickle.load(f)

        stable_cells = class_dic["stable_cell_ids"].flatten()
        inc_cells = class_dic["increase_cell_ids"].flatten()
        dec_cells = class_dic["decrease_cell_ids"].flatten()

        # get goal locations
        g_l = self.pre.get_goal_locations()

        # --------------------------------------------------------------------------------------------------------------
        # PRE
        # --------------------------------------------------------------------------------------------------------------

        # get rate maps and occupancy maps at specified spatial resolution
        rate_maps_pre = self.pre.get_rate_maps(spatial_resolution=spatial_resolution)
        occ_map_pre = self.pre.get_occ_map(spatial_resolution=spatial_resolution)
        x_min,_, y_min,_= self.pre.get_env_dim()
        _,loc,_ = self.pre.get_raster_location_speed()

        # look at cells separately
        if single_cells:
            # look at stable cells first
            rate_maps_pre_stable = rate_maps_pre[:,:,stable_cells]
            per_cell_nr_goals_coded_pre_stable = nr_goals_coded_per_cell(rate_maps=rate_maps_pre_stable, occ_map=occ_map_pre,
                                                                     goal_locations=g_l, env_x_min=x_min, env_y_min=y_min,
                                                                     radius=radius, plotting=False)

            # look at decreasing cells
            rate_maps_pre_dec = rate_maps_pre[:, :, dec_cells]
            per_cell_nr_goals_coded_pre_dec = nr_goals_coded_per_cell(rate_maps=rate_maps_pre_dec, occ_map=occ_map_pre,
                                                                     goal_locations=g_l, env_x_min=x_min, env_y_min=y_min,
                                                                     radius=radius, plotting=False)

            if mean_firing_thresh is not None:
                raster_pre = self.pre.get_raster()
                mean_fir = np.mean(raster_pre, axis=1)/self.params.time_bin_size

                mean_fir_stable = mean_fir[stable_cells]
                mean_fir_dec = mean_fir[dec_cells]

                per_cell_nr_goals_coded_pre_stable = per_cell_nr_goals_coded_pre_stable[mean_fir_stable > mean_firing_thresh]
                per_cell_nr_goals_coded_pre_dec = per_cell_nr_goals_coded_pre_dec[
                    mean_fir_dec > mean_firing_thresh]



            if plotting:
                plt.hist(per_cell_nr_goals_coded_pre_stable, density=True, color="#ffdba1", label="STABLE")
                plt.hist(per_cell_nr_goals_coded_pre_dec, density=True, color="#a0c4e4", label="DECREASING", alpha=0.6)
                plt.title("GOAL CODING: PRE")
                plt.legend()
                plt.show()

            # --------------------------------------------------------------------------------------------------------------
            # POST
            # --------------------------------------------------------------------------------------------------------------
            rate_maps_post = self.post.get_rate_maps(spatial_resolution=spatial_resolution)
            occ_map_post = self.post.get_occ_map(spatial_resolution=spatial_resolution)
            x_min,_, y_min,_= self.post.get_env_dim()
            _,loc,_ = self.post.get_raster_location_speed()

            # look at stable cells first
            rate_maps_post_stable = rate_maps_post[:,:,stable_cells]
            per_cell_nr_goals_coded_post_stable = nr_goals_coded_per_cell(rate_maps=rate_maps_post_stable, occ_map=occ_map_post,
                                                                     goal_locations=g_l, env_x_min=x_min, env_y_min=y_min,
                                                                     radius=radius, plotting=False)

            # look at inc cells next
            rate_maps_post_inc = rate_maps_post[:, :, inc_cells]
            per_cell_nr_goals_coded_post_inc = nr_goals_coded_per_cell(rate_maps=rate_maps_post_inc, occ_map=occ_map_post,
                                                                     goal_locations=g_l, env_x_min=x_min, env_y_min=y_min,
                                                                     radius=radius, plotting=False)

            if mean_firing_thresh is not None:
                raster_post = self.post.get_raster()
                mean_fir = np.mean(raster_post, axis=1) / self.params.time_bin_size

                mean_fir_stable = mean_fir[stable_cells]
                mean_fir_inc = mean_fir[inc_cells]

                per_cell_nr_goals_coded_post_stable = per_cell_nr_goals_coded_post_stable[
                    mean_fir_stable > mean_firing_thresh]
                per_cell_nr_goals_coded_post_dec = per_cell_nr_goals_coded_post_inc[
                    mean_fir_inc > mean_firing_thresh]

            if plotting:
                plt.hist(per_cell_nr_goals_coded_post_stable, density=True, color="#ffdba1", label="STABLE")
                plt.hist(per_cell_nr_goals_coded_post_inc, density=True, color="#f7959c", label="INCREASING", alpha=0.6)
                plt.title("GOAL CODING: POST")
                plt.legend()
                plt.show()

            return np.median(per_cell_nr_goals_coded_pre_stable), np.median(per_cell_nr_goals_coded_pre_dec), \
                   np.median(per_cell_nr_goals_coded_post_stable), np.median(per_cell_nr_goals_coded_post_inc)

        # look at summary rate map (mean)
        else:
            # ----------------------------------------------------------------------------------------------------------
            # PRE
            # ----------------------------------------------------------------------------------------------------------
            rate_maps_pre_stable = rate_maps_pre[:,:,stable_cells]
            nr_goals_coded_pre_stable = nr_goals_coded_subset_of_cells(rate_maps=rate_maps_pre_stable,
                                                                         occ_map=occ_map_pre,
                                                                         goal_locations=g_l, env_x_min=x_min,
                                                                         env_y_min=y_min,
                                                                         radius=radius, plotting=False)

            # look at decreasing cells
            rate_maps_pre_dec = rate_maps_pre[:, :, dec_cells]
            nr_goals_coded_pre_dec = nr_goals_coded_subset_of_cells(rate_maps=rate_maps_pre_dec, occ_map=occ_map_pre,
                                                                      goal_locations=g_l, env_x_min=x_min,
                                                                      env_y_min=y_min,
                                                                      radius=radius, plotting=False)

            # ----------------------------------------------------------------------------------------------------------
            # POST
            # ----------------------------------------------------------------------------------------------------------
            rate_maps_post = self.post.get_rate_maps(spatial_resolution=spatial_resolution)
            occ_map_post = self.post.get_occ_map(spatial_resolution=spatial_resolution)
            x_min, _, y_min, _ = self.post.get_env_dim()
            _, loc, _ = self.post.get_raster_location_speed()

            # look at stable cells first
            rate_maps_post_stable = rate_maps_post[:, :, stable_cells]
            nr_goals_coded_post_stable = nr_goals_coded_subset_of_cells(rate_maps=rate_maps_post_stable,
                                                                          occ_map=occ_map_post,
                                                                          goal_locations=g_l, env_x_min=x_min,
                                                                          env_y_min=y_min,
                                                                          radius=radius, plotting=False)

            # look at inc cells next
            rate_maps_post_inc = rate_maps_post[:, :, inc_cells]
            nr_goals_coded_post_inc = nr_goals_coded_subset_of_cells(rate_maps=rate_maps_post_inc,
                                                                       occ_map=occ_map_post,
                                                                       goal_locations=g_l, env_x_min=x_min,
                                                                       env_y_min=y_min,
                                                                       radius=radius, plotting=False)

            return nr_goals_coded_pre_stable, nr_goals_coded_pre_dec, \
                   nr_goals_coded_post_stable, nr_goals_coded_post_inc

    def goal_coding(self, spatial_resolution=1, plotting=True, r=20):

        # load cell labels (stable, increasing, decreasing)
        with open(self.params.pre_proc_dir + "cell_classification/" +
                  self.session_name + "_"+self.params.stable_cell_method+".pickle", "rb") as f:
            class_dic = pickle.load(f)

        stable_cells = class_dic["stable_cell_ids"].flatten()
        inc_cells = class_dic["increase_cell_ids"].flatten()
        dec_cells = class_dic["decrease_cell_ids"].flatten()

        # get goal locations
        g_l = self.session_params.goal_locations

        # --------------------------------------------------------------------------------------------------------------
        # PRE
        # --------------------------------------------------------------------------------------------------------------

        # get rate maps and occupancy maps at specified spatial resolution
        rate_maps_pre = self.pre.get_rate_maps(spatial_resolution=spatial_resolution)
        occ_map_pre = self.pre.get_occ_map(spatial_resolution=spatial_resolution)
        x_min,_, y_min,_= self.pre.get_env_dim()
        _,loc,_ = self.pre.get_raster_location_speed()

        # look at stable cells first
        rate_maps_pre_stable = rate_maps_pre[:,:,stable_cells]

        per_cell_goal_coding_pre_stable=goal_coding_per_cell(rate_maps=rate_maps_pre_stable, occ_map=occ_map_pre,
                                                              goal_locations=g_l, env_x_min=x_min, env_y_min=y_min,
                                                              radius=r, plotting=False)

            # look at stable cells first
        rate_maps_pre_dec = rate_maps_pre[:, :, dec_cells]
        per_cell_goal_coding_pre_dec = goal_coding_per_cell(rate_maps=rate_maps_pre_dec, occ_map=occ_map_pre,
                                                              goal_locations=g_l, env_x_min=x_min, env_y_min=y_min,
                                                              radius=r, plotting=False)

        if plotting:

            gc_pre_stable = np.sort(per_cell_goal_coding_pre_stable)
            gc_pre_dec = np.sort(per_cell_goal_coding_pre_dec)
            # calculate the proportional values of samples
            p_gc_pre_stable = 1. * np.arange(gc_pre_stable.shape[0]) / (gc_pre_stable.shape[0] - 1)
            p_gc_pre_dec = 1. * np.arange(gc_pre_dec.shape[0]) / (gc_pre_dec.shape[0] - 1)
            p_mwu = mannwhitneyu(gc_pre_stable, gc_pre_dec)[1]

            plt.plot(gc_pre_stable, p_gc_pre_stable, color="#ffdba1", label="STABLE")
            plt.plot(gc_pre_dec, p_gc_pre_dec, color="#a0c4e4", label="DECREASING")
            plt.gca().set_xscale("log")
            plt.ylabel("CDF")
            plt.xlabel("GOAL CODING")
            plt.title("PRE: STABLE CELLS vs. INC. CELLS \n" + "MWU , p-value = " + str(np.round(p_mwu, 5)))
            plt.legend()
            plt.show()


        # --------------------------------------------------------------------------------------------------------------
        # PRE
        # --------------------------------------------------------------------------------------------------------------

        rate_maps_post = self.post.get_rate_maps(spatial_resolution=spatial_resolution)
        occ_map_post = self.post.get_occ_map(spatial_resolution=spatial_resolution)
        x_min,_, y_min,_= self.post.get_env_dim()
        _,loc,_ = self.post.get_raster_location_speed()

        # look at stable cells first
        rate_maps_post_stable = rate_maps_post[:,:,stable_cells]
        per_cell_goal_coding_post_stable = goal_coding_per_cell(rate_maps=rate_maps_post_stable, occ_map=occ_map_post,
                                                              goal_locations=g_l, env_x_min=x_min, env_y_min=y_min,
                                                              radius=20, plotting=False)
        # look at inc cells
        rate_maps_post_inc = rate_maps_post[:,:,inc_cells]
        per_cell_goal_coding_post_inc = goal_coding_per_cell(rate_maps=rate_maps_post_inc, occ_map=occ_map_post,
                                                                goal_locations=g_l, env_x_min=x_min, env_y_min=y_min,
                                                                radius=20, plotting=False)

        if plotting:


            gc_post_stable = np.sort(per_cell_goal_coding_post_stable)
            gc_post_inc = np.sort(per_cell_goal_coding_post_inc)
            # calculate the proportional values of samples
            p_gc_post_stable = 1. * np.arange(gc_post_stable.shape[0]) / (gc_post_stable.shape[0] - 1)
            p_gc_post_inc = 1. * np.arange(gc_post_inc.shape[0]) / (gc_post_inc.shape[0] - 1)
            p_mwu = mannwhitneyu(gc_post_stable, gc_post_inc)[1]

            plt.plot(gc_post_stable, p_gc_post_stable, color="#ffdba1", label="STABLE")
            plt.plot(gc_post_inc, p_gc_post_inc, color="#f7959c", label="INCREASING")
            plt.gca().set_xscale("log")
            plt.ylabel("CDF")
            plt.xlabel("GOAL CODING")
            plt.title("POST: STABLE CELLS vs. INC. CELLS \n" + "MWU , p-value = " + str(np.round(p_mwu, 5)))
            plt.legend()
            plt.show()


            gc_post = np.sort(per_cell_goal_coding_post_stable)
            gc_pre = np.sort(per_cell_goal_coding_pre_stable)
            # calculate the proportional values of samples
            p_gc_post = 1. * np.arange(gc_post.shape[0]) / (gc_post.shape[0] - 1)
            p_gc_pre = 1. * np.arange(gc_pre.shape[0]) / (gc_pre.shape[0] - 1)
            p_mwu = mannwhitneyu(gc_post, gc_pre)[1]

            plt.plot(gc_post, p_gc_post, label="POST", color="r")
            plt.plot(gc_pre, p_gc_pre, label="PRE", color="b")
            plt.ylabel("CDF")
            plt.xlabel("GOAL CODING")
            plt.gca().set_xscale("log")
            plt.title("STABLE CELLS \n" + "MWU , p-value = " + str(np.round(p_mwu, 5)))
            plt.legend()
            plt.show()

        return np.median(per_cell_goal_coding_pre_stable), np.median(per_cell_goal_coding_post_stable), \
                np.median(per_cell_goal_coding_pre_dec), np.median(per_cell_goal_coding_post_inc)

    def distance_peak_firing_closest_goal(self, plotting=True):
        # load all rate maps
        rate_maps_pre = self.pre.get_rate_maps(spatial_resolution=1)
        rate_maps_post = self.post.get_rate_maps(spatial_resolution=1)

        # get goal locations
        g_l = self.pre.get_goal_locations()

        x_min_pre,_, y_min_pre,_= self.pre.get_env_dim()
        x_min_post, _, y_min_post, _ = self.post.get_env_dim()

        # load cell labels
        with open(self.params.pre_proc_dir + "cell_classification/" +
                  self.session_name + "_" + self.params.stable_cell_method + ".pickle",
                  "rb") as f:
            class_dic = pickle.load(f)

        stable_cell_ids = class_dic["stable_cell_ids"].flatten()

        inc_cell_ids = class_dic["increase_cell_ids"].flatten()

        dec_cell_ids = class_dic["decrease_cell_ids"].flatten()

        rate_maps_stable_pre = rate_maps_pre[:,:,stable_cell_ids]
        rate_maps_dec_pre = rate_maps_pre[:,:,dec_cell_ids]

        rate_maps_stable_post = rate_maps_post[:,:,stable_cell_ids]
        rate_maps_inc_post = rate_maps_post[:,:, inc_cell_ids]

        min_distances_stable_pre = distance_peak_firing_to_closest_goal(rate_maps=rate_maps_stable_pre,
                                                                    goal_locations=g_l,
                                                                    env_x_min=x_min_pre, env_y_min=y_min_pre)

        min_distances_dec_pre = distance_peak_firing_to_closest_goal(rate_maps=rate_maps_dec_pre,
                                                                    goal_locations=g_l,
                                                                    env_x_min=x_min_pre, env_y_min=y_min_pre)

        min_distances_stable_post = distance_peak_firing_to_closest_goal(rate_maps=rate_maps_stable_post,
                                                                    goal_locations=g_l,
                                                                    env_x_min=x_min_post, env_y_min=y_min_post)

        min_distances_inc_post = distance_peak_firing_to_closest_goal(rate_maps=rate_maps_inc_post,
                                                                    goal_locations=g_l,
                                                                    env_x_min=x_min_post, env_y_min=y_min_post)


        if plotting:
            stable_sorted_pre = np.sort(min_distances_stable_pre)
            dec_sorted_pre = np.sort(min_distances_dec_pre)
            inc_sorted_post = np.sort(min_distances_inc_post)
            stable_sorted_post = np.sort(min_distances_stable_post)

            p_stable_pre = 1. * np.arange(stable_sorted_pre.shape[0]) / (stable_sorted_pre.shape[0] - 1)
            p_dec_pre = 1. * np.arange(dec_sorted_pre.shape[0]) / (dec_sorted_pre.shape[0] - 1)
            p_inc_post = 1. * np.arange(inc_sorted_post.shape[0]) / (inc_sorted_post.shape[0] - 1)
            p_stable_post = 1. * np.arange(stable_sorted_post.shape[0]) / (stable_sorted_post.shape[0] - 1)

            plt.plot(stable_sorted_pre, p_stable_pre, color="#ffdba1", label="STABLE")
            plt.plot(dec_sorted_pre, p_dec_pre, color="#a0c4e4", label="DECREASING")

            plt.legend()
            plt.xlabel("Min. distance: peak firing loc. to closest goal / cm")
            plt.ylabel("CDF")
            plt.title("PRE")
            plt.show()

            plt.plot(stable_sorted_post, p_stable_post, color="#ffdba1", label="STABLE")
            plt.plot(inc_sorted_post, p_inc_post, color="red", label="INCREASING")

            plt.legend()
            plt.xlabel("Min. distance: peak firing loc. to closest goal / cm")
            plt.ylabel("CDF")
            plt.title("POST")
            plt.show()

            plt.plot(dec_sorted_pre, p_dec_pre, color="#a0c4e4", label="DECREASING")
            plt.plot(inc_sorted_post, p_inc_post, color="red", label="INCREASING")

            plt.legend()
            plt.xlabel("Min. distance: peak firing loc. to closest goal / cm")
            plt.ylabel("CDF")
            plt.title("PRE-POST")
            plt.show()

            plt.plot(stable_sorted_pre, p_stable_pre, color="b", label="PRE")
            plt.plot(stable_sorted_post, p_stable_post, color="r", label="POST")

            plt.legend()
            plt.xlabel("Min. distance: peak firing loc. to closest goal / cm")
            plt.ylabel("CDF")
            plt.title("STABLE CELLS: PRE-POST")
            plt.show()

        else:
            return min_distances_stable_pre, min_distances_dec_pre, min_distances_stable_post, min_distances_inc_post

    # spatial information
    # ------------------------------------------------------------------------------------------------------------------

    def spatial_information_per_cell(self, spatial_resolution=10, only_visited=True, plotting=True,
                                     info_measure="sparsity", return_p_values=False, remove_nan=True):
        """
        computes spatial information per cell using Skaggs information or sparsity

        @param spatial_resolution: spatial resolution of rate maps to be used to compute spatial information
        @type spatial_resolution: int
        @param only_visited: whether to only use visited spatial bins
        @type only_visited: bool
        """

        # load cell labels (stable, increasing, decreasing)
        with open(self.params.pre_proc_dir + "cell_classification/" +
                  self.session_name + "_"+self.params.stable_cell_method+".pickle", "rb") as f:
            class_dic = pickle.load(f)

        stable_cells = class_dic["stable_cell_ids"].flatten()
        inc_cells = class_dic["increase_cell_ids"].flatten()
        dec_cells = class_dic["decrease_cell_ids"].flatten()

        raster_pre = self.pre.get_raster()
        mean_pre = np.mean(raster_pre, axis=1)
        raster_post = self.post.get_raster()
        mean_post = np.mean(raster_post, axis=1)

        # dec_cells = dec_cells[mean_post[dec_cells] < 0.05]
        # stable_cells = stable_cells[mean_pre[stable_cells] > 0]
        # inc_cells = inc_cells[mean_pre[inc_cells] > 0]

        # get rate maps and occupancy maps at specified spatial resolution
        rate_maps_pre = self.pre.get_rate_maps(spatial_resolution=spatial_resolution)
        rate_maps_post = self.post.get_rate_maps(spatial_resolution=spatial_resolution)
        occ_map_pre = self.pre.get_occ_map(spatial_resolution=spatial_resolution)
        occ_map_post = self.post.get_occ_map(spatial_resolution=spatial_resolution)

        # reshape --> 2D bins to vector
        rate_maps_pre = np.reshape(rate_maps_pre,
                                   (rate_maps_pre.shape[0] * rate_maps_pre.shape[1], rate_maps_pre.shape[2]))
        occ_map_pre = np.reshape(occ_map_pre, (occ_map_pre.shape[0] * occ_map_pre.shape[1]))
        rate_maps_post = np.reshape(rate_maps_post,
                                   (rate_maps_post.shape[0] * rate_maps_post.shape[1], rate_maps_post.shape[2]))
        occ_map_post = np.reshape(occ_map_post, (occ_map_post.shape[0] * occ_map_post.shape[1]))

        # compute occupancy probabilities
        prob_occ_pre = occ_map_pre / occ_map_pre.sum()
        prob_occ_post = occ_map_post / occ_map_post.sum()

        if only_visited:
            # only use bins that were visited
            prob_occ_pre = occ_map_pre[occ_map_pre>0]/occ_map_pre[occ_map_pre>0].sum()
            rate_maps_pre = rate_maps_pre[occ_map_pre>0,:]

            prob_occ_post = occ_map_post[occ_map_post>0]/occ_map_post[occ_map_post>0].sum()
            rate_maps_post = rate_maps_post[occ_map_post>0,:]

        # initialize sparsity and skaggs info list
        sparsity_pre = []
        skaggs_info_pre_per_sec = []
        skaggs_info_pre_per_spike = []

        # compute for PRE
        # --------------------------------------------------------------------------------------------------------------
        for cell_id, rate_map_pre in enumerate(rate_maps_pre.T):
            if np.count_nonzero(rate_map_pre) == 0:
                sparse_cell = np.nan
                skaggs_info_per_sec = np.nan
                skaggs_info_per_spike = np.nan
            else:
                # compute sparsity
                # sparse_cell = np.round(np.mean(rate_map_pre) ** 2 / np.mean(np.square(rate_map_pre)), 4)

                sparse_cell = (np.sum(prob_occ_pre * rate_map_pre) ** 2) / np.sum(prob_occ_pre * (rate_map_pre ** 2))

                # find good bins so that there is no problem with the log
                good_bins = (rate_map_pre / rate_map_pre.mean() > 0.0000001)
                mean_rate = np.sum(rate_map_pre[good_bins]*prob_occ_pre[good_bins])
                skaggs_info_per_sec = np.sum(rate_map_pre[good_bins]*prob_occ_pre[good_bins]*
                                             np.log(rate_map_pre[good_bins]/mean_rate))
                skaggs_info_per_spike = np.sum(rate_map_pre[good_bins]/mean_rate*prob_occ_pre[good_bins]*
                                             np.log(rate_map_pre[good_bins]/mean_rate))

            sparsity_pre.append(sparse_cell)
            skaggs_info_pre_per_sec.append(skaggs_info_per_sec)
            skaggs_info_pre_per_spike.append(skaggs_info_per_spike)

        # compute for POST
        # --------------------------------------------------------------------------------------------------------------

        sparsity_post = []
        skaggs_info_post_per_sec = []
        skaggs_info_post_per_spike = []

        for rate_map_post in rate_maps_post.T:
            if np.count_nonzero(rate_map_post) == 0:
                sparse_cell = np.nan
                skaggs_info_per_sec = np.nan
                skaggs_info_per_spike = np.nan
            else:
                # compute sparsity
                # sparse_cell = np.round(np.mean(rate_map_post.flatten()) ** 2 /
                #                        np.mean(np.square(rate_map_post.flatten())), 4)

                sparse_cell = (np.sum(prob_occ_post * rate_map_post) ** 2) / np.sum(prob_occ_post * (rate_map_post ** 2))

                good_bins = (rate_map_post / rate_map_post.mean() > 0.0000001)
                mean_rate = np.sum(rate_map_post[good_bins]*prob_occ_post[good_bins])
                skaggs_info_per_sec = np.sum(rate_map_post[good_bins]*prob_occ_post[good_bins]*
                                         np.log(rate_map_post[good_bins]/mean_rate))

                skaggs_info_per_spike = np.sum(rate_map_post[good_bins]/mean_rate*prob_occ_post[good_bins]*
                                         np.log(rate_map_post[good_bins]/mean_rate))

                # plt.imshow(np.expand_dims(rate_map_post,1), interpolation='nearest', aspect='auto')
                # plt.title(str(skaggs_info_sec)+"\n"+str(sparse_cell))
                # plt.show()

            sparsity_post.append(sparse_cell)
            skaggs_info_post_per_sec.append(skaggs_info_per_sec)
            skaggs_info_post_per_spike.append(skaggs_info_per_spike)

        if info_measure == "skaggs_spike":
            info_post = np.array(skaggs_info_post_per_spike)
            info_pre = np.array(skaggs_info_pre_per_spike)
        elif info_measure == "skaggs_second":
            info_post = np.array(skaggs_info_post_per_sec)
            info_pre = np.array(skaggs_info_pre_per_sec)
        elif info_measure == "sparsity":
            info_post = np.array(sparsity_post)
            info_pre = np.array(sparsity_pre)
        
        # compute for PRE
        pre_stable = info_pre[stable_cells]
        pre_inc = info_pre[inc_cells]
        pre_dec = info_pre[dec_cells]

        if remove_nan:
            # remove all nan values (cell wasn't firing during PRE)
            pre_stable = pre_stable[~np.isnan(pre_stable)]
            pre_dec = pre_dec[~np.isnan(pre_dec)]
            pre_inc = pre_inc[~np.isnan(pre_inc)]

        # sort for CDF
        pre_stable_sorted = np.sort(pre_stable)
        pre_dec_sorted = np.sort(pre_dec)

        p_pre_mwu = mannwhitneyu(np.nan_to_num(pre_dec_sorted), np.nan_to_num(pre_stable_sorted))[1]
        p_pre_mwu_one_sided = mannwhitneyu(pre_dec, pre_stable, alternative="greater")[1]
        
        # spatial information POST
        post_stable = info_post[stable_cells]
        post_inc = info_post[inc_cells]
        post_dec = info_post[dec_cells]

        if remove_nan:
            # remove all nan values (cell wasn't firing during PRE)
            post_stable = post_stable[~np.isnan(post_stable)]
            post_inc = post_inc[~np.isnan(post_inc)]
            post_dec = post_dec[~np.isnan(post_dec)]

        post_stable_sorted = np.sort(post_stable)
        post_inc_sorted = np.sort(post_inc)
        
        p_post_mwu = mannwhitneyu(np.nan_to_num(post_inc_sorted), np.nan_to_num(post_stable_sorted))[1]
        p_post_mwu_one_sided = mannwhitneyu(post_inc, post_stable, alternative="greater")[1]
        
        # compute for stable cells only
        p_stable_mwu = mannwhitneyu(np.nan_to_num(post_stable_sorted), np.nan_to_num(pre_stable_sorted))[1]
        
        # compute combined spatial information (pre and post)
        all_stable = np.hstack((post_stable, pre_stable))
        all_non_stable = np.hstack((pre_dec, post_inc))
        
        all_stable_sorted = np.sort(all_stable)
        all_non_stable_sorted = np.sort(all_non_stable)
        p_all_mwu_one_sided = mannwhitneyu(np.nan_to_num(all_non_stable_sorted), np.nan_to_num(pre_stable_sorted),
                                            alternative="greater")[1]
        
        # plotting
        # --------------------------------------------------------------------------------------------------------------
        if plotting:
            # PRE
            # calculate the proportional values of samples
            p_pre_stable = 1. * np.arange(pre_stable_sorted.shape[0]) / (pre_stable_sorted.shape[0] - 1)
            p_pre_dec = 1. * np.arange(pre_dec_sorted.shape[0]) / (pre_dec_sorted.shape[0] - 1)
            plt.plot(pre_stable_sorted, p_pre_stable, color="#ffdba1", label="STABLE")
            plt.plot(pre_dec_sorted, p_pre_dec, color="#a0c4e4", label="DECREASING")
            # plt.gca().set_xscale("log")
            plt.ylabel("CDF")
            plt.xlabel(info_measure)
            plt.title("PRE: STABLE CELLS vs. DEC. CELLS \n" + "MWU , p-value = " + str(np.round(p_pre_mwu_one_sided, 5)))
            plt.legend()
            plt.show()
    
            # POST
            # calculate the proportional values of samples
            p_post_stable = 1. * np.arange(post_stable_sorted.shape[0]) / (post_stable_sorted.shape[0] - 1)
            p_post_inc = 1. * np.arange(post_inc_sorted.shape[0]) / (post_inc_sorted.shape[0] - 1)
            plt.plot(post_stable_sorted, p_post_stable, color="#ffdba1", label="STABLE")
            plt.plot(post_inc_sorted, p_post_inc, color="#f7959c", label="INCREASING")
            # plt.gca().set_xscale("log")
            plt.ylabel("CDF")
            plt.xlabel(info_measure)
            plt.title("POST: STABLE CELLS vs. INC. CELLS \n" + "MWU , p-value = " + str(np.round(p_post_mwu_one_sided, 5)))
            plt.legend()
            plt.show()

            # spatial information PRE - POST for stable cells
            plt.plot(pre_stable_sorted, p_pre_stable, color="red", label="PRE")
            plt.plot(post_stable_sorted, p_post_stable, color="blue", label="POST")
            # plt.gca().set_xscale("log")
            plt.ylabel("CDF")
            plt.xlabel(info_measure)
            plt.title("PRE-POST, STABLE CELLS \n" + "MWU , p-value = " + str(np.round(p_stable_mwu, 5)))
            plt.legend()
            plt.show()
            
            # all cells 
            # calculate the proportional values of samples
            # p_all_stable = 1. * np.arange(all_stable_sorted.shape[0]) / (all_stable_sorted.shape[0] - 1)
            p_all_non_stable = 1. * np.arange(all_non_stable_sorted.shape[0]) / (all_non_stable_sorted.shape[0] - 1)
            plt.plot(pre_stable_sorted, p_pre_stable, color="#ffdba1", label="stable")
            plt.plot(all_non_stable_sorted, p_all_non_stable, color="green", label="non-stable")
            # plt.gca().set_xscale("log")
            plt.ylabel("CDF")
            plt.xlabel(info_measure)
            plt.title("POST: STABLE CELLS vs. NON-STABLE CELLS \n" + "MWU , p-value = " + str(np.round(p_all_mwu_one_sided, 5)))
            plt.legend()
            plt.show()

        else:
            if return_p_values:
                return p_pre_mwu_one_sided, p_post_mwu_one_sided, p_stable_mwu, p_all_mwu_one_sided
            else:
                return pre_stable, post_stable, pre_dec, post_inc, post_dec, pre_inc

    def location_decoding_stable_cells(self, test_perc=0.5, plotting=True):

        with open(self.params.pre_proc_dir + "cell_classification/" +
                  self.session_name +"_"+self.params.stable_cell_method + ".pickle", "rb") as f:
            class_dic = pickle.load(f)

        stable_cells = class_dic["stable_cell_ids"].flatten()

        trials_to_use = self.pre.default_trials

        first_trial = trials_to_use[0]
        last_trial = trials_to_use[-1]
        nr_test_trials = int((last_trial - first_trial) * test_perc)

        shuff_trials_to_use = np.array(np.copy(trials_to_use))
        np.random.shuffle(shuff_trials_to_use)

        test_trials = shuff_trials_to_use[:nr_test_trials]
        train_trials = shuff_trials_to_use[nr_test_trials:]

        # get rate map --> need to add x_min, y_min of environment to have proper location info
        rate_maps = self.pre.get_rate_maps(spatial_resolution=1, trials_to_use=train_trials)

        # flatten rate maps
        rate_maps_flat = np.reshape(rate_maps, (rate_maps.shape[0] * rate_maps.shape[1], rate_maps.shape[2]))

        test_raster_orig, test_loc_orig, _ = self.pre.get_raster_location_speed(trials_to_use=test_trials)

        # test_raster = test_raster_orig[:,:50]
        # test_loc = test_loc_orig[:50, :]
        test_raster = test_raster_orig
        test_loc = test_loc_orig

        test_raster_stable = test_raster[stable_cells, :]
        rate_maps_flat_stable = rate_maps_flat[:, stable_cells]
        error_stable = []
        for pop_vec, loc in zip(test_raster_stable.T, test_loc):
            if np.count_nonzero(pop_vec) == 0:
                continue
            bl = bayes_likelihood(frm=rate_maps_flat_stable.T, pop_vec=pop_vec, log_likeli=False)
            bl_area = np.reshape(bl, (rate_maps.shape[0], rate_maps.shape[1]))
            pred_bin = np.unravel_index(bl_area.argmax(), bl_area.shape)
            pred_x = pred_bin[0] + self.pre.x_min
            pred_y = pred_bin[1] + self.pre.y_min

            # plt.scatter(pred_x, pred_y, color="red")
            # plt.scatter(loc[0], loc[1], color="gray")
            error_stable.append(np.sqrt((pred_x - loc[0]) ** 2 + (pred_y - loc[1]) ** 2))

        # test in post
        test_raster_orig, test_loc_orig, _ = self.post.get_raster_location_speed(trials_to_use=None)
        test_raster = test_raster_orig
        test_loc = test_loc_orig

        test_raster_stable = test_raster[stable_cells, :]
        pred_loc_stable = []
        error_stable_post = []
        error_stable_post_shuffle = []
        for pop_vec, loc in zip(test_raster_stable.T, test_loc):
            if np.count_nonzero(pop_vec) == 0:
                continue
            bl = bayes_likelihood(frm=rate_maps_flat_stable.T, pop_vec=pop_vec, log_likeli=False)
            bl_area = np.reshape(bl, (rate_maps.shape[0], rate_maps.shape[1]))
            pred_bin = np.unravel_index(bl_area.argmax(), bl_area.shape)
            pred_x = pred_bin[0] + self.pre.x_min
            pred_y = pred_bin[1] + self.pre.y_min
            pred_loc_stable.append([pred_x, pred_y])

            # plt.scatter(pred_x, pred_y, color="red")
            # plt.scatter(loc[0], loc[1], color="gray")
            error_stable_post.append(np.sqrt((pred_x - loc[0]) ** 2 + (pred_y - loc[1]) ** 2))

            # compute control
            bl_c = bayes_likelihood(frm=rate_maps_flat_stable.T, pop_vec=np.random.permutation(pop_vec), log_likeli=False)
            bl_area_c = np.reshape(bl_c, (rate_maps.shape[0], rate_maps.shape[1]))
            pred_bin_c = np.unravel_index(bl_area_c.argmax(), bl_area_c.shape)
            pred_x_c = pred_bin_c[0] + self.pre.x_min
            pred_y_c = pred_bin_c[1] + self.pre.y_min

            error_stable_post_shuffle.append(np.sqrt((pred_x_c - loc[0]) ** 2 + (pred_y_c - loc[1]) ** 2))
        # plt.show()
        error_stable_post = np.array(error_stable_post)
        error_stable_post_shuffle = np.array(error_stable_post_shuffle)

        error_pre_stable_sorted = np.sort(error_stable)
        error_post_stable_sorted = np.sort(error_stable_post)
        error_post_stable_shuffle_sorted = np.sort(error_stable_post_shuffle)

        p_error_pre_stable = 1. * np.arange(error_pre_stable_sorted.shape[0]) / (error_pre_stable_sorted.shape[0] - 1)
        p_error_post_stable = 1. * np.arange(error_post_stable_sorted.shape[0]) / (error_post_stable_sorted.shape[0] - 1)
        p_error_post_stable_shuffle = 1. * np.arange(error_post_stable_shuffle_sorted.shape[0]) / (
                    error_post_stable_shuffle_sorted.shape[0] - 1)

        p_mwu = mannwhitneyu(error_stable, error_stable_post)[1]

        if plotting:
            plt.plot(error_pre_stable_sorted, p_error_pre_stable, color="#ffdba1", label="PRE")
            plt.plot(error_post_stable_sorted, p_error_post_stable, color="#a0c4e4", label="POST")
            plt.plot(error_post_stable_shuffle_sorted, p_error_post_stable_shuffle, color="gray", label="Control POST")
            # plt.gca().set_xscale("log")
            plt.ylabel("CDF")
            plt.xlabel("Error (cm)")
            plt.title("Stable cells: PRE - POST decoding \n" + "MWU , p-value = " + str(np.round(p_mwu, 5)))
            plt.legend()
            plt.show()
        else:
            return error_pre_stable_sorted, error_post_stable_sorted, error_post_stable_shuffle_sorted

    def location_decoding_learning_vs_drift(self):
            error_stable_learning = self.pre.decode_location_beginning_end_learning(cells_to_use="stable", plotting=False)
            p_error_stable = 1. * np.arange(error_stable_learning.shape[0]) / (error_stable_learning.shape[0] - 1)
            error_dec_learning = self.pre.decode_location_beginning_end_learning(cells_to_use="decreasing", plotting=False)
            p_error_dec = 1. * np.arange(error_dec_learning.shape[0]) / (error_dec_learning.shape[0] - 1)

            # compute error for drift

            with open(self.params.pre_proc_dir + "cell_classification/" +
                      self.session_name + "_" + self.params.stable_cell_method + ".pickle", "rb") as f:
                class_dic = pickle.load(f)

            stable_cells = class_dic["stable_cell_ids"].flatten()
            # use last 5 trials of pre
            train_trials = range(len(self.pre.trial_loc_list)-5, len(self.pre.trial_loc_list))

            rate_maps = self.pre.get_rate_maps(spatial_resolution=1, trials_to_use=train_trials)

            rate_maps_flat = np.reshape(rate_maps, (rate_maps.shape[0] * rate_maps.shape[1], rate_maps.shape[2]))

            rate_maps_flat_stable = rate_maps_flat[:, stable_cells]

            # test in post
            test_raster, test_loc, _ = self.post.get_raster_location_speed(trials_to_use=range(5))

            test_raster_stable = test_raster[stable_cells, :]

            error_stable_drift = []
            for pop_vec, loc in zip(test_raster_stable.T, test_loc):
                if np.count_nonzero(pop_vec) == 0:
                    continue
                bl = bayes_likelihood(frm=rate_maps_flat_stable.T, pop_vec=pop_vec, log_likeli=False)
                bl_area = np.reshape(bl, (rate_maps.shape[0], rate_maps.shape[1]))
                pred_bin = np.unravel_index(bl_area.argmax(), bl_area.shape)
                pred_x = pred_bin[0] + self.pre.x_min
                pred_y = pred_bin[1] + self.pre.y_min

                # plt.scatter(pred_x, pred_y, color="red")
                # plt.scatter(loc[0], loc[1], color="gray")
                error_stable_drift.append(np.sqrt((pred_x - loc[0]) ** 2 + (pred_y - loc[1]) ** 2))

            error_stable_drift = np.array(error_stable_drift)
            error_stable_drift_sorted = np.sort(error_stable_drift)
            p_error_stable_drift = 1. * np.arange(error_stable_drift.shape[0]) / (error_stable_drift.shape[0] - 1)

            plt.plot(error_stable_drift_sorted, p_error_stable_drift, label="stable drift")
            plt.plot(error_stable_learning, p_error_stable, label="stable")
            plt.plot(error_dec_learning, p_error_dec, label="dec")
            plt.legend()
            plt.show()

    # poisson hmm
    # ------------------------------------------------------------------------------------------------------------------

    def cross_val_poisson_hmm_cb_pre_post(self, cl_ar=np.arange(1, 100, 10), cell_selection="all"):
        # --------------------------------------------------------------------------------------------------------------
        # cross validation of poisson hmm fits to data
        #
        # args:     - cl_ar, range object: #clusters to fit to data
        # --------------------------------------------------------------------------------------------------------------

        print(" - CROSS-VALIDATING POISSON HMM ON CHEESEBOARD: PRE AND POST --> OPTIMAL #MODES ...")
        print("  ... using " + str(self.params.cross_val_splits) + "\n")
        # get pre and post raster data
        pre_raster, pre_times = self.pre.get_raster_and_trial_times()
        post_raster, post_times = self.post.get_raster_and_trial_times()

        raster = np.hstack((pre_raster, post_raster))
        trial_lengths = pre_times + post_times

        if self.params.cross_val_splits == "trial_splitting":
            trial_end = np.cumsum(np.array(trial_lengths))
            trial_start = np.concatenate([[0], trial_end[:-1]])
            # test_range = np.vstack((trial_start, trial_end))
            test_range_per_fold = []
            for lo, hi in zip(trial_start, trial_end):
                test_range_per_fold.append(np.array(list(range(lo, hi))))

        elif self.params.cross_val_splits == "custom_splits":

            # number of folds
            nr_folds = 10
            # how many times to fit for each split
            max_nr_fitting = 5
            # how many chunks (for pre-computed splits)
            nr_chunks = 10

            unobserved_lo_array = pickle.load(
                open("temp_data/unobserved_lo_cv" + str(nr_folds) + "_" + str(nr_chunks) + "_chunks", "rb"))
            unobserved_hi_array = pickle.load(
                open("temp_data/unobserved_hi_cv" + str(nr_folds) + "_" + str(nr_chunks) + "_chunks", "rb"))

            # set number of time bins
            bin_num = raster.shape[1]
            bins = np.arange(bin_num + 1)

            # length of one chunk
            n_chunks = int(bin_num / nr_chunks)
            test_range_per_fold = []
            for fold in range(nr_folds):

                # unobserved_lo: start bins (in spike data resolution) for all test data chunks
                unobserved_lo = []
                unobserved_hi = []
                for lo, hi in zip(unobserved_lo_array[fold], unobserved_hi_array[fold]):
                    unobserved_lo.append(bins[lo * n_chunks])
                    unobserved_hi.append(bins[hi * n_chunks])

                unobserved_lo = np.array(unobserved_lo)
                unobserved_hi = np.array(unobserved_hi)

                test_range = []
                for lo, hi in zip(unobserved_lo, unobserved_hi):
                    test_range += (list(range(lo, hi)))
                test_range_per_fold.append(np.array(test_range))

        nr_cores = 12

        folder_name = self.params.session_name + "_pre_post_cb"+ self.cell_type
        new_ml = MlMethodsOnePopulation(params=self.params)
        new_ml.parallelize_cross_val_model(nr_cluster_array=cl_ar, nr_cores=nr_cores, model_type="POISSON_HMM",
                                           raster_data=raster, folder_name=folder_name, splits=test_range_per_fold)
        new_ml.cross_val_view_results(folder_name=folder_name)

        if cell_selection == "stable":
            # load only stable cells
            with open(
                    self.params.pre_proc_dir + "cell_classification/" + self.params.session_name + "_k_means.pickle",
                    "rb") as f:
                class_dic = pickle.load(f)

            stable_ids = class_dic["stable_cell_ids"]

            raster = raster[stable_ids, :]

            folder_name = self.params.session_name + "_pre_post_cb_stable" + self.cell_type

        elif cell_selection == "non_stable":

            # load only stable cells
            with open(
                    self.params.pre_proc_dir + "cell_classification/" + self.params.session_name + "_k_means.pickle",
                    "rb") as f:
                class_dic = pickle.load(f)

            stable_ids = class_dic["stable_cell_ids"]

            raster = np.delete(raster, stable_ids, axis=0)

            folder_name = self.params.session_name + "_pre_post_cb_non_stable" + self.cell_type

        elif cell_selection == "all":

            folder_name = self.params.session_name + "_pre_post_cb_" + self.cell_type

        nr_cores = 12

        new_ml = MlMethodsOnePopulation(params=self.params)
        new_ml.parallelize_cross_val_model(nr_cluster_array=cl_ar, nr_cores=nr_cores, model_type="POISSON_HMM",
                                           raster_data=raster, folder_name=folder_name)
        new_ml.cross_val_view_results(folder_name=folder_name)

    def view_cross_val_results_cb_pre_post(self, range_to_plot=None, cell_selection="all"):
        # --------------------------------------------------------------------------------------------------------------
        # views cross validation results
        #
        # args:     - model_type, string: which type of model ("POISSON_HMM")
        #           - custom_splits, bool: whether custom splits were used for cross validation
        # --------------------------------------------------------------------------------------------------------------
        if cell_selection == "stable":
            folder_name = self.session_name +"_pre_post_cb_stable"+self.cell_type
        elif cell_selection == "non_stable":
            folder_name = self.session_name +"_pre_post_cb_non_stable"+self.cell_type
        elif cell_selection == "all":
            folder_name = self.session_name +"_pre_post_cb_"+self.cell_type

        new_ml = MlMethodsOnePopulation(params=self.params)
        new_ml.cross_val_view_results(folder_name=folder_name, range_to_plot=range_to_plot)

    def fit_poisson_hmm_cb_pre_post(self, nr_modes, cell_selection="all"):
        # --------------------------------------------------------------------------------------------------------------
        # fits poisson hmm to data
        #
        # args:     - nr_modes, int: #clusters to fit to data
        #           - file_identifier, string: string that is added at the end of file for identification
        # --------------------------------------------------------------------------------------------------------------

        print(" - FITTING POISSON HMM WITH "+str(nr_modes)+" MODES ...\n")

        # get pre and post raster data
        pre_raster = self.pre.get_raster()
        post_raster = self.post.get_raster()

        raster = np.hstack((pre_raster, post_raster))

        if cell_selection == "stable":
            # load only stable cells
            with open(
                    self.params.pre_proc_dir + "cell_classification/" + self.params.session_name + "_k_means.pickle",
                    "rb") as f:
                class_dic = pickle.load(f)

            stable_ids = class_dic["stable_cell_ids"]

            raster = raster[stable_ids, :]
            file_name = self.params.session_name + "_pre_post_cb_stable_" + \
                        self.cell_type + "_" + str(nr_modes) + "_modes"

        elif cell_selection == "non_stable":
            # load only stable cells
            with open(
                    self.params.pre_proc_dir + "cell_classification/" + self.params.session_name + "_k_means.pickle",
                    "rb") as f:
                class_dic = pickle.load(f)

            stable_ids = class_dic["stable_cell_ids"]

            raster = np.delete(raster, stable_ids, axis=0)
            file_name = self.params.session_name + "_pre_post_cb_non_stable_" + \
                        self.cell_type + "_" + str(nr_modes) + "_modes"

        elif cell_selection == "all":
            file_name = self.params.session_name + "_pre_post_cb_" + self.cell_type + "_" + str(nr_modes) + "_modes"

        model = PoissonHMM(n_components=nr_modes)
        model.fit(raster.T)
        model.set_time_bin_size(time_bin_size=self.params.time_bin_size)

        with open(self.params.pre_proc_dir+"phmm/"+file_name+".pkl", "wb") as file: pickle.dump(model, file)

        print("  - ... DONE!\n")

    def one_phmm_model_pre_post_evaluate(self, nr_modes):
        # --------------------------------------------------------------------------------------------------------------
        # fits poisson hmm to data and evaluates the goodness of the model by comparing basic statistics (avg. firing
        # rate, correlation values, k-statistics) between real data and data sampled from the model
        #
        # args:     - nr_modes, int: #clusters to fit to data
        #           - load_from_file, bool: whether to load model from file or to fit model again
        # --------------------------------------------------------------------------------------------------------------

        print(" - EVALUATING POISSON HMM FIT (BASIC STATISTICS) ...")

        # get pre and post raster data
        pre_raster = self.pre.get_raster()
        post_raster = self.post.get_raster()

        raster = np.hstack((pre_raster, post_raster))

        nr_time_bins = raster.shape[1]
        # X = X[:, :1000]

        file_name = self.params.session_name + "_pre_post_cb_" + self.cell_type+"_"+str(nr_modes)+"_modes"

        # check if model file exists already --> otherwise fit model again
        if os.path.isfile(self.params.pre_proc_dir+"phmm/" + file_name + ".pkl"):
            print("- LOADING PHMM MODEL FROM FILE\n")
            with open(self.params.pre_proc_dir+"phmm/" + file_name + ".pkl", "rb") as file:
                model = pickle.load(file)
        else:
            print("- PHMM MODEL FILE NOT FOUND --> FITTING PHMM TO DATA\n")
            model = PoissonHMM(n_components=nr_modes)
            model.fit(raster.T)

        samples, sequence = model.sample(nr_time_bins)
        samples = samples.T

        evaluate_clustering_fit(real_data=raster, samples=samples, binning="TEMPORAL_SPIKE",
                                   time_bin_size=0.1, plotting=True)

    def one_phmm_model_pre_post_cb_analysis(self, nr_modes, cell_selection="all"):
        # --------------------------------------------------------------------------------------------------------------
        # fits poisson hmm to data and evaluates the goodness of the model by comparing basic statistics (avg. firing
        # rate, correlation values, k-statistics) between real data and data sampled from the model
        #
        # args:     - nr_modes, int: #clusters to fit to data
        #           - load_from_file, bool: whether to load model from file or to fit model again
        # --------------------------------------------------------------------------------------------------------------

        print(" - EVALUATING POISSON HMM FIT (BASIC STATISTICS) ...")

        # get pre and post raster data
        pre_raster = self.pre.get_raster()
        post_raster = self.post.get_raster()

        if cell_selection == "stable":
            # load only stable cells
            # load cell labels
            with open(self.params.pre_proc_dir + "cell_classification/" +
                      self.session_name + "_" + self.params.stable_cell_method + ".pickle",
                      "rb") as f:
                class_dic = pickle.load(f)

            stable_ids = class_dic["stable_cell_ids"]

            pre_raster = pre_raster[stable_ids, :]
            post_raster = post_raster[stable_ids, :]

            file_name = self.params.session_name + "_pre_post_cb_stable_" + self.cell_type+"_"+str(nr_modes)+"_modes"

        elif cell_selection == "non_stable":
            # load only stable cells
            # load cell labels
            with open(self.params.pre_proc_dir + "cell_classification/" +
                      self.session_name + "_" + self.params.stable_cell_method + ".pickle",
                      "rb") as f:
                class_dic = pickle.load(f)

            stable_ids = class_dic["stable_cell_ids"]

            pre_raster = np.delete(pre_raster, stable_ids, axis=0)
            post_raster = np.delete(post_raster, stable_ids, axis=0)

            file_name = self.params.session_name + "_pre_post_cb_non_stable_" + self.cell_type+"_"+str(nr_modes)+"_modes"

        elif cell_selection == "all":
            file_name = self.params.session_name + "_pre_post_cb_" + self.cell_type+"_"+str(nr_modes)+"_modes"

        raster = np.hstack((pre_raster, post_raster))

        # check if model file exists already --> otherwise fit model again
        if os.path.isfile(self.params.pre_proc_dir+"phmm/" + file_name + ".pkl"):
            print("- LOADING PHMM MODEL FROM FILE\n")
            with open(self.params.pre_proc_dir+"phmm/" + file_name + ".pkl", "rb") as file:
                model = pickle.load(file)
        else:
            print("- PHMM MODEL FILE NOT FOUND --> FITTING PHMM TO DATA\n")
            model = PoissonHMM(n_components=nr_modes)
            model.fit(raster.T)

        # get #occurences modes for pre and post
        pre_modes, pre_counts = np.unique(model.predict(pre_raster.T), return_counts=True)
        pre_counts = (pre_counts / np.sum(pre_counts)) * 100

        # get #occurences modes for pre and post
        post_modes, post_counts = np.unique(model.predict(post_raster.T), return_counts=True)
        post_counts = (post_counts / np.sum(post_counts)) * 100

        # # get lambdas and look at their similarity
        #
        # lambdas = model.means_
        #
        # D = pairwise_distances(lambdas, metric="euclidean")
        # plt.imshow(D)
        # plt.colorbar()
        # plt.show()
        #
        # plt.hist(D.flatten())
        # plt.show()
        # thresh = 0.2
        #
        # lambdas_bin = lambdas.copy()
        # lambdas_bin[lambdas_bin >= thresh] = 1
        # lambdas_bin[lambdas_bin < thresh] = 0
        # lambdas_bin=lambdas_bin.astype(bool)
        # plt.imshow(lambdas_bin)
        # plt.show()
        # D = pairwise_distances(lambdas_bin, metric="jaccard")
        # plt.imshow(D)
        # plt.colorbar()
        # plt.show()
        #
        #
        # exit()

        lines = []
        for i in range(pre_counts.shape[0]):
            pair = [(pre_modes[i], 0), (pre_modes[i], pre_counts[i])]
            lines.append(pair)

        linecoll = matcoll.LineCollection(lines, colors="violet")
        fig, ax = plt.subplots()

        ax.add_collection(linecoll)

        lines = []
        for i in range(post_counts.shape[0]):
            pair = [(post_modes[i], 0), (post_modes[i], post_counts[i])]
            lines.append(pair)

        linecoll = matcoll.LineCollection(lines, colors="lightskyblue")

        ax.add_collection(linecoll)

        plt.scatter(pre_modes, pre_counts, c="violet", label="PRE")
        plt.scatter(post_modes, post_counts, c="lightskyblue", alpha=0.8, label="POST")
        plt.legend()
        plt.xlabel("MODE ID")
        plt.ylabel("MODE OCCURENCE (%)")
        plt.show()

        print("HERE")

    def pre_post_models_analysis(self, gc_threshold=0.7):

        pre_raster, _, _ = self.pre.get_raster_location_speed()
        post_raster, _, _ = self.post.get_raster_location_speed()
        # check how many cells
        nr_cells = pre_raster.shape[0]

        # compute mean firing in PRE
        mean_firing_pre = np.mean(pre_raster, axis=1)
        # compute mean firing in POST
        mean_firing_post = np.mean(post_raster, axis=1)

        if self.params.stable_cell_method == "k_means":
            # load only stable cells
            with open(self.params.pre_proc_dir + "cell_classification/" +
                      self.params.session_name + "_k_means.pickle", "rb") as f:
                class_dic = pickle.load(f)

        elif self.params.stable_cell_method == "mean_firing_awake":
            # load only stable cells
            with open(self.params.pre_proc_dir + "cell_classification/" +
                      self.params.session_name + "_mean_firing_awake.pickle", "rb") as f:
                class_dic = pickle.load(f)

        stable_cells = class_dic["stable_cell_ids"]
        inc_cells = class_dic["increase_cell_ids"]
        dec_cells = class_dic["decrease_cell_ids"]

        # get PRE model
        with open(self.params.pre_proc_dir + "phmm/" + self.default_pre_phmm_model + ".pkl", "rb") as file:
            pre_model = pickle.load(file)

        pre_model_means = pre_model.means_
        # z-score for visualization
        pre_model_means_z = zscore(pre_model_means, axis=1)

        # get POST model
        with open(self.params.pre_proc_dir + "phmm/" + self.default_post_phmm_model + ".pkl", "rb") as file:
            post_model = pickle.load(file)

        post_model_means = post_model.means_
        # z-score for visualization
        post_model_means_z = zscore(post_model_means, axis=1)
        # find goal coding modes

        gc_pre = []
        for mode in range(pre_model_means.shape[0]):
            gc_pre.append(self.pre.analyze_modes_goal_coding(file_name=self.default_pre_phmm_model, mode_ids=mode))
        gc_pre = np.array(gc_pre)
        gc_pre_sorted = gc_pre.argsort()

        gc_post = []
        for mode in range(post_model_means.shape[0]):
            gc_post.append(self.pre.analyze_modes_goal_coding(file_name=self.default_pre_phmm_model, mode_ids=mode))
        gc_post = np.array(gc_post)
        gc_post_sorted = gc_post.argsort()

        # compute cell labels
        dec_stable_inc = np.hstack((dec_cells.flatten(), stable_cells.flatten(), inc_cells.flatten()))
        cell_labels = np.hstack((np.zeros(dec_cells.flatten().shape[0]), np.ones(stable_cells.flatten().shape[0]),
                                 2* np.ones(inc_cells.flatten().shape[0])))

        # order pre modes
        pre_model_means_sorted = pre_model_means_z[:, dec_stable_inc]
        pre_model_means_sorted = pre_model_means_sorted[gc_pre_sorted, :]

        # order post modes
        post_model_means_sorted = post_model_means_z[:, dec_stable_inc]
        post_model_means_sorted = post_model_means_sorted[gc_post_sorted, :]

        fig = plt.figure(figsize=(8,7))
        gs = fig.add_gridspec(13, 6)

        ax1 = fig.add_subplot(gs[0, :])
        ax1.set_title("CELL CLASSIFICATION (b:dec, r:inc, w:stable)")
        ax1.imshow(np.expand_dims(cell_labels, 0), cmap="bwr")
        ax1.get_xaxis().set_ticks([])
        ax1.get_yaxis().set_ticks([])
        ax2 = fig.add_subplot(gs[1:6, :])
        ax2.imshow(pre_model_means_sorted)
        ax2.get_xaxis().set_ticks([])
        ax2.set_ylabel("PRE MODES ORDERED")
        ax3 = fig.add_subplot(gs[6:, :])
        ax3.imshow(post_model_means_sorted)
        ax3.set_ylabel("POST MODES ORDERED")
        ax3.set_xlabel("CELLS ORDERED")
        plt.show()

        # compute entropy per cell (selective participation) PRE
        entropy_per_cell_pre = []
        for cell_means in pre_model_means.T:
            entropy_per_cell_pre.append(entropy(cell_means))
        entropy_per_cell_pre = np.nan_to_num(np.array(entropy_per_cell_pre))

        # compute entropy per cell (selective participation) POST
        entropy_per_cell_post = []
        for cell_means in post_model_means.T:
            entropy_per_cell_post.append(entropy(cell_means))
        entropy_per_cell_post = np.nan_to_num(np.array(entropy_per_cell_post))

        plt.subplot(2,1,1)
        plt.hist(entropy_per_cell_pre[stable_cells], color="#ffdba1", label="STABLE CELLS", density=True, bins=20)
        plt.hist(entropy_per_cell_pre[dec_cells], color="#a0c4e4", label="DECREASING CELLS", alpha=0.7, density=True, bins=20)
        _, y_max = plt.gca().get_ylim()
        plt.vlines(np.median(entropy_per_cell_pre[stable_cells]),0,y_max, colors="y")
        plt.vlines(np.median(entropy_per_cell_pre[dec_cells]),0,y_max, colors="b")
        plt.legend()
        plt.ylabel("DENSITY")
        plt.title("PRE - ALL MODES")
        plt.subplot(2,1,2)
        plt.hist(entropy_per_cell_post[stable_cells], color="#ffdba1", label="STABLE CELLS", density=True, bins=20)
        plt.hist(entropy_per_cell_post[inc_cells], color="#f7959c", label="INCREASING CELLS", alpha=0.7, density=True, bins=20)
        _, y_max = plt.gca().get_ylim()
        plt.vlines(np.median(entropy_per_cell_post[stable_cells]),0,y_max, colors="y")
        plt.vlines(np.median(entropy_per_cell_post[inc_cells]),0,y_max, colors="r")
        plt.legend()
        plt.ylabel("DENSITY")
        plt.xlabel("ENTROPY LAMBDA VALUES PER CELL")
        plt.title("POST - ALL MODES")
        plt.show()

        # only look at goal coding modes

        gc_pre_modes = np.argwhere(gc_pre > gc_threshold).flatten()
        gc_post_modes = np.argwhere(gc_post > gc_threshold).flatten()

        # compute entropy per cell (selective participation) PRE
        entropy_per_cell_pre = []
        for cell_means in pre_model_means[gc_pre_modes,:].T:
            entropy_per_cell_pre.append(entropy(cell_means))
        entropy_per_cell_pre = np.nan_to_num(np.array(entropy_per_cell_pre))

        # compute entropy per cell (selective participation) POST
        entropy_per_cell_post = []
        for cell_means in post_model_means[gc_post_modes,:].T:
            entropy_per_cell_post.append(entropy(cell_means))
        entropy_per_cell_post = np.nan_to_num(np.array(entropy_per_cell_post))


        plt.subplot(2,1,1)
        plt.hist(entropy_per_cell_pre[stable_cells], color="#ffdba1", label="STABLE CELLS", density=True, bins=20)
        plt.hist(entropy_per_cell_pre[dec_cells], color="#a0c4e4", label="DECREASING CELLS", alpha=0.7, density=True, bins=20)
        _, y_max = plt.gca().get_ylim()
        plt.vlines(np.median(entropy_per_cell_pre[stable_cells]),0,y_max, colors="y")
        plt.vlines(np.median(entropy_per_cell_pre[dec_cells]),0,y_max, colors="b")
        plt.legend()
        plt.ylabel("DENSITY")
        plt.title("PRE - GC MODES")
        plt.subplot(2,1,2)
        plt.hist(entropy_per_cell_post[stable_cells], color="#ffdba1", label="STABLE CELLS", density=True, bins=20)
        plt.hist(entropy_per_cell_post[inc_cells], color="#f7959c", label="INCREASING CELLS", alpha=0.7, density=True, bins=20)
        _, y_max = plt.gca().get_ylim()
        plt.vlines(np.median(entropy_per_cell_post[stable_cells]),0,y_max, colors="y")
        plt.vlines(np.median(entropy_per_cell_post[inc_cells]),0,y_max, colors="r")
        plt.legend()
        plt.ylabel("DENSITY")
        plt.xlabel("ENTROPY LAMBDA VALUES PER CELL")
        plt.title("POST - GC MODES")
        plt.show()

        # participation in goal coding PRE
        pre_means_gc = pre_model_means[gc_pre_modes,:]
        pre_mean_non_gc = np.delete(pre_model_means, gc_pre_modes, axis=0)

        goal_coding_modes_means_overall_modes_pre = np.mean(pre_means_gc, axis=0)/mean_firing_pre
        non_goal_coding_modes_means_overall_modes_pre = np.mean(pre_mean_non_gc, axis=0) / mean_firing_pre
        # plt.figure(figsize=(5,8))
        plt.scatter(goal_coding_modes_means_overall_modes_pre[stable_cells],
                    non_goal_coding_modes_means_overall_modes_pre[stable_cells], color="#ffdba1", label="STABLE CELLS")
        plt.scatter(goal_coding_modes_means_overall_modes_pre[dec_cells],
                    non_goal_coding_modes_means_overall_modes_pre[dec_cells], color="#a0c4e4", label="DECREASING CELLS")
        plt.legend(loc="upper left")
        plt.xlabel("PARTICIPATION GOAL CODING")
        plt.ylabel("PARTICIPATION NON GOAL CODING")
        plt.gca().set_aspect("equal")
        _,x_max = plt.gca().get_xlim()
        plt.hlines(1,0,x_max, color="gray", zorder=-1000)
        _,y_max = plt.gca().get_ylim()
        plt.vlines(1,0,y_max, color="gray", zorder=-1000)
        plt.title("CELL PARTICIPATION PRE")
        plt.show()

        # participation in goal coding POST
        post_means_gc = post_model_means[gc_post_modes, :]
        post_mean_non_gc = np.delete(post_model_means, gc_post_modes, axis=0)

        goal_coding_modes_means_overall_modes_post = np.mean(post_means_gc, axis=0) / mean_firing_post
        non_goal_coding_modes_means_overall_modes_post = np.mean(post_mean_non_gc, axis=0) / mean_firing_post
        # plt.figure(figsize=(5,8))
        plt.scatter(goal_coding_modes_means_overall_modes_post[stable_cells],
                    non_goal_coding_modes_means_overall_modes_post[stable_cells], color="#ffdba1", label="STABLE CELLS")
        plt.scatter(goal_coding_modes_means_overall_modes_post[inc_cells],
                    non_goal_coding_modes_means_overall_modes_post[inc_cells], color="#f7959c", label="INCREASING CELLS")
        plt.legend(loc="upper left")
        plt.xlabel("PARTICIPATION GOAL CODING")
        plt.ylabel("PARTICIPATION NON GOAL CODING")
        plt.gca().set_aspect("equal")
        _, x_max = plt.gca().get_xlim()
        plt.hlines(1, 0, x_max, color="gray", zorder=-1000)
        _, y_max = plt.gca().get_ylim()
        plt.vlines(1, 0, y_max, color="gray", zorder=-1000)
        plt.title("CELL PARTICIPATION POST")
        plt.show()

        plt.scatter(goal_coding_modes_means_overall_modes_pre[stable_cells],
                    non_goal_coding_modes_means_overall_modes_pre[stable_cells], color="grey", label="PRE")
        plt.scatter(goal_coding_modes_means_overall_modes_post[stable_cells],
                    non_goal_coding_modes_means_overall_modes_post[stable_cells], color="red", alpha=0.7,
                    label="POST")
        plt.legend(loc="upper left")
        plt.xlabel("PARTICIPATION GOAL CODING")
        plt.ylabel("PARTICIPATION NON GOAL CODING")
        plt.gca().set_aspect("equal")
        _, x_max = plt.gca().get_xlim()
        plt.hlines(1, 0, x_max, color="gray", zorder=-1000)
        _, y_max = plt.gca().get_ylim()
        plt.vlines(1, 0, y_max, color="gray", zorder=-1000)
        plt.title("GOAL CODING STABLE CELLS PRE - POST")
        plt.show()


        diff = np.mean(pre_means_gc, axis=0) - np.mean(pre_mean_non_gc, axis=0)/\
               (np.mean(pre_means_gc, axis=0) + np.mean(pre_mean_non_gc, axis=0))

        plt.hist(diff[stable_cells], color="#ffdba1", density=True, label="STABLE CELLS")
        plt.hist(diff[dec_cells], color="#a0c4e4", alpha=0.8, density=True, label="DECREASING CELLS")
        plt.xlabel("DELTA IN LAMBDAS: GOAL CODING MODES - NON GOAL CODING MODES")
        plt.ylabel("DENSITY")
        plt.title("PRE")
        plt.legend()
        plt.show()

        diff_post = np.mean(post_means_gc, axis=0) - np.mean(post_mean_non_gc, axis=0)/\
               (np.mean(post_means_gc, axis=0) + np.mean(post_mean_non_gc, axis=0))

        plt.hist(diff_post[stable_cells], color="#ffdba1", density=True, label="STABLE CELLS")
        plt.hist(diff_post[inc_cells], color="#f7959c", alpha=0.8, density=True, label="INCREASING CELLS")
        plt.xlabel("DELTA IN LAMBDAS: GOAL CODING MODES - NON GOAL CODING MODES")
        plt.ylabel("DENSITY")
        plt.title("POST")
        plt.legend()

        plt.show()

    def pre_post_models_goal_coding(self, gc_threshold=0.9, plotting=False):

        pre_raster, _, _ = self.pre.get_raster_location_speed()
        post_raster, _, _ = self.post.get_raster_location_speed()
        # check how many cells
        nr_cells = pre_raster.shape[0]

        # compute mean firing in PRE
        mean_firing_pre = np.mean(pre_raster, axis=1)
        # compute mean firing in POST
        mean_firing_post = np.mean(post_raster, axis=1)

        # select only cells that fire in PRE and POST
        cell_sel = np.logical_and(mean_firing_pre>0, mean_firing_post>0)
        to_delete = np.argwhere(~cell_sel).flatten()

        if self.params.stable_cell_method == "k_means":
            # load only stable cells
            with open(self.params.pre_proc_dir + "cell_classification/" +
                      self.session_name + "_k_means.pickle", "rb") as f:
                class_dic = pickle.load(f)

        elif self.params.stable_cell_method == "mean_firing_awake":
            # load only stable cells
            with open(self.params.pre_proc_dir + "cell_classification/" +
                      self.session_name + "_mean_firing_awake.pickle", "rb") as f:
                class_dic = pickle.load(f)

        stable_cells = class_dic["stable_cell_ids"].flatten()
        # delete stable cells which dont fire in pre and post
        for del_cell in to_delete:
            stable_cells = stable_cells[stable_cells != del_cell]

        # get PRE model
        with open(self.params.pre_proc_dir + "phmm/" + self.default_pre_phmm_model + ".pkl", "rb") as file:
            pre_model = pickle.load(file)
        pre_model_means = pre_model.means_

        # get POST model
        with open(self.params.pre_proc_dir + "phmm/" + self.default_post_phmm_model + ".pkl", "rb") as file:
            post_model = pickle.load(file)

        post_model_means = post_model.means_

        gc_pre = []
        for mode in range(pre_model_means.shape[0]):
            gc_pre.append(self.pre.analyze_modes_goal_coding(file_name=self.default_pre_phmm_model, mode_ids=mode))
        gc_pre = np.array(gc_pre)

        gc_post = []
        for mode in range(post_model_means.shape[0]):
            gc_post.append(self.pre.analyze_modes_goal_coding(file_name=self.default_pre_phmm_model, mode_ids=mode))
        gc_post = np.array(gc_post)

        # only look at goal coding modes
        gc_pre_modes = np.argwhere(gc_pre > gc_threshold).flatten()
        gc_post_modes = np.argwhere(gc_post > gc_threshold).flatten()

        # participation in goal coding PRE
        pre_means_gc = pre_model_means[gc_pre_modes,:]
        pre_mean_non_gc = np.delete(pre_model_means, gc_pre_modes, axis=0)

        goal_coding_modes_means_overall_modes_pre = np.mean(pre_means_gc, axis=0)/mean_firing_pre
        non_goal_coding_modes_means_overall_modes_pre = np.mean(pre_mean_non_gc, axis=0) / mean_firing_pre

        # participation in goal coding POST
        post_means_gc = post_model_means[gc_post_modes, :]
        post_mean_non_gc = np.delete(post_model_means, gc_post_modes, axis=0)

        goal_coding_modes_means_overall_modes_post = np.mean(post_means_gc, axis=0) / mean_firing_post
        non_goal_coding_modes_means_overall_modes_post = np.mean(post_mean_non_gc, axis=0) / mean_firing_post

        # compute gain in goal coding
        gain_gc = goal_coding_modes_means_overall_modes_post[stable_cells] - goal_coding_modes_means_overall_modes_pre[stable_cells]

        if plotting:
            plt.hist(gain_gc, bins=20)
            plt.xlabel("GAIN GC")
            _, y_max = plt.gca().get_ylim()
            plt.vlines(np.median(gain_gc), 0, y_max, color="gray")
            plt.show()

            plt.scatter(goal_coding_modes_means_overall_modes_pre[stable_cells],
                        non_goal_coding_modes_means_overall_modes_pre[stable_cells], color="grey", label="PRE")
            plt.scatter(goal_coding_modes_means_overall_modes_post[stable_cells],
                        non_goal_coding_modes_means_overall_modes_post[stable_cells], color="red", alpha=0.7,
                        label="POST")
            plt.legend(loc="upper left")
            plt.xlabel("PARTICIPATION GOAL CODING")
            plt.ylabel("PARTICIPATION NON GOAL CODING")
            plt.gca().set_aspect("equal")
            _, x_max = plt.gca().get_xlim()
            plt.hlines(1, 0, x_max, color="gray", zorder=-1000)
            _, y_max = plt.gca().get_ylim()
            plt.vlines(1, 0, y_max, color="gray", zorder=-1000)
            plt.title("GOAL CODING STABLE CELLS PRE - POST")
            plt.show()

        return np.median(gain_gc)

    # correlations
    # ------------------------------------------------------------------------------------------------------------------

    def cell_type_correlations(self, awake_smoothing = 3):

        # load cell labels
        # --------------------------------------------------------------------------------------------------------------
        with open(self.params.pre_proc_dir + "cell_classification/" +
                  self.params.session_name + "_"+self.params.stable_cell_method +".pickle", "rb") as f:
            class_dic = pickle.load(f)

        stable_cells = class_dic["stable_cell_ids"].flatten()
        inc_cells = class_dic["increase_cell_ids"].flatten()
        dec_cells = class_dic["decrease_cell_ids"].flatten()

        n_stable_cells = len(stable_cells)
        n_inc_cells = len(inc_cells)
        n_dec_cells = len(dec_cells)

        # PRE DATA
        # --------------------------------------------------------------------------------------------------------------
        raster_pre = self.pre.get_raster()
        # apply smoothing
        raster_pre = uniform_filter1d(raster_pre, size=awake_smoothing, axis=1)

        # select stable and dec for pre
        raster_stable_dec_pre = zscore(np.vstack((raster_pre[stable_cells, :], raster_pre[dec_cells, :])), axis=1)

        # compute all correlations
        all_corr_stable_dec_pre = np.corrcoef(raster_stable_dec_pre)

        # select all within stable correlations
        corr_stable_pre = upper_tri_without_diag(all_corr_stable_dec_pre[:n_stable_cells,:n_stable_cells])

        # select within dec correlations
        # select all within stable correlations
        corr_dec_pre = upper_tri_without_diag(all_corr_stable_dec_pre[n_stable_cells+1:,n_stable_cells+1:])

        # select all stable to dec correlations
        corr_stable_dec_pre = all_corr_stable_dec_pre[:n_stable_cells, n_stable_cells+1:].flatten()
        plt.subplot(2,1,1)
        plt.hist(corr_stable_pre, np.arange(np.min(np.hstack((corr_stable_pre,corr_stable_dec_pre))),
                 np.max(np.hstack((corr_stable_pre,corr_stable_dec_pre))),0.02), density=True, color="#ffdba1",label="STABLE")
        plt.hist(corr_stable_dec_pre,np.arange(np.min(np.hstack((corr_stable_pre,corr_stable_dec_pre))),
                 np.max(np.hstack((corr_stable_pre,corr_stable_dec_pre))),0.02), density=True, color="blue", alpha=0.5, label="STABLE-DEC")
        plt.title("CORRELATIONS - PRE")
        plt.grid(color="grey")
        plt.ylabel("DENSITY")
        plt.legend()
        plt.subplot(2,1,2)
        plt.hist(corr_stable_pre, np.arange(np.min(np.hstack((corr_stable_pre,corr_dec_pre))),
                 np.max(np.hstack((corr_stable_pre,corr_dec_pre))),0.02), density=True, color="#ffdba1",label="STABLE")
        plt.hist(corr_dec_pre, np.arange(np.min(np.hstack((corr_stable_pre,corr_dec_pre))),
                 np.max(np.hstack((corr_stable_pre,corr_dec_pre))),0.02), density=True, color="#a0c4e4", label="DEC", alpha=0.5)
        plt.grid(color="grey")
        plt.ylabel("DENSITY")
        plt.legend()
        plt.xlabel("CORRELATIONS")
        plt.show()

        # POST DATA
        # --------------------------------------------------------------------------------------------------------------
        raster_post = self.post.get_raster()
        # apply smoothing
        raster_post = uniform_filter1d(raster_post, size=awake_smoothing, axis=1)

        # select stable and dec for pre
        raster_stable_inc_post = zscore(np.vstack((raster_post[stable_cells, :], raster_post[inc_cells, :])), axis=1)

        # compute all correlations
        all_corr_stable_inc_post = np.corrcoef(raster_stable_inc_post)

        # select all within stable correlations
        corr_stable_post = upper_tri_without_diag(all_corr_stable_inc_post[:n_stable_cells, :n_stable_cells])

        # select within dec correlations
        # select all within stable correlations
        corr_inc_post = upper_tri_without_diag(all_corr_stable_inc_post[n_stable_cells + 1:, n_stable_cells + 1:])

        # select all stable to inc correlations
        corr_stable_inc_post = all_corr_stable_inc_post[:n_stable_cells, n_stable_cells + 1:].flatten()


        plt.subplot(2, 1, 1)
        plt.hist(corr_stable_post, np.arange(np.min(np.hstack((corr_stable_post, corr_stable_inc_post))),
                                            np.max(np.hstack((corr_stable_post, corr_stable_inc_post))), 0.02),
                 density=True, color="#ffdba1", label="STABLE")
        plt.hist(corr_stable_dec_pre, np.arange(np.min(np.hstack((corr_stable_post, corr_stable_inc_post))),
                                                np.max(np.hstack((corr_stable_post, corr_stable_inc_post))), 0.02),
                 density=True, color="red", alpha=0.5, label="STABLE-INC")
        plt.title("CORRELATIONS - POST")
        plt.grid(color="grey")
        plt.ylabel("DENSITY")
        plt.legend()
        plt.subplot(2, 1, 2)
        plt.hist(corr_stable_post, np.arange(np.min(np.hstack((corr_stable_post, corr_inc_post))),
                                            np.max(np.hstack((corr_stable_post, corr_inc_post))), 0.02), density=True,
                 color="#ffdba1", label="STABLE")
        plt.hist(corr_inc_post, np.arange(np.min(np.hstack((corr_stable_post, corr_inc_post))),
                                         np.max(np.hstack((corr_stable_post, corr_inc_post))), 0.02), density=True,
                 color="#f7959c", label="INC", alpha=0.5)
        plt.grid(color="grey")
        plt.ylabel("DENSITY")
        plt.legend()
        plt.xlabel("CORRELATIONS")
        plt.show()

        # corr_test = upper_tri_without_diag(np.corrcoef(raster_stable_pre_z))

    def distinguishing_goals(self, plotting=True):

        stable, dec, inc, all = self.pre.distinguishing_goals(plotting=False)

        stable = stable[~np.isnan(stable)]
        dec = dec[~np.isnan(dec)]
        inc = inc[~np.isnan(inc)]
        all = all[~np.isnan(all)]

        if plotting:
            c="white"
            res = [stable, dec, inc, all]
            bplot = plt.boxplot(res, positions=[1,2,3,4], patch_artist=True,
                                labels=["Stable", "Dec", "Inc", "All"],
                                boxprops=dict(color=c),
                                capprops=dict(color=c),
                                whiskerprops=dict(color=c),
                                flierprops=dict(color=c, markeredgecolor=c),
                                medianprops=dict(color=c),showfliers=False)
            plt.ylabel("Mean accuracy SVM")
            plt.title("PRE")
            plt.show()

        stable, dec, inc, all_cells = self.post.distinguishing_goals(plotting=False)

        stable = stable[~np.isnan(stable)]
        dec = dec[~np.isnan(dec)]
        inc = inc[~np.isnan(inc)]
        all_cells = all[~np.isnan(all_cells)]

        if plotting:
            c="white"
            res = [stable, dec, inc, all_cells]
            bplot = plt.boxplot(res, positions=[1,2,3,4], patch_artist=True,
                                labels=["Stable", "Dec", "Inc", "All"],
                                boxprops=dict(color=c),
                                capprops=dict(color=c),
                                whiskerprops=dict(color=c),
                                flierprops=dict(color=c, markeredgecolor=c),
                                medianprops=dict(color=c),showfliers=False)
            plt.ylabel("Mean accuracy SVM")
            plt.title("POST")
            plt.show()

    def identify_single_goals(self, plotting=True):

        stable, dec, inc, all = self.pre.identify_single_goal(plotting=False)

        stable = stable[~np.isnan(stable)]
        dec = dec[~np.isnan(dec)]
        inc = inc[~np.isnan(inc)]
        all = all[~np.isnan(all)]

        if plotting:
            c="white"
            res = [stable, dec, inc, all]
            bplot = plt.boxplot(res, positions=[1,2,3,4], patch_artist=True,
                                labels=["Stable", "Dec", "Inc", "All"],
                                boxprops=dict(color=c),
                                capprops=dict(color=c),
                                whiskerprops=dict(color=c),
                                flierprops=dict(color=c, markeredgecolor=c),
                                medianprops=dict(color=c),showfliers=False)
            plt.ylabel("Mean accuracy SVM - Multiclass")
            plt.title("PRE")
            plt.show()

        stable, dec, inc, all_cells = self.post.identify_single_goal(plotting=False)

        stable = stable[~np.isnan(stable)]
        dec = dec[~np.isnan(dec)]
        inc = inc[~np.isnan(inc)]
        all_cells = all[~np.isnan(all_cells)]

        if plotting:
            c="white"
            res = [stable, dec, inc, all_cells]
            bplot = plt.boxplot(res, positions=[1,2,3,4], patch_artist=True,
                                labels=["Stable", "Dec", "Inc", "All"],
                                boxprops=dict(color=c),
                                capprops=dict(color=c),
                                whiskerprops=dict(color=c),
                                flierprops=dict(color=c, markeredgecolor=c),
                                medianprops=dict(color=c),showfliers=False)
            plt.ylabel("Mean accuracy SVM - Multiclass")
            plt.title("POST")
            plt.show()


"""#####################################################################################################################
#   PRE AND POST CHEESEBOARD TASK WITH LONG SLEEP IN BETWEEN
#####################################################################################################################"""


class PreLongSleepPost(LongSleep, PrePostCheeseboard):
    """Class for long sleep and PRE and POST cheeseboard task"""

    def __init__(self, sleep_data_obj, pre, post, params, session_params=None):

        # initialize parent classes
        LongSleep.__init__(self, sleep_data_obj=sleep_data_obj, params=params, session_params=session_params)

        # TODO: check which class attributes are overwritten by PrePostCheeseboard

        PrePostCheeseboard.__init__(self, pre=pre, post=post, params=params, session_params=session_params)

        self.cell_type = sleep_data_obj.get_cell_type()

    def firing_rate_distributions(self, cells_to_use="stable", plotting=True, separate_sleep_phases=True):
        print("Processing " +self.session_name)
        # get rasters from exploration before/after
        raster_pre = self.pre.get_raster()
        raster_post = self.post.get_raster()

        with open(self.params.pre_proc_dir + "cell_classification/" +
                  self.session_name + "_" + self.params.stable_cell_method + ".pickle", "rb") as f:
            class_dic = pickle.load(f)

        if cells_to_use == "stable":
            cell_ids = class_dic["stable_cell_ids"].flatten()
        elif cells_to_use =="increasing":
            cell_ids = class_dic["increase_cell_ids"].flatten()
        elif cells_to_use == "decreasing":
            cell_ids = class_dic["decrease_cell_ids"].flatten()

        if separate_sleep_phases:
            raster_sleep_nrem = self.get_sleep_phase_raster(sleep_phase="nrem")
            raster_sleep_rem = self.get_sleep_phase_raster(sleep_phase="rem")

            firing_pre_mean = np.mean(raster_pre[cell_ids, :], axis=1)
            firing_sleep_nrem_mean = np.mean(raster_sleep_nrem[cell_ids, :], axis=1)
            firing_sleep_rem_mean = np.mean(raster_sleep_rem[cell_ids, :], axis=1)
            firing_post_mean = np.mean(raster_post[cell_ids, :], axis=1)

            # normalize mean firing rates
            max_mean_fir = np.max(np.vstack((firing_pre_mean, firing_sleep_nrem_mean, firing_sleep_rem_mean,
                                             firing_post_mean)), axis=0)

            firing_pre_norm = firing_pre_mean/max_mean_fir
            firing_sleep_nrem_norm = firing_sleep_nrem_mean / max_mean_fir
            firing_sleep_rem_norm = firing_sleep_rem_mean / max_mean_fir
            firing_post_norm = firing_post_mean / max_mean_fir

            if plotting:
                p_pre_norm = 1. * np.arange(firing_pre_norm.shape[0]) / (firing_pre_norm.shape[0] - 1)
                p_sleep_nrem_norm = 1. * np.arange(firing_sleep_nrem_norm.shape[0]) / (firing_sleep_nrem_norm.shape[0] - 1)
                p_sleep_rem_norm = 1. * np.arange(firing_sleep_rem_norm.shape[0]) / (firing_sleep_rem_norm.shape[0] - 1)
                p_post_norm = 1. * np.arange(firing_post_norm.shape[0]) / (firing_post_norm.shape[0] - 1)

                plt.plot(np.sort(firing_pre_norm), p_pre_norm, label="PRE")
                plt.plot(np.sort(firing_sleep_rem_norm), p_sleep_rem_norm, label="REM")
                plt.plot(np.sort(firing_sleep_nrem_norm), p_sleep_nrem_norm, label="NREM")
                plt.plot(np.sort(firing_post_norm), p_post_norm, label="POST")
                plt.title(cells_to_use)
                plt.xlabel("Mean firing rate / normalized")
                plt.ylabel("CDF")
                plt.legend()
                plt.show()
            else:
                return firing_pre_norm, firing_sleep_rem_norm, firing_sleep_nrem_norm, firing_post_norm

        else:
            raster_sleep = self.get_sleep_raster()
            firing_pre_mean = np.mean(raster_pre[cell_ids, :], axis=1)
            firing_sleep_mean = np.mean(raster_sleep[cell_ids, :], axis=1)
            firing_post_mean = np.mean(raster_post[cell_ids, :], axis=1)

            # normalize mean firing rates
            max_mean_fir = np.max(np.vstack((firing_pre_mean, firing_sleep_mean, firing_post_mean)), axis=0)

            firing_pre_norm = firing_pre_mean/max_mean_fir
            firing_sleep_norm = firing_sleep_mean / max_mean_fir
            firing_post_norm = firing_post_mean / max_mean_fir

            if plotting:

                p_pre_norm = 1. * np.arange(firing_pre_norm.shape[0]) / (firing_pre_norm.shape[0] - 1)
                p_sleep_norm = 1. * np.arange(firing_sleep_norm.shape[0]) / (firing_sleep_norm.shape[0] - 1)
                p_post_norm = 1. * np.arange(firing_post_norm.shape[0]) / (firing_post_norm.shape[0] - 1)

                plt.plot(np.sort(firing_pre_norm), p_pre_norm, label="PRE")
                plt.plot(np.sort(firing_sleep_norm), p_sleep_norm, label="Sleep")
                plt.plot(np.sort(firing_post_norm), p_post_norm, label="POST")
                plt.title(cells_to_use)
                plt.xlabel("Mean firing rate / normalized")
                plt.ylabel("CDF")
                plt.legend()
                plt.show()
            else:
                return firing_pre_norm, firing_sleep_norm, firing_post_norm

    def firing_rate_distributions_all_cells(self, plotting=True, measure="mean", chunks_in_min=5):

        # get rasters from exploration before/after & sleep
        raster_pre = self.pre.get_raster()
        raster_post = self.post.get_raster()
        raster_sleep = self.get_sleep_raster()

        with open(self.params.pre_proc_dir + "cell_classification/" +
                  self.session_name + "_" + self.params.stable_cell_method + ".pickle", "rb") as f:
            class_dic = pickle.load(f)

        stable_cells = class_dic["stable_cell_ids"].flatten()
        inc_cells = class_dic["increase_cell_ids"].flatten()
        dec_cells = class_dic["decrease_cell_ids"].flatten()

        chunk_size = int(chunks_in_min/self.params.time_bin_size)

        # go through PRE first
        nr_chunks_pre = int(raster_pre.shape[1]/chunk_size)
        firing_pre = np.zeros((raster_pre.shape[0], nr_chunks_pre))
        for chunk in range(nr_chunks_pre):
            if measure == "mean":
                firing_pre[:, chunk] = np.mean(
                    raster_pre[:, chunk*chunk_size:(chunk+1)*chunk_size], axis=1)/self.params.time_bin_size
            elif measure == "max":
                firing_pre[:, chunk] = np.max(
                    raster_pre[:, chunk * chunk_size:(chunk + 1) * chunk_size], axis=1) / self.params.time_bin_size

        # go through sleep
        nr_chunks_sleep = int(raster_sleep.shape[1]/chunk_size)
        firing_sleep = np.zeros((raster_sleep.shape[0], nr_chunks_sleep))
        for chunk in range(nr_chunks_sleep):
            if measure == "mean":
                firing_sleep[:, chunk] = np.mean(
                    raster_sleep[:, chunk*chunk_size:(chunk+1)*chunk_size], axis=1)/self.params.time_bin_size
            elif measure == "max":
                firing_sleep[:, chunk] = np.max(
                    raster_sleep[:, chunk * chunk_size:(chunk + 1) * chunk_size], axis=1) / self.params.time_bin_size

        # go through POST
        nr_chunks_post = int(raster_post.shape[1]/chunk_size)
        firing_post = np.zeros((raster_post.shape[0], nr_chunks_post))
        for chunk in range(nr_chunks_post):
            if measure == "mean":
                firing_post[:, chunk] = np.mean(
                    raster_post[:, chunk*chunk_size:(chunk+1)*chunk_size], axis=1)/self.params.time_bin_size
            elif measure == "max":
                firing_post[:, chunk] = np.max(
                    raster_post[:, chunk * chunk_size:(chunk + 1) * chunk_size], axis=1) / self.params.time_bin_size

        # combine to z-score for each cell
        all_firing = np.hstack((firing_pre, firing_sleep, firing_post))
        all_firing_z = zscore(all_firing, axis=1)

        # split again into pre, sleep, post
        firing_pre_z = all_firing_z[:,:firing_pre.shape[1]]
        firing_sleep_z = all_firing_z[:, firing_pre.shape[1]:(firing_pre.shape[1]+firing_sleep.shape[1])]
        firing_post_z = all_firing_z[:, (firing_pre.shape[1]+firing_sleep.shape[1]):]

        # split into subsets
        firing_pre_z_stable = firing_pre_z[stable_cells, :].flatten()
        firing_pre_z_dec = firing_pre_z[dec_cells, :].flatten()
        firing_pre_z_inc = firing_pre_z[inc_cells, :].flatten()

        firing_sleep_z_stable = firing_sleep_z[stable_cells, :].flatten()
        firing_sleep_z_dec = firing_sleep_z[dec_cells, :].flatten()
        firing_sleep_z_inc = firing_sleep_z[inc_cells, :].flatten()

        firing_post_z_stable = firing_post_z[stable_cells, :].flatten()
        firing_post_z_dec = firing_post_z[dec_cells, :].flatten()
        firing_post_z_inc = firing_post_z[inc_cells, :].flatten()

        if plotting:

            p_pre_stable = 1. * np.arange(firing_pre_z_stable.shape[0]) / (firing_pre_z_stable.shape[0] - 1)
            p_sleep_stable = 1. * np.arange(firing_sleep_z_stable.shape[0]) / (firing_sleep_z_stable.shape[0] - 1)
            p_post_stable = 1. * np.arange(firing_post_z_stable.shape[0]) / (firing_post_z_stable.shape[0] - 1)

            p_pre_dec = 1. * np.arange(firing_pre_z_dec.shape[0]) / (firing_pre_z_dec.shape[0] - 1)
            p_sleep_dec = 1. * np.arange(firing_sleep_z_dec.shape[0]) / (firing_sleep_z_dec.shape[0] - 1)
            p_post_dec = 1. * np.arange(firing_post_z_dec.shape[0]) / (firing_post_z_dec.shape[0] - 1)

            p_pre_inc = 1. * np.arange(firing_pre_z_inc.shape[0]) / (firing_pre_z_inc.shape[0] - 1)
            p_sleep_inc = 1. * np.arange(firing_sleep_z_inc.shape[0]) / (firing_sleep_z_inc.shape[0] - 1)
            p_post_inc = 1. * np.arange(firing_post_z_inc.shape[0]) / (firing_post_z_inc.shape[0] - 1)

            plt.plot(np.sort(firing_pre_z_stable), p_pre_stable, color="magenta", label="stable")
            plt.plot(np.sort(firing_pre_z_inc), p_pre_inc, color="orange", label="inc")
            plt.plot(np.sort(firing_pre_z_dec), p_pre_dec, color="turquoise", label="dec")
            plt.xlabel(measure + " (Hz)")
            plt.ylabel("CDF")
            plt.legend()
            plt.title("PRE")
            plt.show()

            plt.plot(np.sort(firing_sleep_z_stable), p_sleep_stable, color="magenta", label="stable")
            plt.plot(np.sort(firing_sleep_z_inc), p_sleep_inc, color="orange",label="inc")
            plt.plot(np.sort(firing_sleep_z_dec), p_sleep_dec, color="turquoise",label="dec")
            plt.xlabel(measure + " (Hz)")
            plt.ylabel("CDF")
            plt.legend()
            plt.title("Sleep")
            plt.show()

            plt.plot(np.sort(firing_post_z_stable), p_post_stable, color="magenta",label="stable")
            plt.plot(np.sort(firing_post_z_inc), p_post_inc, color="orange",label="inc")
            plt.plot(np.sort(firing_post_z_dec), p_post_dec, color="turquoise",label="dec")
            plt.xlabel(measure + " (Hz)")
            plt.ylabel("CDF")
            plt.legend()
            plt.title("Post")
            plt.show()

        else:
            return firing_pre_z_stable, firing_pre_z_dec, firing_pre_z_inc, firing_sleep_z_stable, firing_sleep_z_dec, \
                   firing_sleep_z_inc, firing_post_z_stable, firing_post_z_dec, firing_post_z_inc

    def firing_rate_ratios_all_cells(self, plotting=True, measure="mean", chunks_in_min=5):

        # get rasters from exploration before/after & sleep
        raster_pre = self.pre.get_raster()
        raster_post = self.post.get_raster()
        raster_sleep = self.get_sleep_raster()

        with open(self.params.pre_proc_dir + "cell_classification/" +
                  self.session_name + "_" + self.params.stable_cell_method + ".pickle", "rb") as f:
            class_dic = pickle.load(f)

        stable_cells = class_dic["stable_cell_ids"].flatten()
        inc_cells = class_dic["increase_cell_ids"].flatten()
        dec_cells = class_dic["decrease_cell_ids"].flatten()

        chunk_size = int(chunks_in_min/self.params.time_bin_size)

        # go through PRE first
        nr_chunks_pre = int(raster_pre.shape[1]/chunk_size)
        firing_pre = np.zeros((raster_pre.shape[0], nr_chunks_pre))
        for chunk in range(nr_chunks_pre):
            if measure == "mean":
                firing_pre[:, chunk] = np.mean(
                    raster_pre[:, chunk*chunk_size:(chunk+1)*chunk_size], axis=1)/self.params.time_bin_size
            elif measure == "max":
                firing_pre[:, chunk] = np.max(
                    raster_pre[:, chunk * chunk_size:(chunk + 1) * chunk_size], axis=1) / self.params.time_bin_size

        # go through sleep
        nr_chunks_sleep = int(raster_sleep.shape[1]/chunk_size)
        firing_sleep = np.zeros((raster_sleep.shape[0], nr_chunks_sleep))
        for chunk in range(nr_chunks_sleep):
            if measure == "mean":
                firing_sleep[:, chunk] = np.mean(
                    raster_sleep[:, chunk*chunk_size:(chunk+1)*chunk_size], axis=1)/self.params.time_bin_size
            elif measure == "max":
                firing_sleep[:, chunk] = np.max(
                    raster_sleep[:, chunk * chunk_size:(chunk + 1) * chunk_size], axis=1) / self.params.time_bin_size

        # go through POST
        nr_chunks_post = int(raster_post.shape[1]/chunk_size)
        firing_post = np.zeros((raster_post.shape[0], nr_chunks_post))
        for chunk in range(nr_chunks_post):
            if measure == "mean":
                firing_post[:, chunk] = np.mean(
                    raster_post[:, chunk*chunk_size:(chunk+1)*chunk_size], axis=1)/self.params.time_bin_size
            elif measure == "max":
                firing_post[:, chunk] = np.max(
                    raster_post[:, chunk * chunk_size:(chunk + 1) * chunk_size], axis=1) / self.params.time_bin_size

        # combine to z-score for each cell
        all_firing = np.hstack((firing_pre, firing_sleep, firing_post))

        ratio_pre_sleep = np.mean(firing_sleep, axis=1)/np.mean(firing_pre, axis=1)
        ratio_post_sleep = np.mean(firing_sleep, axis=1)/np.mean(firing_post, axis=1)

        ratio_pre_sleep[ratio_pre_sleep==np.inf] = np.nan
        ratio_post_sleep[ratio_post_sleep==np.inf] = np.nan

        ratio_pre_sleep_stable = ratio_pre_sleep[stable_cells]
        ratio_post_sleep_stable = ratio_post_sleep[stable_cells]

        ratio_pre_sleep_dec = ratio_pre_sleep[dec_cells]
        ratio_post_sleep_dec = ratio_post_sleep[dec_cells]

        ratio_pre_sleep_inc = ratio_pre_sleep[inc_cells]
        ratio_post_sleep_inc = ratio_post_sleep[inc_cells]

        if plotting:
            c = "white"
            y_dat = [ratio_pre_sleep_stable[~np.isnan(ratio_pre_sleep_stable)], ratio_post_sleep_stable[~np.isnan(ratio_post_sleep_stable)]]
            plt.figure(figsize=(2, 4))
            bplot = plt.boxplot(y_dat, positions=[1, 2], patch_artist=True,
                                labels=["sleep/PRE", "sleep/POST"],
                                boxprops=dict(color=c),
                                capprops=dict(color=c),
                                whiskerprops=dict(color=c),
                                flierprops=dict(color=c, markeredgecolor=c),
                                medianprops=dict(color=c), showfliers=False)
            # whis = [0.01, 99.99]
            plt.title("Stable")
            plt.show()

            c = "white"
            y_dat = [ratio_pre_sleep_dec[~np.isnan(ratio_pre_sleep_dec)], ratio_post_sleep_dec[~np.isnan(ratio_post_sleep_dec)]]
            plt.figure(figsize=(2, 4))
            bplot = plt.boxplot(y_dat, positions=[1, 2], patch_artist=True,
                                labels=["sleep/PRE", "sleep/POST"],
                                boxprops=dict(color=c),
                                capprops=dict(color=c),
                                whiskerprops=dict(color=c),
                                flierprops=dict(color=c, markeredgecolor=c),
                                medianprops=dict(color=c), showfliers=False)
            # whis = [0.01, 99.99]
            plt.title("Dec")
            plt.show()

            c = "white"
            y_dat = [ratio_pre_sleep_inc[~np.isnan(ratio_pre_sleep_inc)], ratio_post_sleep_inc[~np.isnan(ratio_post_sleep_inc)]]
            plt.figure(figsize=(2, 4))
            bplot = plt.boxplot(y_dat, positions=[1, 2], patch_artist=True,
                                labels=["sleep/PRE", "sleep/POST"],
                                boxprops=dict(color=c),
                                capprops=dict(color=c),
                                whiskerprops=dict(color=c),
                                flierprops=dict(color=c, markeredgecolor=c),
                                medianprops=dict(color=c), showfliers=False)
            # whis = [0.01, 99.99]
            plt.title("Inc")
            plt.show()

        else:
            return ratio_pre_sleep_stable[~np.isnan(ratio_pre_sleep_stable)], \
                   ratio_post_sleep_stable[~np.isnan(ratio_post_sleep_stable)], \
                   ratio_pre_sleep_dec[~np.isnan(ratio_pre_sleep_dec)], \
                   ratio_post_sleep_dec[~np.isnan(ratio_post_sleep_dec)], \
                   ratio_pre_sleep_inc[~np.isnan(ratio_pre_sleep_inc)], \
                   ratio_post_sleep_inc[~np.isnan(ratio_post_sleep_inc)]

    def over_expressed_modes_goal_coding(self, template_type, pre_file_name=None, post_file_name=None,
                                         rem_pop_vec_threshold=10, cells_to_compare="stable", perc_inc_thresh=20,
                                         post_or_pre="post", plotting=False, shuffling=False):
        """
        checks which modes in sleep are over expressed when only stable cells are used. Then, goal coding properties
        of these modes are analyzed

        :param template_type: which template to use ("phmm" or "ising")
        :type template_type: str
        :param pre_file_name: name of PRE phmm model, if None --> uses default
        :type pre_file_name: str or None
        :param post_file_name: name of POST phmm model, if None --> uses default
        :type post_file_name: str or None
        :param rem_pop_vec_threshold: min. number of rem vectors per epoch (shorter epochs are discarded)
        :type rem_pop_vec_threshold: int
        :param cells_to_compare: compare with "stable" or "increasing" cells
        :type cells_to_compare: str
        :param perc_inc_thresh: defines threshold for over expression (e.g. 20 --> if 20% increase of occurence when
                                only using stable to base line [all cells] --> mode is over expressed)
        :type perc_inc_thresh: int
        :param post_or_pre: checking goal coding in pre ("pre") or post ("post")
        :type post_or_pre: str
        :param plotting: whether to plot results
        :type plotting: bool
        :return: frac_goal_coding_increase_stable_rem, frac_goal_coding_stable_rem,
                 frac_goal_coding_increase_stable_nrem, frac_goal_coding_stable_nrem
        :rtype: float, float, float, float
        """
        print("ANALYZING GOAL CODING OF STABLE CELLS \n")

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_nrem_event, event_times_nrem, event_lengths_nrem, event_duration_nrem_in_s, pre_prob_nrem, \
        post_prob_nrem= self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="nrem",
                                                     pop_vec_threshold=2, cells_to_use="all")


        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_rem_event, event_times_rem, event_lengths_rem, event_duration_rem_in_s, pre_prob_rem, post_prob_rem= \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold,
                                                     cells_to_use="all")



        _, _, _, _, pre_prob_rem_stable, post_prob_rem_stable= \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold,
                                                     cells_to_use=cells_to_compare, shuffling=shuffling)


        _, _, _, _, pre_prob_nrem_stable, post_prob_nrem_stable= \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="nrem",
                                                     pop_vec_threshold=rem_pop_vec_threshold,
                                                     cells_to_use=cells_to_compare, shuffling=shuffling)

        if post_or_pre == "pre":
            nr_modes = pre_prob_rem.shape[1]
            # check reactivated modes
            # ----------------------------------------------------------------------------------------------------------
            # rem with all cells
            modes_rem = np.argmax(pre_prob_rem, axis=1)
            # rem with only stable cells
            modes_rem_stable = np.argmax(pre_prob_rem_stable, axis=1)
            # nrem with all cells
            modes_nrem = np.argmax(pre_prob_nrem, axis=1)
            # nrem with stable cells
            modes_nrem_stable = np.argmax(pre_prob_nrem_stable, axis=1)

        elif post_or_pre == "post":
            nr_modes = post_prob_rem.shape[1]
            # check reactivated modes
            # ----------------------------------------------------------------------------------------------------------
            # rem with all cells
            modes_rem = np.argmax(post_prob_rem, axis=1)
            # rem with only stable cells
            modes_rem_stable = np.argmax(post_prob_rem_stable, axis=1)
            # nrem with all cells
            modes_nrem = np.argmax(post_prob_nrem, axis=1)
            # nrem with stable cells
            modes_nrem_stable = np.argmax(post_prob_nrem_stable, axis=1)

        mode_ids = np.arange(nr_modes)

        modes_rem, counts = np.unique(modes_rem, return_counts=True)
        counts_rem = np.zeros(nr_modes)
        counts_rem[modes_rem] = counts

        modes_rem_stable, counts = np.unique(modes_rem_stable, return_counts=True)
        counts_rem_stable = np.zeros(nr_modes)
        counts_rem_stable[modes_rem_stable] = counts

        modes_nrem, counts = np.unique(modes_nrem, return_counts=True)
        counts_nrem = np.zeros(nr_modes)
        counts_nrem[modes_nrem] = counts

        modes_nrem_stable, counts = np.unique(modes_nrem_stable, return_counts=True)
        counts_nrem_stable = np.zeros(nr_modes)
        counts_nrem_stable[modes_nrem_stable] = counts

        if plotting:

            lines = []
            for i in range(mode_ids.shape[0]):
                pair = [(mode_ids[i], 0), (mode_ids[i], counts_rem[i])]
                lines.append(pair)

            linecoll = matcoll.LineCollection(lines)
            fig, ax = plt.subplots()
            ax.add_collection(linecoll)
            plt.scatter(mode_ids, counts_rem, label="ALL CELLS")
            plt.scatter(mode_ids, counts_rem_stable, label="ONLY "+cells_to_compare)
            plt.xlabel("MODE ID")
            plt.ylabel("COUNTS")
            plt.title("REM")
            plt.legend()
            plt.show()


            lines = []
            for i in range(mode_ids.shape[0]):
                pair = [(mode_ids[i], 0), (mode_ids[i], counts_nrem[i])]
                lines.append(pair)

            linecoll = matcoll.LineCollection(lines)
            fig, ax = plt.subplots()
            ax.add_collection(linecoll)
            plt.scatter(mode_ids, counts_nrem, label="ALL CELLS")
            plt.scatter(mode_ids, counts_nrem_stable, label="ONLY "+cells_to_compare)
            plt.xlabel("MODE ID")
            plt.ylabel("COUNTS")
            plt.title("NREM")
            plt.legend()
            plt.show()

        counts_nrem[counts_nrem==0] = np.nan
        perc_change_nrem = np.nan_to_num(counts_nrem_stable/(counts_nrem/100)-100)
        modes_increase_stable_nrem = perc_change_nrem > perc_inc_thresh

        counts_nrem[counts_rem==0] = np.nan
        perc_change_rem = np.nan_to_num(counts_rem_stable/(counts_rem/100)-100)
        modes_increase_stable_rem = perc_change_rem > perc_inc_thresh

        ################################################################################################################
        # Awake data
        ################################################################################################################

        if post_or_pre == "pre":
            awake = self.pre
            def_model = self.default_pre_phmm_model
        elif post_or_pre == "post":
            awake = self.post
            def_model = self.default_post_phmm_model

        # rem
        # --------------------------------------------------------------------------------------------------------------
        frac_goal_coding_increase_stable_rem = awake.analyze_modes_goal_coding(file_name=def_model,
                                                              mode_ids=mode_ids[modes_increase_stable_rem],
                                                              plotting=plotting)

        frac_goal_coding_stable_rem = awake.analyze_modes_goal_coding(file_name=def_model,
                                                              mode_ids=mode_ids[~modes_increase_stable_rem],
                                                              plotting=plotting)
        # nrem
        # --------------------------------------------------------------------------------------------------------------

        frac_goal_coding_increase_stable_nrem = awake.analyze_modes_goal_coding(file_name=def_model,
                                                                                  mode_ids=mode_ids[
                                                                                      modes_increase_stable_nrem],
                                                                                  plotting=plotting)

        frac_goal_coding_stable_nrem = awake.analyze_modes_goal_coding(file_name=def_model,
                                                                         mode_ids=mode_ids[~modes_increase_stable_nrem],
                                                                         plotting=plotting)

        return frac_goal_coding_increase_stable_rem, frac_goal_coding_stable_rem, frac_goal_coding_increase_stable_nrem, \
               frac_goal_coding_stable_nrem

    def over_expressed_modes_spatial_information(self, template_type, pre_file_name=None, post_file_name=None,
                                                 rem_pop_vec_threshold=10, cells_to_compare="stable",
                                                 perc_inc_thresh=20, post_or_pre="post", plotting=False):
        """
        checks which modes in sleep are over expressed when only stable cells are used. Then, goal coding properties
        of these modes are analyzed

        :param template_type: which template to use ("phmm" or "ising")
        :type template_type: str
        :param pre_file_name: name of PRE phmm model, if None --> uses default
        :type pre_file_name: str or None
        :param post_file_name: name of POST phmm model, if None --> uses default
        :type post_file_name: str or None
        :param rem_pop_vec_threshold: min. number of rem vectors per epoch (shorter epochs are discarded)
        :type rem_pop_vec_threshold: int
        :param cells_to_compare: compare with "stable" or "increasing" cells
        :type cells_to_compare: str
        :param perc_inc_thresh: defines threshold for over expression (e.g. 20 --> if 20% increase of occurence when
                                only using stable to base line [all cells] --> mode is over expressed)
        :type perc_inc_thresh: int
        :param post_or_pre: checking goal coding in pre ("pre") or post ("post")
        :type post_or_pre: str
        :param plotting: whether to plot results
        :type plotting: bool
        :return: frac_goal_coding_increase_stable_rem, frac_goal_coding_stable_rem,
                 frac_goal_coding_increase_stable_nrem, frac_goal_coding_stable_nrem
        :rtype: float, float, float, float
        """
        print("ANALYZING GOAL CODING OF STABLE CELLS \n")

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_nrem_event, event_times_nrem, event_lengths_nrem, event_duration_nrem_in_s, pre_prob_nrem, \
        post_prob_nrem= self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="nrem",
                                                     pop_vec_threshold=2, cells_to_use="all")


        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_rem_event, event_times_rem, event_lengths_rem, event_duration_rem_in_s, pre_prob_rem, post_prob_rem= \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold,
                                                     cells_to_use="all")



        _, _, _, _, pre_prob_rem_stable, post_prob_rem_stable= \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold,
                                                     cells_to_use=cells_to_compare)


        _, _, _, _, pre_prob_nrem_stable, post_prob_nrem_stable= \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="nrem",
                                                     pop_vec_threshold=rem_pop_vec_threshold,
                                                     cells_to_use=cells_to_compare)

        if post_or_pre == "pre":
            nr_modes = pre_prob_rem.shape[1]
            # check reactivated modes
            # ----------------------------------------------------------------------------------------------------------
            # rem with all cells
            modes_rem = np.argmax(pre_prob_rem, axis=1)
            # rem with only stable cells
            modes_rem_stable = np.argmax(pre_prob_rem_stable, axis=1)
            # nrem with all cells
            modes_nrem = np.argmax(pre_prob_nrem, axis=1)
            # nrem with stable cells
            modes_nrem_stable = np.argmax(pre_prob_nrem_stable, axis=1)

        elif post_or_pre == "post":
            nr_modes = post_prob_rem.shape[1]
            # check reactivated modes
            # ----------------------------------------------------------------------------------------------------------
            # rem with all cells
            modes_rem = np.argmax(post_prob_rem, axis=1)
            # rem with only stable cells
            modes_rem_stable = np.argmax(post_prob_rem_stable, axis=1)
            # nrem with all cells
            modes_nrem = np.argmax(post_prob_nrem, axis=1)
            # nrem with stable cells
            modes_nrem_stable = np.argmax(post_prob_nrem_stable, axis=1)

        mode_ids = np.arange(nr_modes)

        modes_rem, counts = np.unique(modes_rem, return_counts=True)
        counts_rem = np.zeros(nr_modes)
        counts_rem[modes_rem] = counts

        modes_rem_stable, counts = np.unique(modes_rem_stable, return_counts=True)
        counts_rem_stable = np.zeros(nr_modes)
        counts_rem_stable[modes_rem_stable] = counts

        modes_nrem, counts = np.unique(modes_nrem, return_counts=True)
        counts_nrem = np.zeros(nr_modes)
        counts_nrem[modes_nrem] = counts

        modes_nrem_stable, counts = np.unique(modes_nrem_stable, return_counts=True)
        counts_nrem_stable = np.zeros(nr_modes)
        counts_nrem_stable[modes_nrem_stable] = counts

        if plotting:

            lines = []
            for i in range(mode_ids.shape[0]):
                pair = [(mode_ids[i], 0), (mode_ids[i], counts_rem[i])]
                lines.append(pair)

            linecoll = matcoll.LineCollection(lines)
            fig, ax = plt.subplots()
            ax.add_collection(linecoll)
            plt.scatter(mode_ids, counts_rem, label="ALL CELLS")
            plt.scatter(mode_ids, counts_rem_stable, label="ONLY "+cells_to_compare)
            plt.xlabel("MODE ID")
            plt.ylabel("COUNTS")
            plt.title("REM")
            plt.legend()
            plt.show()


            lines = []
            for i in range(mode_ids.shape[0]):
                pair = [(mode_ids[i], 0), (mode_ids[i], counts_nrem[i])]
                lines.append(pair)

            linecoll = matcoll.LineCollection(lines)
            fig, ax = plt.subplots()
            ax.add_collection(linecoll)
            plt.scatter(mode_ids, counts_nrem, label="ALL CELLS")
            plt.scatter(mode_ids, counts_nrem_stable, label="ONLY "+cells_to_compare)
            plt.xlabel("MODE ID")
            plt.ylabel("COUNTS")
            plt.title("NREM")
            plt.legend()
            plt.show()

        counts_nrem[counts_nrem==0] = np.nan
        perc_change_nrem = np.nan_to_num(counts_nrem_stable/(counts_nrem/100)-100)
        modes_increase_stable_nrem = perc_change_nrem > perc_inc_thresh

        counts_nrem[counts_rem==0] = np.nan
        perc_change_rem = np.nan_to_num(counts_rem_stable/(counts_rem/100)-100)
        modes_increase_stable_rem = perc_change_rem > perc_inc_thresh

        ################################################################################################################
        # Awake data
        ################################################################################################################

        if post_or_pre == "pre":
            awake = self.pre
            def_model = self.default_pre_phmm_model
        elif post_or_pre == "post":
            awake = self.post
            def_model = self.default_post_phmm_model

        # rem
        # --------------------------------------------------------------------------------------------------------------
        spatial_info_increase_stable_rem = awake.analyze_modes_spatial_information(file_name=def_model,
                                                              mode_ids=mode_ids[modes_increase_stable_rem],
                                                              plotting=False)

        spatial_info_stable_rem = awake.analyze_modes_spatial_information(file_name=def_model,
                                                              mode_ids=mode_ids[~modes_increase_stable_rem],
                                                              plotting=False)
        # nrem
        # --------------------------------------------------------------------------------------------------------------

        spatial_info_increase_stable_nrem = awake.analyze_modes_spatial_information(file_name=def_model,
                                                                                  mode_ids=mode_ids[
                                                                                      modes_increase_stable_nrem],
                                                                                  plotting=False)

        spatial_info_stable_nrem = awake.analyze_modes_spatial_information(file_name=def_model,
                                                                         mode_ids=mode_ids[~modes_increase_stable_nrem],
                                                                         plotting=False)

        return spatial_info_increase_stable_rem, spatial_info_stable_rem, spatial_info_increase_stable_nrem, \
               spatial_info_stable_nrem

    def memory_drift_rem_nrem_decoding_similarity_plot_decoded_map(self, pre_file_name=None, post_file_name=None,
                                                                   rem_pop_vec_threshold=100, cells_to_use="all",
                                                                   save_fig=False, pre_or_post="pre"):

        _, _, default_ising_pre, default_ising_post = self.long_sleep[0].get_pre_post_templates()

        if pre_or_post == "pre":
            ising_map = default_ising_pre
        elif pre_or_post == "post":
            ising_map = default_ising_post

        with open(self.params.pre_proc_dir + 'awake_ising_maps/' + ising_map + '.pkl',
                  'rb') as f:
            model_dic = pickle.load(f)

        # factor a.u. to cm
        scaling_fac = self.session_params.data_params_dictionary["spatial_factor"]

        # ising maps are decoded using 5 a.u.
        spatial_bin_size = 5 * scaling_fac

        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        _, _, _, _, pre_prob_rem, post_prob_rem = \
            self.memory_drift_long_sleep_get_results(template_type="ising", pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold,
                                                     cells_to_use=cells_to_use)

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        _, _, _, _, pre_prob_nrem, \
        post_prob_nrem = self.memory_drift_long_sleep_get_results(template_type="ising", pre_file_name=pre_file_name,
                                                                  post_file_name=post_file_name, part_to_analyze="nrem",
                                                                  pop_vec_threshold=2, cells_to_use=cells_to_use)

        if pre_or_post == "pre":
            # rem pre map results and normalize
            rem_mode_freq = np.zeros(pre_prob_rem.shape[1])
            map_result_rem = np.argmax(pre_prob_rem, axis=1)

        elif pre_or_post == "post":
            # rem post map results and normalize
            rem_mode_freq = np.zeros(post_prob_rem.shape[1])
            map_result_rem = np.argmax(post_prob_rem, axis=1)

        rem_mode_id, rem_mode_count = np.unique(map_result_rem, return_counts=True)
        rem_mode_freq[rem_mode_id] = rem_mode_count
        rem_mode_freq_norm = rem_mode_freq / np.sum(rem_mode_freq)

        if pre_or_post == "pre":
            # get nrem pre map results and normalize
            nrem_mode_freq = np.zeros(pre_prob_nrem.shape[1])
            map_result_nrem = np.argmax(pre_prob_nrem, axis=1)
        elif pre_or_post == "post":
            # get nrem pre map results and normalize
            nrem_mode_freq = np.zeros(post_prob_nrem.shape[1])
            map_result_nrem = np.argmax(post_prob_nrem, axis=1)

        nrem_mode_id, nrem_mode_count = np.unique(map_result_nrem, return_counts=True)
        nrem_mode_freq[nrem_mode_id] = nrem_mode_count
        nrem_mode_freq_norm = nrem_mode_freq / np.sum(nrem_mode_freq)

        print("Correlation, R = "+str(pearsonr(nrem_mode_freq_norm, rem_mode_freq_norm)))

        nrem_mode_freq_spatial = np.reshape(nrem_mode_freq_norm,
                                                (model_dic["res_map"].shape[1], model_dic["res_map"].shape[2]))
        rem_mode_freq_spatial = np.reshape(rem_mode_freq_norm,
                                               (model_dic["res_map"].shape[1], model_dic["res_map"].shape[2]))

        nrem_mode_freq_spatial[model_dic["occ_map"] == 0] = np.nan
        rem_mode_freq_spatial[model_dic["occ_map"] == 0] = np.nan

        plt.style.use('default')

        # plt.imshow(model_dic["occ_map"].T, origin="lower")
        # for g_l in self.pre.goal_locations:
        #     plt.scatter((g_l[0] - self.pre.x_min+8) / spatial_bin_size, (g_l[1] - self.pre.y_min+1) / spatial_bin_size)
        # plt.show()

        max_val = np.nanmax(np.hstack((nrem_mode_freq_spatial.flatten(),rem_mode_freq_spatial.flatten())))

        plt.imshow(nrem_mode_freq_spatial.T, origin='lower', cmap="Reds", vmin=0, vmax=max_val)
        a = plt.colorbar()
        a.set_label("Prob. of decoding bin")
        plt.xticks([0, 16, 32, 48], np.array([0, 16, 32, 48]) * spatial_bin_size)
        plt.yticks([0, 16, 32, 48], np.array([0, 16, 32, 48]) * spatial_bin_size)
        plt.xlabel("x (cm)")
        plt.ylabel("y (cm)")
        plt.title("NREM")
        plt.xlim(-2,50)
        plt.ylim(-2,50)
        # plt.gca().yaxis.set_major_locator(MaxNLocator(integer=True))
        # corrected goal locations based on occupancy map/tracking data ... there was an offset
        for g_l in self.pre.goal_locations:
            plt.scatter((g_l[0] - self.pre.x_min + 8) / spatial_bin_size,
                        (g_l[1] - self.pre.y_min + 1) / spatial_bin_size,
                        label="goal locations", color="black", s=30)
        handles, labels = plt.gca().get_legend_handles_labels()
        by_label = OrderedDict(zip(labels, handles))
        plt.gca().legend(by_label.values(), by_label.keys())
        if save_fig:
            plt.rcParams['svg.fonttype'] = 'none'
            plt.savefig("decoding_ising_nrem.svg", transparent="True")
            plt.close()
        else:
            plt.show()

        plt.imshow(rem_mode_freq_spatial.T, origin='lower', cmap="Reds", vmin=0, vmax=max_val)
        a = plt.colorbar()
        a.set_label("Prob. of decoding bin")
        plt.xticks([0, 16, 32, 48], np.array([0, 16, 32, 48]) * spatial_bin_size)
        plt.yticks([0, 16, 32, 48], np.array([0, 16, 32, 48]) * spatial_bin_size)
        plt.title("REM")
        plt.xlabel("x (cm)")
        plt.ylabel("y (cm)")
        plt.xlim(-2,50)
        plt.ylim(-2,50)
        # plt.gca().yaxis.set_major_locator(MaxNLocator(integer=True))
        # corrected goal locations based on occupancy map/tracking data ... there was an offset
        for g_l in self.pre.goal_locations:
            plt.scatter((g_l[0] - self.pre.x_min + 8) / spatial_bin_size,
                        (g_l[1] - self.pre.y_min + 1) / spatial_bin_size,
                        label="goal locations", color="black", s=30)
        handles, labels = plt.gca().get_legend_handles_labels()
        by_label = OrderedDict(zip(labels, handles))
        plt.gca().legend(by_label.values(), by_label.keys())
        if save_fig:
            plt.rcParams['svg.fonttype'] = 'none'
            plt.savefig("decoding_ising_rem.svg", transparent="True")
            plt.close()
        else:
            plt.show()

    # Decoding analysis
    # ------------------------------------------------------------------------------------------------------------------

    def memory_drift_cosine_similarity_pop_vec_spatial_bins(self, new_time_bin_size=None):
        """
        computes pre-post similarity using population vectors of sleep and spatial bins of pre or post exploration.
        Only considers non-HSE periods

        @param new_time_bin_size: whether to use different time bin size for the population vectors than defined in
        parameters in seconds
        @type new_time_bin_size: float
        """
        print(" - COMPUTING MEMORY DRIFT USING COSINE SIM. AND SPATIAL BINS")

        rate_maps_orig = np.array(self.exploration_fam.get_rate_maps())
        rate_maps_fam = np.reshape(rate_maps_orig, (rate_maps_orig.shape[0], rate_maps_orig.shape[1]*rate_maps_orig.shape[2]))

        rate_maps_orig = np.array(self.exploration_novel.get_rate_maps())
        rate_maps_novel = np.reshape(rate_maps_orig, (rate_maps_orig.shape[0], rate_maps_orig.shape[1]*rate_maps_orig.shape[2]))

        raster_sleep = self.sleep_fam.get_raster()

        # plt.imshow(raster_sleep, interpolation='nearest', aspect='auto', cmap="jet")
        # a = plt.colorbar()
        # plt.show()

        ind_hse = np.array(find_hse(x=raster_sleep)).flatten()

        # remove high synchrony events
        raster_sleep = np.delete(raster_sleep, ind_hse, axis=1)
        # raster_sleep = raster_sleep[:,:10]

        if new_time_bin_size is not None:
            # down/up sample data
            time_bin_scaler = int(new_time_bin_size / self.params.time_bin_size)

            new_raster = np.zeros((raster_sleep.shape[0], int(raster_sleep.shape[1] / time_bin_scaler)))

            # down sample spikes by combining multiple bins
            for i in range(new_raster.shape[1]):
                new_raster[:, i] = np.sum(raster_sleep[:, (i * time_bin_scaler): ((1 + i) * time_bin_scaler)], axis=1)

            raster_sleep = new_raster

        sim_fam = np.zeros((raster_sleep.shape[1], rate_maps_fam.shape[1]))
        sim_novel = np.zeros((raster_sleep.shape[1], rate_maps_novel.shape[1]))
        # compute correlation between mean prob. vectors for each mode and all spatially binned population vectors
        for mode_id, pop_vec in enumerate(raster_sleep.T):
            for spat_bin_id, spat_bin_fam in enumerate(rate_maps_fam.T):
                sim_fam[mode_id, spat_bin_id] = np.nan_to_num((1-distance.cosine(pop_vec, spat_bin_fam)))
            for spat_bin_id, spat_bin_nov in enumerate(rate_maps_novel.T):
                sim_novel[mode_id, spat_bin_id] = np.nan_to_num((1-distance.cosine(pop_vec, spat_bin_nov)))

        sim_fam_max= np.max(sim_fam, axis=1)
        sim_novel_max = np.max(sim_novel, axis=1)

        drift_meas = (sim_novel_max - sim_fam_max)/(sim_novel_max + sim_fam_max)
        plt.plot(drift_meas)
        plt.title("SIMILARITY BEFORE - AFTER")
        plt.ylabel("SIMILARITY BEFORE / AFTER")
        plt.xlabel("TIME BINS (" + str(new_time_bin_size)+"s)")
        plt.show()

    def memory_drift_cosine_similarity_pop_vec_temporal_bins(self, new_time_bin_size=0.1):

        print(" - COMPUTING MEMORY DRIFT USING COSINE SIM. AND TEMPORAL BINNING")

        raster_fam = self.exploration_fam.get_raster()
        raster_novel = self.exploration_novel.get_raster()

        raster_sleep = self.sleep_fam.get_raster()

        # ind_hse = np.array(find_hse(x=raster_sleep)).flatten()
        #
        # # remove high synchrony events
        # raster_sleep = np.delete(raster_sleep, ind_hse, axis=1)
        # raster_sleep = raster_sleep[:,:10]


        # # down/up sample data
        # time_bin_scaler = int(new_time_bin_size / self.params.time_bin_size)
        #
        # new_raster = np.zeros((raster_sleep.shape[0], int(raster_sleep.shape[1] / time_bin_scaler)))
        #
        # # down sample spikes by combining multiple bins
        # for i in range(new_raster.shape[1]):
        #     new_raster[:, i] = np.sum(raster_sleep[:, (i * time_bin_scaler): ((1 + i) * time_bin_scaler)], axis=1)
        #
        # raster_sleep = new_raster

        sim_fam = np.zeros((raster_sleep.shape[1], raster_fam.shape[1]))
        sim_novel = np.zeros((raster_sleep.shape[1], raster_novel.shape[1]))
        # compute correlation between mean prob. vectors for each mode and all spatially binned population vectors
        for mode_id, pop_vec in enumerate(raster_sleep.T):
            for spat_bin_id, spat_bin_fam in enumerate(raster_fam.T):
                sim_fam[mode_id, spat_bin_id] = np.nan_to_num((1-distance.cosine(pop_vec, spat_bin_fam)))
            for spat_bin_id, spat_bin_nov in enumerate(raster_novel.T):
                sim_novel[mode_id, spat_bin_id] = np.nan_to_num((1-distance.cosine(pop_vec, spat_bin_nov)))

        # plt.imshow(sim_fam,  interpolation='nearest', aspect='auto')
        # plt.colorbar()
        # plt.show()
        # plt.imshow(sim_novel,  interpolation='nearest', aspect='auto')
        # plt.colorbar()
        # plt.show()

        sim_fam_max= np.max(sim_fam, axis=1)
        sim_novel_max = np.max(sim_novel, axis=1)

        drift_meas = (sim_novel_max - sim_fam_max)/(sim_novel_max + sim_fam_max)
        plt.plot(drift_meas)
        plt.show()

    def memory_drift_cosine_similarity_pop_vec_poisson_hmm_modes(self, file_phmm_before, file_phmm_after):
        # --------------------------------------------------------------------------------------------------------------
        # analyzes memory drift using lambda vectors (poisson HMM) established during awake behavior. Compares sleep
        # activity to all lambda vectors of behavior before vs. to all lambda vectors of behavior after
        #
        # parameters:   - file_phmm_before, string: file name containing phmm model for behavior before
        #               - file_phmm_after, string: file name containing phmm model for behavior after
        #
        # returns:      -
        # --------------------------------------------------------------------------------------------------------------

        print(" - COMPUTING MEMORY DRIFT - PHMM LAMBDA PER MODE")

        _, _, _, _, _, _, phmm_means_before = self.exploration_fam.fit_poisson_hmm(file_name=file_phmm_before)
        _, _, _, _, _, _, phmm_means_after = self.exploration_novel.fit_poisson_hmm(file_name=file_phmm_after)

        # plt.imshow(phmm_means_after, interpolation='nearest', aspect='auto')
        # plt.show()
        #
        # plt.imshow(phmm_means_before, interpolation='nearest', aspect='auto')
        # plt.show()

        raster_sleep = self.sleep_fam.get_spike_binned_raster()

        # raster_sleep = raster_sleep[:, :1000]

        # ind_hse = np.array(find_hse(x=raster_sleep)).flatten()
        #
        # # remove high synchrony events
        # raster_sleep = np.delete(raster_sleep, ind_hse, axis=1)
        # raster_sleep = raster_sleep[:,:10]


        # # down/up sample data
        # time_bin_scaler = int(new_time_bin_size / self.params.time_bin_size)
        #
        # new_raster = np.zeros((raster_sleep.shape[0], int(raster_sleep.shape[1] / time_bin_scaler)))
        #
        # # down sample spikes by combining multiple bins
        # for i in range(new_raster.shape[1]):
        #     new_raster[:, i] = np.sum(raster_sleep[:, (i * time_bin_scaler): ((1 + i) * time_bin_scaler)], axis=1)
        #
        # raster_sleep = new_raster

        sim_fam = np.zeros((raster_sleep.shape[1], phmm_means_before.shape[0]))
        sim_novel = np.zeros((raster_sleep.shape[1], phmm_means_after.shape[0]))
        # compute correlation between mean prob. vectors for each mode and all spatially binned population vectors
        for mode_id, pop_vec in enumerate(raster_sleep.T):
            for spat_bin_id, spat_bin_fam in enumerate(phmm_means_before):
                sim_fam[mode_id, spat_bin_id] = np.nan_to_num((1-distance.cosine(pop_vec, spat_bin_fam)))
                # sim_fam[mode_id, spat_bin_id], _ = pearsonr(pop_vec, spat_bin_fam)
            for spat_bin_id, spat_bin_nov in enumerate(phmm_means_after):
                sim_novel[mode_id, spat_bin_id] = np.nan_to_num((1-distance.cosine(pop_vec, spat_bin_nov)))
                # sim_novel[mode_id, spat_bin_id], _ = pearsonr(pop_vec, spat_bin_nov)

        # plt.imshow(sim_fam,  interpolation='nearest', aspect='auto')
        # plt.colorbar()
        # plt.show()
        # plt.imshow(sim_novel,  interpolation='nearest', aspect='auto')
        # plt.colorbar()
        # plt.show()

        sim_fam_max = np.max(sim_fam, axis=1)
        sim_novel_max = np.max(sim_novel, axis=1)

        drift_meas = (sim_novel_max - sim_fam_max)/(sim_novel_max + sim_fam_max)
        plt.plot(drift_meas)
        plt.show()

    def memory_drift_rem_nrem_decoding_spatial(self, rem_pop_vec_threshold=100):
        """

        computes change in spatial information during sleep. Spatial information is computed using the phmm modes

        :param rem_pop_vec_threshold: min. number of pop. vec. for REM
        :type rem_pop_vec_threshold: int
        """
        means, std_modes, mode_freq, env, state_sequence = self.pre.fit_spatial_gaussians_for_modes()

        spar, skaggs_per_second, skaggs_per_spike = self.pre.phmm_mode_spatial_information_from_model()

        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_rem_event, event_times_rem, event_lengths_rem, event_duration_rem_in_s, pre_prob_rem, post_prob_rem= \
            self.memory_drift_long_sleep_get_results(template_type="phmm", pre_file_name=None,
                                                     post_file_name=None, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold)

        pre_prob_rem_norm = (pre_prob_rem / np.sum(pre_prob_rem, axis=1, keepdims=True)).T

        spar_per_bin = np.expand_dims(spar, 0) @ pre_prob_rem_norm

        plt.plot(moving_average(a=spar_per_bin.flatten(), n=2000))
        plt.ylabel("Sparsity of mode * norm. likelihood")
        plt.xlabel("Pop. vec. (sleep)")
        plt.title("REM")
        plt.show()

        skaggs_sec_per_bin = np.expand_dims(skaggs_per_second, 0) @ pre_prob_rem_norm

        plt.plot(moving_average(a=skaggs_sec_per_bin.flatten(), n=2000))
        plt.ylabel("Skaggs (second) mode * norm. likelihood")
        plt.xlabel("Pop. vec. (sleep)")
        plt.title("REM")
        plt.show()

        skaggs_spike_per_bin = np.expand_dims(skaggs_per_spike, 0) @ pre_prob_rem_norm

        plt.plot(moving_average(a=skaggs_spike_per_bin.flatten(), n=2000))
        plt.ylabel("Skaggs (spike) mode * norm. likelihood")
        plt.xlabel("Pop. vec. (sleep)")
        plt.title("REM")
        plt.show()


        mode_freq_norm = mode_freq / np.sum(mode_freq)
        freq_per_bin = np.expand_dims(mode_freq_norm, 0) @ pre_prob_rem_norm

        plt.plot(moving_average(a=freq_per_bin.flatten(), n=2000))
        plt.ylabel("Norm. frequency of modes during behavior * norm. likelihood")
        plt.xlabel("Pop. vec. (sleep)")
        plt.title("REM")
        plt.show()

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_nrem_event, event_times_nrem, event_lengths_nrem, event_duration_nrem_in_s, pre_prob_nrem, \
        post_prob_nrem= self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="nrem",
                                                     pop_vec_threshold=2)

        pre_prob_nrem_norm = (pre_prob_nrem / np.sum(pre_prob_nrem, axis=1, keepdims=True)).T

        spar_per_bin = np.expand_dims(spar, 0) @ pre_prob_nrem_norm

        plt.plot(moving_average(a=spar_per_bin.flatten(), n=2000))
        plt.ylabel("Sparsity of mode * norm. likelihood")
        plt.xlabel("Pop. vec. (sleep)")
        plt.title("NREM")
        plt.show()

        skaggs_sec_per_bin = np.expand_dims(skaggs_per_second, 0) @ pre_prob_nrem_norm

        plt.plot(moving_average(a=skaggs_sec_per_bin.flatten(), n=2000))
        plt.ylabel("Skaggs (second) mode * norm. likelihood")
        plt.xlabel("Pop. vec. (sleep)")
        plt.title("NREM")
        plt.show()

        skaggs_spike_per_bin = np.expand_dims(skaggs_per_spike, 0) @ pre_prob_nrem_norm
        plt.plot(moving_average(a=skaggs_spike_per_bin.flatten(), n=2000))
        plt.ylabel("Skaggs (spike) mode * norm. likelihood")
        plt.xlabel("Pop. vec. (sleep)")
        plt.title("NREM")
        plt.show()


        mode_freq_norm = mode_freq / np.sum(mode_freq)
        freq_per_bin = np.expand_dims(mode_freq_norm, 0) @ pre_prob_nrem_norm

        plt.plot(moving_average(a=freq_per_bin.flatten(), n=2000))
        plt.ylabel("Norm. frequency of modes during behavior * norm. likelihood")
        plt.xlabel("Pop. vec. (sleep)")
        plt.title("NREM")
        plt.show()

    # PRE/POST template analysis
    # ------------------------------------------------------------------------------------------------------------------

    def drift_correlation_structure_equalized_firing_rates(self, bins_per_corr_matrix=600, n_smoothing=40,
                                                           plot_for_control=False, plotting=True, cells_to_use="stable"):
        # --------------------------------------------------------------------------------------------------------------
        # analyzes memory drift using correlation structure. Computes correlation matrix of awake behavior before and
        # correlation matrix of behavior after -> compares correlation matrix computed from sliding window during sleep
        # with before/after correlation matrix using Pearson correlation value
        #
        # parameters:   - correlation_window_size, int: size of sliding window (nr. time bins) to compute correlations
        #                 during sleep
        #
        # returns:      -
        # --------------------------------------------------------------------------------------------------------------

        # check if cheeseboard data or exploration data is used

        # get rasters from exploration before/after

        # load only stable cells
        with open(
                self.params.pre_proc_dir + "cell_classification/" + self.params.session_name + "_k_means.pickle",
                "rb") as f:
            class_dic = pickle.load(f)

        if cells_to_use == "stable":
            cell_ids = class_dic["stable_cell_ids"]
        elif cells_to_use == "increasing":
            cell_ids = class_dic["increase_cell_ids"]
        elif cells_to_use == "decreasing":
            cell_ids = class_dic["decrease_cell_ids"]
        else:
           raise Exception("Cell subset name not defined!")

        nr_cells = cell_ids.shape[0]

        raster_sleep = []

        for l_s in self.long_sleep:
            r = l_s.get_raster()
            raster_sleep.append(r)

        raster_sleep = np.hstack(raster_sleep)

        # only select stable cells
        raster_sleep = raster_sleep[cell_ids, :]

        len_chunk_s = 200
        nr_chunks = raster_sleep.shape[1]*self.params.time_bin_size/len_chunk_s
        chunk_size = np.round(raster_sleep.shape[1]/nr_chunks).astype(int)
        # split into equal size parts to find min. number of spikes
        raster_sleep_chunks = down_sample_array_sum(x=raster_sleep, chunk_size=chunk_size)

        # nr of windows with zero spikes
        nr_time_bins_more_than_zero_spikes = np.count_nonzero(np.where(raster_sleep_chunks==0, 1, 0), axis=0)
        # binarize
        nr_time_bins_more_than_zero_spikes = np.where(nr_time_bins_more_than_zero_spikes==0, 1, 0).astype(bool)

        # remove time time windows if there is a single cell that doesn't spike
        raster_sleep_chunks_filtered = raster_sleep_chunks[:, nr_time_bins_more_than_zero_spikes]

        # min. number of spikes per window
        min_nr_spikes = np.min(raster_sleep_chunks_filtered, axis=1).astype(int)

        # need to filter periods where there is no spike for at least one cell in the initial raster
        nr_time_bins_more_than_zero_spikes_orig_resolution = nr_time_bins_more_than_zero_spikes.repeat(chunk_size).astype(bool)
        raster_sleep_filtered = raster_sleep[:,:nr_time_bins_more_than_zero_spikes_orig_resolution.shape[0]]
        raster_sleep_filtered = raster_sleep_filtered[:,nr_time_bins_more_than_zero_spikes_orig_resolution]

        additional_chunk_to_remove = []
        equalized_raster = np.zeros(raster_sleep_filtered.shape)
        # go through rasters and remove random spikes to equalize firing rates over time
        for chunk_id in range(int(np.round(nr_chunks))):
            # if end of chunk is bigger that filtered raster --> leave loop
            if (chunk_id+1)*chunk_size > raster_sleep_filtered.shape[1]:
                additional_chunk_to_remove.append(chunk_id)
                break
            # find all time bins with spikes
            raster_chunk = raster_sleep_filtered[:, chunk_id*chunk_size:(chunk_id+1)*chunk_size]
            raster_chunk_copy = np.copy(raster_chunk)
            # go trough all cells
            for cell_id, cell_min_nr_spikes in enumerate(min_nr_spikes):
                # keep removing spikes until min. nr. spikes is reached
                cell_chunk = np.copy(raster_chunk[cell_id,:]).astype(int)
                nr_spikes_in_chunk = np.sum(cell_chunk)
                while nr_spikes_in_chunk > cell_min_nr_spikes:
                    nr_spikes_to_remove = nr_spikes_in_chunk - cell_min_nr_spikes
                    # find time bins with spikes
                    cell_bins_with_spikes = np.argwhere(cell_chunk > 0).flatten()
                    # chunk with zero spikes for one cell
                    if cell_bins_with_spikes.shape[0] == 0:
                        additional_chunk_to_remove.append(chunk_id)
                        break
                    # there are less time bins with spikes than spikes to remove --> need to go through while loop
                    # another time
                    elif nr_spikes_to_remove > cell_bins_with_spikes.shape[0]:
                        spikes_to_remove = np.random.choice(a=cell_bins_with_spikes, size=cell_bins_with_spikes.shape[0],
                                                            replace=False)
                    else:
                        spikes_to_remove = np.random.choice(a=cell_bins_with_spikes, size=nr_spikes_to_remove,
                                                            replace=False)
                    # remove n spikes from bins to match min_nr_spikes
                    cell_chunk[spikes_to_remove] -= 1
                    # check if more spikes need to be removed
                    nr_spikes_in_chunk = np.sum(cell_chunk)

                equalized_raster[cell_id, chunk_id*chunk_size:(chunk_id+1)*chunk_size] = cell_chunk

            # # if last chunk is too small --> do not add it
            # if chunk_size == raster_chunk_copy.shape[1]:
            #     equalized_raster[:, chunk_id*chunk_size:(chunk_id+1)*chunk_size] = raster_chunk_copy
            # else:
            #     break
        if len(additional_chunk_to_remove)>0:
            # remove additional chunks, TODO: might be useless
            min_chunks_to_remove = min(additional_chunk_to_remove)
            equalized_raster = equalized_raster[:, :min_chunks_to_remove*chunk_size]

        if plot_for_control:
            # check equalized raster
            equalized_raster_test = down_sample_array_sum(x=equalized_raster, chunk_size=chunk_size)
            plt.figure(figsize=(7,3))
            plt.imshow(equalized_raster_test)
            plt.title("Spikes per chunk")
            plt.xlabel("Chunk ID")
            plt.ylabel("Cell ID")
            plt.show()
            plt.figure(figsize=(9,2))
            plt.title("Spikes per time bin")
            plt.xlabel("Time bin")
            plt.ylabel("Cell ID")
            plt.imshow(equalized_raster[:,:500])
            plt.show()

        # compute correlations from equalized rasters --> 60 seconds was a good estimate to get full rank corr. matrices
        len_corr_chunk_s = 200
        nr_chunks_corr = equalized_raster.shape[1]*self.params.time_bin_size/len_corr_chunk_s
        chunk_size = np.round(equalized_raster.shape[1]/nr_chunks_corr).astype(int)

        corr_matrices_sleep = []

        for i in range(int(nr_chunks_corr)):
            corr_matrices_sleep.append(upper_tri_without_diag(np.corrcoef(equalized_raster[:, i*chunk_size:(i+1)*chunk_size])))

        raster_pre = self.pre.get_raster()
        raster_post = self.post.get_raster()

        # compute correlations of exploration before/after
        corr_pre = np.nan_to_num(np.corrcoef(raster_pre[cell_ids, :]))
        corr_post = np.nan_to_num(np.corrcoef(raster_post[cell_ids, :]))

        corr_post = upper_tri_without_diag(corr_post)
        corr_pre = upper_tri_without_diag(corr_pre)

        sim_pearson = []
        # for each sliding window compute similarity with behavior before/after
        for corr in corr_matrices_sleep:
            sim_post = abs(pearsonr(corr, corr_post)[0])
            sim_pre = abs(pearsonr(corr, corr_pre)[0])
            sim_pearson.append((sim_post - sim_pre) / (sim_post + sim_pre))

        if plotting:
            x_axis = (np.arange(len(sim_pearson)) * len_corr_chunk_s)/60/60
            fig = plt.figure()
            ax = fig.add_subplot()
            ax.plot(x_axis, sim_pearson, color="red", label="PEARSON")
            plt.title("CORRELATION STRUCTURE SIMILARITY: BEFORE - AFTER\n #Duration to compute corr: " +
                      str(np.round(len_corr_chunk_s/60,1))+" min")
            plt.xlabel("TIME (h)")
            plt.ylabel("SIMILARITY BEFORE - AFTER / PEARSON")
            plt.ylim(-1, 1)
            plt.show()

            # apply some smoothing
            sim_pearson_smooth = moving_average(a=np.array(sim_pearson), n=20)

            x_axis = (np.arange(len(sim_pearson_smooth)) * len_corr_chunk_s/200)/60/60
            fig = plt.figure()
            ax = fig.add_subplot()
            ax.plot(x_axis, sim_pearson_smooth, color="red", label="PEARSON")
            plt.title("CORRELATION STRUCTURE SIMILARITY: BEFORE - AFTER\n #Duration to compute corr: " +
                      str(np.round(len_corr_chunk_s/60,1))+" min - smoothed")
            plt.xlabel("TIME")
            plt.ylabel("SIMILARITY BEFORE - AFTER / PEARSON")
            plt.ylim(-1, 1)
            plt.show()
        else:
            return sim_pearson

    def drift_correlation_structure(self, plot_file_name="test", bins_per_corr_matrix=600,
                                           only_stable_cells=False, n_smoothing=40):
        # --------------------------------------------------------------------------------------------------------------
        # analyzes memory drift using correlation structure. Computes correlation matrix of awake behavior before and
        # correlation matrix of behavior after -> compares correlation matrix computed from sliding window during sleep
        # with before/after correlation matrix using Pearson correlation value
        #
        # parameters:   - correlation_window_size, int: size of sliding window (nr. time bins) to compute correlations
        #                 during sleep
        #
        # returns:      -
        # --------------------------------------------------------------------------------------------------------------

        # check if cheeseboard data or exploration data is used

            # get rasters from exploration before/after
            raster_pre = self.pre.get_raster()
            raster_post = self.post.get_raster()

            if only_stable_cells:
                # load only stable cells
                with open(
                        self.params.pre_proc_dir + "cell_classification/" + self.params.session_name + "_k_means.pickle",
                        "rb") as f:
                    class_dic = pickle.load(f)

                stable_ids = class_dic["stable_cell_ids"]

                nr_cells = stable_ids.shape[0]

                corr_sleep = np.zeros((int(nr_cells * (nr_cells - 1) / 2), 0))

                for l_s in self.long_sleep:
                    c_m = l_s.get_correlation_matrices(bins_per_corr_matrix=bins_per_corr_matrix,
                                                       cell_selection=stable_ids, only_upper_triangle=True)
                    corr_sleep = np.hstack((corr_sleep, c_m))

                # compute correlations of exploration before/after
                corr_pre = np.nan_to_num(np.corrcoef(raster_pre[stable_ids, :]))
                corr_post = np.nan_to_num(np.corrcoef(raster_post[stable_ids, :]))


            else:

                # compute correlations of exploration before/after
                corr_pre = np.nan_to_num(np.corrcoef(raster_pre))
                corr_post = np.nan_to_num(np.corrcoef(raster_post))

                nr_cells = raster_pre.shape[0]

                # corr_sleep = np.zeros((nr_cells ** 2, 0))
                # only off-diagonal elements
                corr_sleep = np.zeros((int(nr_cells * (nr_cells - 1) / 2), 0))

                for l_s in self.long_sleep:
                    c_m = l_s.get_correlation_matrices(bins_per_corr_matrix=bins_per_corr_matrix, only_upper_triangle=True)
                    corr_sleep = np.hstack((corr_sleep, c_m))

            # downsample speed
            #
            # speed = down_sample_array_mean(np.expand_dims(np.array(speed_list), 0), chunk_size=bins_per_corr_matrix).flatten()
            # speed = speed[:corr_sleep.shape[1]]
            
            corr_post = upper_tri_without_diag(corr_post)
            corr_pre = upper_tri_without_diag(corr_pre)

            sim_pearson = []

            # for each sliding window compute similarity with behavior before/after
            for corr in corr_sleep.T:
                sim_post = abs(pearsonr(corr, corr_post)[0])
                sim_pre = abs(pearsonr(corr, corr_pre)[0])
                sim_pearson.append((sim_post - sim_pre) / (sim_post + sim_pre))
                # TODO: how to deal with negative correlation values

            x_axis = (np.arange(len(sim_pearson))*bins_per_corr_matrix*self.params.time_bin_size/60)/60
            fig = plt.figure()
            ax = fig.add_subplot()
            ax.plot(x_axis, sim_pearson, color="red", label="PEARSON")
            plt.title("CORRELATION STRUCTURE SIMILARITY: BEFORE - AFTER\n #BINS PER WINDOW: " + str(bins_per_corr_matrix))
            plt.xlabel("TIME (hours)")
            plt.ylabel("SIMILARITY BEFORE - AFTER / PEARSON")
            plt.ylim(-1, 1)
            plt.show()

            sim_pearson = np.array(sim_pearson)

            s = sim_pearson.copy()
            control = []
            # control --> do 50 shuffles
            for i in range(50):
                np.random.shuffle(s)
                s_smooth = moving_average(a=s, n=n_smoothing)
                control.append(s_smooth)

            x_axis = (np.arange(s_smooth.shape[0]) * bins_per_corr_matrix * self.params.time_bin_size/60) / 60
            control = np.array(control)
            con_mean = np.mean(control, axis=0)
            con_std = np.std(control, axis=0)
            fig = plt.figure()
            ax = fig.add_subplot()
            # smoothing
            sim_pearson_s = moving_average(a=np.array(sim_pearson), n=n_smoothing)
            ax.plot(x_axis, con_mean, color="grey", label="CONTROL (MEAN +- STD), 50 SHUFFLES")
            ax.plot(x_axis,con_mean + con_std, color="grey", linestyle="dashed")
            ax.plot(x_axis,con_mean - con_std, color="grey", linestyle="dashed")

            ax.plot(x_axis, sim_pearson_s, color="red", label="DATA")
            plt.title("CORRELATION STRUCTURE SIMILARITY: PRE - POST\n #BINS PER WINDOW: " + str(bins_per_corr_matrix))
            plt.xlabel("TIME (hours)")
            plt.ylabel("PRE_POST SIMILARITY")
            plt.ylim(-0.33, 0.33)
            # plt.ylim(min(sim_pearson_s), -1*min(sim_pearson_s))
            plt.legend()
            plt.show()

            # speed_smooth = moving_average(a=speed, n=n_smoothing)
            # plt.plot(x_axis, speed_smooth)
            # plt.xlabel("TIME (hours)")
            # plt.ylabel("SPEED (cm/s)")
            # plt.show()


            exit()
            plt.savefig(self.params.pre_proc_dir + plot_file_name)

    def drift_population_vectors(self, plot_file_name="test", time_bin_size=60,
                                    only_stable_cells=False, n_smoothing=40):
        # --------------------------------------------------------------------------------------------------------------
        # analyzes memory drift using population vectors. Computes average population vector of awake behavior before and
        # average population vector of behavior after -> compares pop.vecs during sleep
        # with before/after population vector
        #
        # parameters:   - correlation_window_size, int: size of sliding window (nr. time bins) to compute correlations
        #                 during sleep
        #
        # returns:      -
        # --------------------------------------------------------------------------------------------------------------

        # check if cheeseboard data or exploration data is used

        # get rasters from exploration before/after
        raster_pre = self.pre.get_raster()
        raster_post = self.post.get_raster()

        pre_mean = np.mean(raster_pre, axis=1)
        post_mean = np.mean(raster_post, axis=1)

        if only_stable_cells:
            # load only stable cells
            with open(
                    self.params.pre_proc_dir + "cell_classification/" + self.params.session_name + "_k_means.pickle",
                    "rb") as f:
                class_dic = pickle.load(f)

            stable_ids = class_dic["stable_cell_ids"]

            nr_cells = stable_ids.shape[0]

            corr_sleep = np.zeros((int(nr_cells * (nr_cells - 1) / 2), 0))

            for l_s in self.long_sleep:
                c_m = l_s.get_correlation_matrices(bins_per_corr_matrix=bins_per_corr_matrix,
                                                   cell_selection=stable_ids, only_upper_triangle=True)
                corr_sleep = np.hstack((corr_sleep, c_m))

            # compute correlations of exploration before/after
            corr_pre = np.nan_to_num(np.corrcoef(raster_pre[stable_ids, :]))
            corr_post = np.nan_to_num(np.corrcoef(raster_post[stable_ids, :]))


        else:

            nr_cells = raster_pre.shape[0]

            # get rasters
            raster = []
            for l_s in self.long_sleep:
                r = l_s.get_raster(speed_threshold=True)
                raster.append(r)

            raster = np.hstack(raster)
            scaler = int(time_bin_size / self.params.time_bin_size)
            raster = down_sample_array_sum(x=raster, chunk_size=scaler)

        sim_pearson = []
        sim_cos = []
        sim_norm = []

        # for each sliding window compute similarity with behavior before/after
        for pv in raster.T:
            # sim_post = abs(pearsonr(pv, post_mean)[0])
            # sim_pre = abs(pearsonr(pv, pre_mean)[0])
            sim_post = 1-distance.cosine(pv, post_mean)
            sim_pre = 1-distance.cosine(pv, pre_mean)
            sim_pearson.append((sim_post - sim_pre) / (sim_post + sim_pre))

        x_axis = (np.arange(len(sim_pearson)) * time_bin_size/60 )/ 60
        fig = plt.figure()
        ax = fig.add_subplot()
        ax.plot(x_axis, sim_pearson, color="red", label="PEARSON")
        plt.title("POPULATION VECTOR SIMILARITY: BEFORE - AFTER\n TIME BIN SIZE: " + str(time_bin_size))
        plt.xlabel("TIME (hours)")
        plt.ylabel("SIMILARITY BEFORE - AFTER / PEARSON")
        plt.ylim(-1, 1)
        plt.show()

        sim_pearson = np.array(sim_pearson)

        s = sim_pearson.copy()
        control = []
        # control --> do 50 shuffles
        for i in range(50):
            np.random.shuffle(s)
            s_smooth = moving_average(a=s, n=n_smoothing)
            control.append(s_smooth)

        x_axis = (np.arange(s_smooth.shape[0]) * time_bin_size/60) / 60
        control = np.array(control)
        con_mean = np.mean(control, axis=0)
        con_std = np.std(control, axis=0)
        fig = plt.figure()
        ax = fig.add_subplot()
        # smoothing
        sim_pearson_s = moving_average(a=np.array(sim_pearson), n=n_smoothing)
        ax.plot(x_axis, con_mean, color="grey", label="CONTROL (MEAN +- STD), 50 SHUFFLES")
        ax.plot(x_axis, con_mean + con_std, color="grey", linestyle="dashed")
        ax.plot(x_axis, con_mean - con_std, color="grey", linestyle="dashed")

        ax.plot(x_axis, sim_pearson_s, color="red", label="DATA")
        plt.title("POPULATION VECTOR SIMILARITY: PRE - POST\n TIME BIN SIZE: " + str(time_bin_size))
        plt.xlabel("TIME (hours)")
        plt.ylabel("PRE_POST SIMILARITY")
        plt.ylim(-0.33, 0.33)
        # plt.ylim(min(sim_pearson_s), -1*min(sim_pearson_s))
        plt.legend()
        plt.show()

        # speed_smooth = moving_average(a=speed, n=n_smoothing)
        # plt.plot(x_axis, speed_smooth)
        # plt.xlabel("TIME (hours)")
        # plt.ylabel("SPEED (cm/s)")
        # plt.show()

        exit()
        plt.savefig(self.params.pre_proc_dir + plot_file_name)

    def drift_using_pre_post_model(self, file_name, use_full_model=False, part_to_analyze="rem"):

        # check if model file exists already --> otherwise fit model again
        print("- LOADING PHMM MODEL FROM FILE\n")
        with open(self.params.pre_proc_dir+"phmm/" + file_name + ".pkl", "rb") as file:
            model = pickle.load(file)

        nr_modes = model.means_.shape[0]

        # get pre and post raster data
        pre_raster = self.pre.get_raster()
        post_raster = self.post.get_raster()

        # get #occurences modes for pre and post
        pre_mode_ids, pre_counts = np.unique(model.predict(pre_raster.T), return_counts=True)
        pre_counts = (pre_counts / np.sum(pre_counts))

        mode_occ_pre = np.zeros(nr_modes)
        mode_occ_pre[pre_mode_ids] = pre_counts

        # get #occurences modes for pre and post
        post_mode_ids, post_counts = np.unique(model.predict(post_raster.T), return_counts=True)
        post_counts = (post_counts / np.sum(post_counts))

        mode_occ_post = np.zeros(nr_modes)
        mode_occ_post[post_mode_ids] = post_counts

        # define POST and PRE modes by occurence
        pre_modes = []
        post_modes = []
        common_modes = []
        for mode_id, (pre_occ, post_occ) in enumerate(zip(mode_occ_pre, mode_occ_post)):
            if pre_occ < 0.1 * post_occ:
                post_modes.append(mode_id)
            elif post_occ < 0.1 * pre_occ:
                pre_modes.append(mode_id)
            else:
                common_modes.append(mode_id)
        pre_modes = np.array(pre_modes)
        post_modes = np.array(post_modes)
        common_modes = np.array(common_modes)

        results = []
        mls = []
        for l_s in self.long_sleep:
            likelihood, _, _, most_likely_state_sequence = l_s.decode_poisson_hmm_sleep(part_to_analyze=part_to_analyze,
                                                                                        template_file_name=file_name,
                                                            use_full_model=use_full_model, return_results=True)
            results.extend(likelihood)
            mls.extend(most_likely_state_sequence)
        if use_full_model:
            # can use viterbi results or posterior probability
            seq = np.hstack(mls)
        else:
            results = np.vstack(results)
            seq = np.argmax(results, axis=1)

        # look at temporal changes
        nr_bins_per_window = 25
        nr_bins = int(seq.shape[0]/nr_bins_per_window)

        pre_occ_per_window = []
        post_occ_per_window = []
        common_occ_per_window = []
        for window_id in range(nr_bins):
            window = seq[(window_id*nr_bins_per_window):((window_id+1)*nr_bins_per_window)]
            pre_occ_per_window.append(np.count_nonzero(np.isin(window, np.array(pre_modes)))/nr_bins_per_window)
            post_occ_per_window.append(np.count_nonzero(np.isin(window, np.array(post_modes))) / nr_bins_per_window)
            common_occ_per_window.append(np.count_nonzero(np.isin(window, np.array(common_modes))) / nr_bins_per_window)


        pre_occ_per_window_smooth = moving_average(a=np.array(pre_occ_per_window), n=100)
        post_occ_per_window_smooth = moving_average(a=np.array(post_occ_per_window), n=100)
        common_occ_per_window_smooth = moving_average(a=np.array(common_occ_per_window), n=100)

        plt.plot(pre_occ_per_window_smooth, color="violet", label="PRE MODES")
        plt.plot(common_occ_per_window_smooth, color="white", label="COMMON MODES")
        plt.plot(post_occ_per_window_smooth, color="lightskyblue", label="POST MODES")
        plt.ylabel("OCCURENCE (%)")
        plt.xlabel("WINDOW ID")
        plt.title("OCCURENCE OF MODES PER WINDOW OF "+str(nr_bins_per_window)+" BINS (SMOOTHED)")
        plt.legend()
        plt.show()


        exit()
        # look at stationary data

        mode_ids, mode_occ = np.unique(seq, return_counts=True)
        mode_occ_norm_number = mode_occ / np.sum(mode_occ)

        mode_occ_norm = np.zeros(nr_modes)
        mode_occ_norm[mode_ids] = mode_occ_norm_number

        lines = []
        for i in range(pre_modes.shape[0]):
            pair = [(pre_modes[i], 0), (pre_modes[i], mode_occ_norm[pre_modes][i])]
            lines.append(pair)

        linecoll = matcoll.LineCollection(lines, colors="violet")
        fig, ax = plt.subplots()

        ax.add_collection(linecoll)

        lines = []
        for i in range(common_modes.shape[0]):
            pair = [(common_modes[i], 0), (common_modes[i], mode_occ_norm[common_modes][i])]
            lines.append(pair)

        linecoll = matcoll.LineCollection(lines, colors="white")

        ax.add_collection(linecoll)

        lines = []
        for i in range(post_modes.shape[0]):
            pair = [(post_modes[i], 0), (post_modes[i], mode_occ_norm[post_modes][i])]
            lines.append(pair)

        linecoll = matcoll.LineCollection(lines, colors="lightskyblue")

        ax.add_collection(linecoll)

        plt.scatter(pre_modes, mode_occ_norm[pre_modes], c="violet", label="PRE MODES" )
        plt.scatter(post_modes, mode_occ_norm[post_modes], c="lightskyblue", alpha=0.8, label="POST MODES")
        plt.scatter(common_modes, mode_occ_norm[common_modes], c="white", alpha=0.8, label="COMMON MODES")
        plt.legend(loc="upper left")
        plt.xlabel("MODE ID")
        plt.ylabel("MODE OCCURENCE SLEEP (%)")
        plt.title("MODE OCCURENCE DURING SLEEP ("+part_to_analyze+")")
        plt.show()

        # select all
        print("HERE")

    # PRE, sleep, POST correlation comparison
    # ------------------------------------------------------------------------------------------------------------------

    def cell_type_correlations(self, awake_smoothing=3, sleep_smoothing=5, part_to_analyze="rem", nr_blocks=5,
                               plotting=True):

        # load cell labels
        # --------------------------------------------------------------------------------------------------------------
        with open(self.params.pre_proc_dir + "cell_classification/" +
                  self.params.session_name + "_"+self.params.stable_cell_method +".pickle", "rb") as f:
            class_dic = pickle.load(f)

        stable_cells = class_dic["stable_cell_ids"].flatten()
        inc_cells = class_dic["increase_cell_ids"].flatten()
        dec_cells = class_dic["decrease_cell_ids"].flatten()

        n_stable_cells = len(stable_cells)

        # get rasters
        raster_pre = self.pre.get_raster()
        raster_post = self.post.get_raster()

        # filter cells that don't fire
        pre_to_delete = np.argwhere(np.sum(raster_pre, axis=1)==0).flatten()
        post_to_delete = np.argwhere(np.sum(raster_post, axis=1) == 0).flatten()

        # delete stable cells that don't fire in pre or post
        for to_delete in np.hstack((pre_to_delete, post_to_delete)):
            stable_cells = stable_cells[stable_cells != to_delete]

        # delete decreasing cells that don't fire in pre
        for to_delete in pre_to_delete:
            dec_cells = dec_cells[dec_cells != to_delete]

        # delete increasing cells that don't fire in post
        for to_delete in post_to_delete:
            inc_cells = inc_cells[inc_cells != to_delete]

        # PRE DATA
        # --------------------------------------------------------------------------------------------------------------
        # apply smoothing
        raster_pre = uniform_filter1d(raster_pre, size=awake_smoothing, axis=1)

        # select stable and dec for pre correlations
        raster_stable_dec_pre = np.vstack((raster_pre[stable_cells, :], raster_pre[dec_cells, :]))
        # filter silent periods, z-score and compute correlations
        raster_stable_dec_pre = zscore(raster_stable_dec_pre[:, np.sum(raster_stable_dec_pre, axis=0)>1], axis=1)
        corr_stable_dec_pre = squareform(1-pdist(raster_stable_dec_pre,
                                                 "correlation"))[:n_stable_cells, n_stable_cells:].flatten()
        # stable correlations
        raster_stable_pre = raster_pre[stable_cells,:]
        raster_stable_pre = zscore(raster_stable_pre[:, np.sum(raster_stable_pre, axis=0)>1], axis=1)
        corr_stable_pre = 1-pdist(raster_stable_pre, "correlation")

        # decreasing correlations
        raster_dec_pre = raster_pre[dec_cells, :]
        raster_dec_pre = raster_dec_pre[:, np.sum(raster_dec_pre, axis=0) > 1]
        raster_dec_pre = zscore(raster_dec_pre, axis=1)
        corr_dec_pre = 1 - pdist(raster_dec_pre, 'correlation')


        # POST DATA
        # --------------------------------------------------------------------------------------------------------------
        # apply smoothing
        raster_post = uniform_filter1d(raster_post, size=awake_smoothing, axis=1)

        # select stable and dec for pre correlations
        raster_stable_inc_post = np.vstack((raster_post[stable_cells, :], raster_post[inc_cells, :]))
        # filter silent periods, z-score and compute correlations
        raster_stable_inc_post = zscore(raster_stable_inc_post[:, np.sum(raster_stable_inc_post, axis=0) > 1], axis=1)
        corr_stable_inc_post = squareform(1 - pdist(raster_stable_inc_post,
                                                   "correlation"))[:n_stable_cells, n_stable_cells:].flatten()
        # stable correlations
        raster_stable_post = raster_post[stable_cells, :]
        raster_stable_post = zscore(raster_stable_post[:, np.sum(raster_stable_post, axis=0) > 1], axis=1)
        corr_stable_post = 1 - pdist(raster_stable_post, "correlation")

        # decreasing correlations
        raster_inc_post = raster_post[inc_cells, :]
        raster_inc_post = raster_inc_post[:, np.sum(raster_inc_post, axis=0) > 1]
        raster_inc_post = zscore(raster_inc_post, axis=1)
        corr_inc_post = 1 - pdist(raster_inc_post, 'correlation')

        # GET SLEEP DATA CORRELATIONS
        # --------------------------------------------------------------------------------------------------------------
        rasters_sleep = []
        for sleep in self.long_sleep:
            rasters, _  = sleep.get_event_spike_rasters(part_to_analyze=part_to_analyze)
            rasters_sleep.append(np.hstack(rasters))

        rasters_sleep = np.hstack(rasters_sleep)

        block_length = int(rasters_sleep.shape[1]/nr_blocks)

        dec_pre = []
        inc_post = []
        stable_pre = []
        stable_post = []
        stable_dec_pre = []
        stable_inc_post = []

        for block_id in range(nr_blocks):
            data = rasters_sleep[:, block_id*block_length:(block_id+1)*block_length]
            # smoothen data
            data = uniform_filter1d(data, size=sleep_smoothing, axis=1)

            # select decreasing cells
            dec_data = data[dec_cells,:]
            dec_data = dec_data[:, np.sum(dec_data, axis=0)>1]
            dec_data = np.nan_to_num(zscore(dec_data, axis=1))
            dec_pre.append(pearsonr(np.nan_to_num(1-pdist(dec_data,"correlation")), corr_dec_pre)[0])

            # select increasing cells
            inc_data = data[inc_cells,:]
            inc_data = inc_data[:, np.sum(inc_data, axis=0)>1]
            inc_data = np.nan_to_num(zscore(inc_data, axis=1))
            inc_post.append(pearsonr(np.nan_to_num(1-pdist(inc_data,"correlation")), corr_inc_post)[0])

            # select stable cells
            stable_data = data[stable_cells,:]
            stable_data = stable_data[:, np.sum(stable_data, axis=0)>1]
            stable_data = np.nan_to_num(zscore(stable_data, axis=1))
            stable_pre.append(pearsonr(np.nan_to_num(1-pdist(stable_data,"correlation")), corr_stable_pre)[0])
            stable_post.append(pearsonr(np.nan_to_num(1 - pdist(stable_data, "correlation")), corr_stable_post)[0])

            # select stable & decreasing cells for PRE
            stable_dec_data = np.vstack((data[stable_cells, :], data[dec_cells, :]))
            stable_dec_data = np.nan_to_num(zscore(stable_dec_data[:, np.sum(stable_dec_data, axis=0)>1], axis=1))
            corr_stable_dec = squareform(np.nan_to_num(1-pdist(stable_dec_data,
                                                 "correlation")))[:n_stable_cells, n_stable_cells:].flatten()
            stable_dec_pre.append(pearsonr(corr_stable_dec, corr_stable_dec_pre)[0])

            # select stable & increasing cells for POST
            stable_inc_data = np.vstack((data[stable_cells, :], data[inc_cells, :]))
            stable_inc_data = np.nan_to_num(zscore(stable_inc_data[:, np.sum(stable_inc_data, axis=0)>1], axis=1))
            corr_stable_inc = squareform(np.nan_to_num(1-pdist(stable_inc_data,
                                                 "correlation")))[:n_stable_cells, n_stable_cells:].flatten()
            stable_inc_post.append(pearsonr(corr_stable_inc, corr_stable_inc_post)[0])

        if plotting:
            plt.figure(figsize=(5,15))
            plt.subplot(5,1,1)
            plt.plot(stable_pre, color="blue", label="PRE")
            plt.plot(stable_post, color="orange", label="POST")
            plt.ylabel("PEARSON R")
            plt.title("STABLE")
            plt.legend()
            #plt.ylim([0, y_max])
            plt.subplot(5, 1, 2)
            plt.plot(inc_post, color="orange")
            plt.title("INC")
            #plt.ylim([0, y_max])
            plt.subplot(5,1,3)
            plt.plot(dec_pre, color="blue")
            plt.title("DEC")
            #plt.ylim([0,y_max])
            plt.subplot(5,1,4)
            plt.plot(stable_inc_post, color="orange")
            plt.title("STABLE-INC")
            #plt.ylim([0, y_max])
            plt.subplot(5,1,5)
            plt.plot(stable_dec_pre, color="blue")
            plt.title("STABLE-DEC")
            #plt.ylim([0, y_max])
            plt.show()

        else:
            return stable_pre, stable_post, inc_post, dec_pre, stable_inc_post, stable_dec_pre

    def sleep_k_means_clustering_post_correlations(self, n_clusters=5):

        # get rasters
        raster = []
        first = 0
        for l_s in self.long_sleep:
            duration = l_s.get_duration_sec()
            r, t = l_s.get_spike_binned_raster(return_estimated_times=True)
            raster.append(r)
            first += duration
        raster = np.hstack(raster)

        # down sample raster
        raster_ds = down_sample_array_mean(x=raster, chunk_size=5000)
        raster_ds = raster_ds / np.max(raster_ds, axis=1, keepdims=True)

        # smooth down sampled rasters
        raster_ds_smoothed = []

        for cell_arr in raster_ds:
            s = moving_average(a=cell_arr, n=50)
            s = s/np.max(s)
            raster_ds_smoothed.append(s)

        raster_ds_smoothed = np.array(raster_ds_smoothed)

        # k-means clustering
        kmeans = KMeans(n_clusters=n_clusters).fit(X=raster_ds_smoothed)
        k_labels = kmeans.labels_

        stable = []
        increase = []
        decrease = []
        # find clusters with constant/decreasing/increasing firing rates
        # compare firing during first 20% with firing during last 20%
        for cl_id in np.unique(k_labels):
            cl_data = raster_ds_smoothed[k_labels == cl_id, :]
            diff = np.mean(cl_data[:, -int(0.2 * cl_data.shape[1]):].flatten()) - np.mean(
                cl_data[:, :int(0.2 * cl_data.shape[1])].flatten())
            if diff < -0.09:
                decrease.append(cl_id)
            elif diff > 0.09:
                increase.append(cl_id)
            else:
                stable.append(cl_id)

        decrease = np.array(decrease)
        increase = np.array(increase)
        stable = np.array(stable)

        print("STABLE CLUSTER IDS: "+str(stable))
        print("INCREASING CLUSTER IDS: " + str(increase))
        print("DECREASING CLUSTER IDS: " + str(decrease))

        stable_cell_ids = []
        decrease_cell_ids = []
        increase_cell_ids = []
        for stable_cluster_id in stable:
            stable_cell_ids.append(np.where(k_labels==stable_cluster_id)[0])
        for dec_cluster_id in decrease:
            decrease_cell_ids.append(np.where(k_labels==dec_cluster_id)[0])
        for inc_cluster_id in increase:
            increase_cell_ids.append(np.where(k_labels==inc_cluster_id)[0])

        # stable_cell_ids = np.hstack(stable_cell_ids)
        # decrease_cell_ids = np.hstack(decrease_cell_ids)
        # increase_cell_ids = np.hstack(increase_cell_ids)
        k_labels_plotting = np.copy(k_labels)
        k_labels_plotting[k_labels_plotting % 2 == 1] = 1
        k_labels_plotting[k_labels_plotting % 2 != 1] = 2
        k_labels_sorted = k_labels.argsort()

        fig = plt.figure(figsize=(8,6))
        gs = fig.add_gridspec(6, 20)

        ax1 = fig.add_subplot(gs[:, 0])
        ax1.set_title("CLUSTERS")
        ax2 = fig.add_subplot(gs[:, 3:-2])
        ax3 = fig.add_subplot(gs[:, -1:])

        # plotting

        ax1.imshow(np.expand_dims(k_labels_plotting[k_labels_sorted], 1), aspect="auto", cmap="Set1")
        ax1.axis("off")
        rate_map = ax2.imshow(raster_ds_smoothed[k_labels_sorted,:], interpolation='nearest', aspect='auto')
        ax2.set_xlabel("BIN ID")
        ax2.set_ylabel("CELLS SORTED")
        a = plt.colorbar(rate_map, cax=ax3)
        a.set_label("#SPIKES / NORMALIZED")
        plt.show()

        # get post raster
        post_raster = self.post.get_raster()

        # go through all increasing clusters and compute correlations
        # in post
        inc_corr = []
        for inc_cluster_cells in increase_cell_ids:
            if inc_cluster_cells.shape[0] > 1:
                inc_clust_raster = post_raster[inc_cluster_cells,:]
                correlations = upper_tri_without_diag(np.corrcoef(inc_clust_raster))
                inc_corr.append(correlations)

        all_corr = upper_tri_without_diag(np.corrcoef(post_raster))
        all_corr = np.nan_to_num(np.array(all_corr))
        # plot distributions
        for i, corr in enumerate(inc_corr):
            corr = np.nan_to_num(np.array(corr))
            plt.hist(all_corr, np.arange(np.min(np.hstack((corr, all_corr))),
                 np.max(np.hstack((corr, all_corr))),0.02), color="gray", density=True, label="ALL CELLS")
            plt.hist(corr, np.arange(np.min(np.hstack((corr, all_corr))),
                 np.max(np.hstack((corr, all_corr))),0.02), color="red", alpha=0.7,
                     label="INCREASING CELLS, CLUSTER "+str(i), density=True)
            plt.legend()
            plt.xlabel("CORRELATIONS")
            plt.ylabel("DENSITY")
            plt.show()

    def plot_cell_classification_mean_firing_rates_awake(self, sorting="awake_average", save_fig=False, plotting=True):

        pre_raster_mean = np.mean(self.pre.get_raster(), axis=1)
        post_raster_mean = np.mean(self.post.get_raster(), axis=1)

        norm_diff = (post_raster_mean - pre_raster_mean) / (post_raster_mean + pre_raster_mean)

        # plt.hist(norm_diff, bins=30, density=True)
        # plt.xlabel("PRE-POST DIFF. NORMALIZED")
        # plt.ylabel("DENSITY")
        # plt.show()

        # stable_cell_ids = np.argwhere((norm_diff < 0.33) & (-0.33 < norm_diff)).flatten()
        # decrease_cell_ids = np.argwhere(norm_diff < -0.33).flatten()
        # increase_cell_ids = np.argwhere(norm_diff > 0.33).flatten()
        with open(self.params.pre_proc_dir + "cell_classification/" +
                  self.session_name + "_"+self.params.stable_cell_method+".pickle", "rb") as f:
            class_dic = pickle.load(f)

        stable_cell_ids = class_dic["stable_cell_ids"].flatten()
        increase_cell_ids = class_dic["increase_cell_ids"].flatten()
        decrease_cell_ids = class_dic["decrease_cell_ids"].flatten()

        # get sleep raster
        raster = []
        duration_sleep = 0
        for l_s in self.long_sleep:
            duration = l_s.get_duration_sec()
            r = l_s.get_raster()
            raster.append(r)
            duration_sleep += duration
        raster = np.hstack(raster)
        duration_sleep_h = np.round(duration_sleep/60/60, 0).astype(int)

        chunk_size = int(raster.shape[1]/20)
        # down sample raster
        raster_ds = down_sample_array_mean(x=raster, chunk_size=chunk_size)
        # raster_ds = raster_ds / np.max(raster_ds, axis=1, keepdims=True)
        # normalize firing per cell to lie between 0 and 1 for visualization
        # min_fir = np.min(np.hstack(
        #     (np.expand_dims(pre_raster_mean, 1), np.expand_dims(post_raster_mean, 1), raster_ds_smoothed)))
        max_fir = np.max(np.hstack(
            (np.expand_dims(pre_raster_mean, 1), raster_ds, np.expand_dims(post_raster_mean, 1))),
            axis=1, keepdims=True)
        raster_ds = raster_ds / max_fir[:, :raster_ds.shape[1]]
        pre_raster_mean = pre_raster_mean / max_fir[:, 0]
        post_raster_mean = post_raster_mean / max_fir[:, 0]
        # smooth down sampled rasters
        raster_ds_smoothed = []
        for cell_arr in raster_ds:
            s = moving_average(a=cell_arr, n=10)
            s = s / np.max(s)
            raster_ds_smoothed.append(s)
        raster_ds_smoothed = np.array(raster_ds_smoothed)

        raster_ds_smoothed_stable = raster_ds_smoothed[stable_cell_ids,:]
        pre_raster_mean_stable = pre_raster_mean[stable_cell_ids]
        post_raster_mean_stable = post_raster_mean[stable_cell_ids]
        # sort stable cells using k-means
        if sorting == "k_means_sorting":
            kmeans = KMeans(n_clusters=10).fit(X=raster_ds_smoothed_stable)
            k_labels = kmeans.labels_
            sorted_ind = k_labels.argsort()
            raster_ds_smoothed_stable_sorted = raster_ds_smoothed_stable[sorted_ind[::-1]]
            pre_raster_mean_stable_sorted = pre_raster_mean_stable[sorted_ind[::-1]]
            post_raster_mean_stable_sorted = post_raster_mean_stable[sorted_ind[::-1]]
        # sort cells according to awake firing rate
        elif sorting == "awake_average":
            raster_ds_smoothed_stable_sorted = raster_ds_smoothed_stable[pre_raster_mean_stable.argsort(),:]
            pre_raster_mean_stable_sorted = pre_raster_mean_stable[pre_raster_mean_stable.argsort()]
            post_raster_mean_stable_sorted = post_raster_mean_stable[pre_raster_mean_stable.argsort()]

        # sort increasing cells using k-means
        raster_ds_smoothed_increasing = raster_ds_smoothed[increase_cell_ids]
        pre_raster_mean_increasing = pre_raster_mean[increase_cell_ids]
        post_raster_mean_increasing = post_raster_mean[increase_cell_ids]
        raster_ds_smoothed_increasing = np.nan_to_num(raster_ds_smoothed_increasing)
        if sorting == "k_means_sorting":
            kmeans = KMeans(n_clusters=10).fit(X=raster_ds_smoothed_increasing)
            k_labels = kmeans.labels_
            sorted_ind = k_labels.argsort()
            raster_ds_smoothed_increasing_sorted = raster_ds_smoothed_increasing[sorted_ind[::-1]]
            pre_raster_mean_increasing_sorted = pre_raster_mean_increasing[sorted_ind[::-1]]
            post_raster_mean_increasing_sorted = post_raster_mean_increasing[sorted_ind[::-1]]
        elif sorting == "awake_average":
            raster_ds_smoothed_increasing_sorted = raster_ds_smoothed_increasing[pre_raster_mean_increasing.argsort(),:]
            pre_raster_mean_increasing_sorted = pre_raster_mean_increasing[pre_raster_mean_increasing.argsort()]
            post_raster_mean_increasing_sorted = post_raster_mean_increasing[pre_raster_mean_increasing.argsort()]

        # sort decreasing cells using k-means
        raster_ds_smoothed_decreasing = raster_ds_smoothed[decrease_cell_ids]
        pre_raster_mean_decreasing = pre_raster_mean[decrease_cell_ids]
        post_raster_mean_decreasing = post_raster_mean[decrease_cell_ids]
        if sorting == "k_means_sorting":
            kmeans = KMeans(n_clusters=10).fit(X=raster_ds_smoothed_decreasing)
            k_labels = kmeans.labels_
            sorted_ind = k_labels.argsort()
            raster_ds_smoothed_decreasing_sorted = raster_ds_smoothed_decreasing[sorted_ind[::-1]]
            pre_raster_mean_decreasing_sorted = pre_raster_mean_decreasing[sorted_ind[::-1]]
            post_raster_mean_decreasing_sorted = post_raster_mean_decreasing[sorted_ind[::-1]]
        elif sorting == "awake_average":
            raster_ds_smoothed_decreasing_sorted = raster_ds_smoothed_decreasing[pre_raster_mean_decreasing.argsort(),:]
            pre_raster_mean_decreasing_sorted = pre_raster_mean_decreasing[pre_raster_mean_decreasing.argsort()]
            post_raster_mean_decreasing_sorted = post_raster_mean_decreasing[pre_raster_mean_decreasing.argsort()]

        # stack them back together for plotting
        pre_raster_mean_sorted = np.hstack((pre_raster_mean_stable_sorted, pre_raster_mean_increasing_sorted, pre_raster_mean_decreasing_sorted))
        post_raster_mean_sorted = np.hstack(
            (post_raster_mean_stable_sorted, post_raster_mean_increasing_sorted, post_raster_mean_decreasing_sorted))

        raster_ds_smoothed_sorted = np.vstack((raster_ds_smoothed_stable_sorted,raster_ds_smoothed_increasing_sorted,raster_ds_smoothed_decreasing_sorted ))

        if save_fig or plotting:

            if save_fig:
                plt.style.use('default')

            fig = plt.figure(figsize=(8, 6))
            gs = fig.add_gridspec(15, 20)
            ax1 = fig.add_subplot(gs[:-3, :2])
            ax2 = fig.add_subplot(gs[:-3, 2:-2])
            ax3 = fig.add_subplot(gs[:-3, -2:])
            ax4 = fig.add_subplot(gs[-1, :10])
            ax1.imshow(np.expand_dims(pre_raster_mean_sorted, 1), vmin=0, vmax=1, interpolation='nearest',
                       aspect='auto')
            ax1.hlines(stable_cell_ids.shape[0], -0.5, 0.5, color="red")
            ax1.hlines(stable_cell_ids.shape[0] + increase_cell_ids.shape[0], -0.5, 0.5, color="red")
            ax1.set_xticks([])
            ax1.set_xlabel("Cells")
            cax = ax2.imshow(raster_ds_smoothed_sorted, vmin=0, vmax=1, interpolation='nearest',
                             aspect='auto')
            ax2.hlines(stable_cell_ids.shape[0], -0.5, raster_ds_smoothed_sorted.shape[1] - 0.5, color="red")
            ax2.hlines(stable_cell_ids.shape[0] + increase_cell_ids.shape[0], -0.5,
                       raster_ds_smoothed_sorted.shape[1] - 0.5,
                       color="red")
            ax2.set_yticks([])
            ax2.set_xticks([0, 0.33 * raster_ds_smoothed_sorted.shape[1], 0.66 * raster_ds_smoothed_sorted.shape[1],
                            raster_ds_smoothed_sorted.shape[1] - 0.5])
            ax2.set_xticklabels(
                ["0", str(int(np.round(duration_sleep_h * 0.33, 0))), str(int(np.round(duration_sleep_h * 0.66, 0))),
                 str(duration_sleep_h)])
            ax2.set_xlabel("Sleep duration (h)")
            ax3.imshow(np.expand_dims(post_raster_mean_sorted, 1), vmin=0, vmax=1, interpolation='nearest',
                       aspect='auto')
            ax3.hlines(stable_cell_ids.shape[0], -0.5, 0.5, color="red")
            ax3.hlines(stable_cell_ids.shape[0] + increase_cell_ids.shape[0], -0.5, 0.5, color="red")
            ax3.set_yticks([])
            ax3.set_xticks([])
            a = fig.colorbar(mappable=cax, cax=ax4, orientation="horizontal", ticks=[0, 1])
            a.ax.set_xticklabels(["0", "1"])
            a.ax.set_xlabel("Normalized firing rate")
            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig("cell_classification.svg", transparent="True")
            else:
                plt.show()
        else:

            return raster_ds_smoothed_stable, pre_raster_mean_stable, post_raster_mean_stable, \
                   raster_ds_smoothed_decreasing, pre_raster_mean_decreasing, post_raster_mean_decreasing, \
                   raster_ds_smoothed_increasing, pre_raster_mean_increasing, post_raster_mean_increasing,

    def plot_likelihoods_ising(self, save_fig=False):

        # get pre map for dimensions
        pre_template = self.pre.load_glm_awake(plotting=False)
        post_template = self.post.load_glm_awake(plotting=False)

        pre_occ = self.pre.get_occ_map(spatial_resolution=5)
        post_occ = self.post.get_occ_map(spatial_resolution=5)


        pre_prob, post_prob, event_times, swr_to_nrem = self.long_sleep[0].decode_activity_using_pre_post(template_type="ising",
                                                                                                          part_to_analyze="rem")

        print("HERE")
        # use only second time bin to plot likelihoods
        pre_like = pre_prob[0][1, :]
        post_like = post_prob[0][1, :]

        pre_like_2d = np.reshape(pre_like, (pre_template.shape[1], pre_template.shape[2]))
        post_like_2d = np.reshape(post_like, (post_template.shape[1], post_template.shape[2]))

        pre_like_2d[pre_occ == 0] = np.nan
        post_like_2d[post_occ == 0] = np.nan
        plt.style.use('default')
        plt.imshow(pre_like_2d, cmap="YlOrRd")
        plt.title("PRE")
        plt.colorbar()
        plt.rcParams['svg.fonttype'] = 'none'
        if save_fig:
            plt.savefig("pre_likeli_ising.svg")
        else:
                plt.show()

        plt.imshow(post_like_2d, cmap="YlOrRd")
        plt.title("POST")
        plt.colorbar()
        plt.rcParams['svg.fonttype'] = 'none'
        if save_fig:
            plt.savefig("post_likeli_ising.svg")
        else:
            plt.show()

    def predict_pre_or_post(self):

        pre_act, _, _ = self.pre.get_raster_location_speed()
        post_act, _, _ = self.post.get_raster_location_speed()

        y = np.hstack((np.zeros(pre_act.shape[1]), np.ones(post_act.shape[1])))
        x = np.hstack((pre_act, post_act))

        per_ind = np.random.permutation(np.arange(x.shape[1]))
        x_shuffled = x[:, per_ind].T
        y_shuffled = y[per_ind]

        # split into test and training
        x_train = x_shuffled[:int(x_shuffled.shape[0] * 0.7)]
        x_test = x_shuffled[int(x_shuffled.shape[0] * 0.7):]

        y_train = y_shuffled[:int(y_shuffled.shape[0] * 0.7)]
        y_test = y_shuffled[int(y_shuffled.shape[0] * 0.7):]

        clf = svm.SVC()
        clf.fit(x_train, y_train)

        print(clf.score(x_test, y_test))

        # get sleep data
        raster_sleep = self.get_sleep_raster()

        predicted = clf.predict(raster_sleep.T)
        plt.plot(moving_average(a=predicted,n=10000))
        plt.show()

    # non-stationarity during sleep and learning
    # ------------------------------------------------------------------------------------------------------------------

    def learning_non_stationarity(self, time_bin_size=60, plasticity_measure="peak_loc", spatial_resolution=3,
                                  nr_fits=15, use_abs=True):

        if plasticity_measure == "peak_loc":
            peak_shift_pre = self.pre.learning_place_field_peak_shift(all_cells=True, plotting=False,
                                                                      spatial_resolution=1)

            less_learning = np.argwhere(peak_shift_pre < 0.5*np.median(peak_shift_pre)).flatten()
            more_learning = np.argwhere(peak_shift_pre > 1.5*np.median(peak_shift_pre)).flatten()

            peak_shift_post = self.post.learning_place_field_peak_shift(all_cells=True, plotting=False,
                                                                        spatial_resolution=1)

            peak_shift_pos_more_learning = peak_shift_post[more_learning]
            peak_shift_pos_less_learning = peak_shift_post[less_learning]

            peak_shift_pos_more_learning_sorted = np.sort(peak_shift_pos_more_learning)
            peak_shift_pos_less_learning_sorted = np.sort(peak_shift_pos_less_learning)

            p_more_learning = 1. * np.arange(peak_shift_pos_more_learning.shape[0]) / \
                              (peak_shift_pos_more_learning.shape[0] - 1)
            p_less_learning = 1. * np.arange(peak_shift_pos_less_learning.shape[0]) / (
                        peak_shift_pos_less_learning.shape[0] - 1)

            plt.plot(peak_shift_pos_more_learning_sorted, p_more_learning, label="More learning", color="magenta")
            plt.plot(peak_shift_pos_less_learning_sorted, p_less_learning, label="Less learning", color="aquamarine")
            plt.title("Peak firing shift POST")
            plt.xlabel("Peak firing location shift / cm")
            plt.ylabel("CDF")
            plt.legend()
            plt.show()

        if plasticity_measure == "rate_map_corr":
            rate_map_corr_pre = self.pre.learning_rate_map_corr(spatial_resolution=spatial_resolution)

            more_learning = np.argwhere(rate_map_corr_pre < 0.5*np.median(rate_map_corr_pre)).flatten()
            less_learning = np.argwhere(rate_map_corr_pre > 1.5*np.median(rate_map_corr_pre)).flatten()

            rate_map_corr_post = self.post.learning_rate_map_corr(spatial_resolution=spatial_resolution)

            rate_map_corr_post_more_learning = rate_map_corr_post[more_learning]
            rate_map_corr_post_less_learning = rate_map_corr_post[less_learning]

            rate_map_corr_post_more_learning_sorted = np.sort(rate_map_corr_post_more_learning)
            rate_map_corr_post_less_learning_sorted = np.sort(rate_map_corr_post_less_learning)

            p_more_learning = 1. * np.arange(rate_map_corr_post_more_learning.shape[0]) / \
                              (rate_map_corr_post_more_learning.shape[0] - 1)
            p_less_learning = 1. * np.arange(rate_map_corr_post_less_learning.shape[0]) / (
                        rate_map_corr_post_less_learning.shape[0] - 1)

            plt.plot(rate_map_corr_post_more_learning_sorted, p_more_learning, label="More learning", color="magenta")
            plt.plot(rate_map_corr_post_less_learning_sorted, p_less_learning, label="Less learning", color="aquamarine")
            plt.title("Rate map correlation POST (first vs. last 5 trials)")
            plt.xlabel("Pearson R")
            plt.ylabel("CDF")
            plt.legend()
            plt.show()

        elif plasticity_measure == "mean_firing":

            mean_diff_pre = self.pre.learning_mean_firing_rate()

            if use_abs:
                mean_diff_pre_abs = np.abs(mean_diff_pre)
                less_learning = np.argwhere(mean_diff_pre_abs < 0.25).flatten()
                more_learning = np.argwhere(mean_diff_pre_abs > 0.25).flatten()
            else:
                less_learning = np.argwhere((-0.25 < mean_diff_pre) & (mean_diff_pre < 0.25)).flatten()
                more_learning = np.argwhere((0.25 < mean_diff_pre) | (mean_diff_pre < -0.25)).flatten()

            mean_diff_post = self.post.learning_mean_firing_rate()

            if use_abs:
                mean_diff_post_abs = np.abs(mean_diff_post)

                mean_diff_post_more_learning = mean_diff_post_abs[more_learning]
                mean_diff_post_less_learning = mean_diff_post_abs[less_learning]

            else:
                mean_diff_post_more_learning = mean_diff_post[more_learning]
                mean_diff_post_less_learning = mean_diff_post[less_learning]

            mean_diff_post_more_learning_sorted = np.sort(mean_diff_post_more_learning)
            mean_diff_post_less_learning_sorted = np.sort(mean_diff_post_less_learning)

            p_more_learning = 1. * np.arange(mean_diff_post_more_learning.shape[0]) / \
                              (mean_diff_post_more_learning.shape[0] - 1)
            p_less_learning = 1. * np.arange(mean_diff_post_less_learning.shape[0]) / (
                    mean_diff_post_less_learning.shape[0] - 1)

            plt.plot(mean_diff_post_more_learning_sorted, p_more_learning, label="More learning", color="magenta")
            plt.plot(mean_diff_post_less_learning_sorted, p_less_learning, label="Less learning", color="aquamarine")
            plt.title("Mean firing diff. POST")
            if use_abs:
                plt.xlabel("Relative ABS. mean firing diff")
            else:
                plt.xlabel("Relative mean firing diff")
            plt.ylabel("CDF")
            plt.legend()
            plt.show()


        raster = []
        first = 0
        for l_s in self.long_sleep:
            duration = l_s.get_duration_sec()
            r = l_s.get_raster()
            raster.append(r)
            first += duration

        raster = np.hstack(raster)

        scaler = int(time_bin_size / self.params.time_bin_size)

        raster = down_sample_array_sum(x=raster, chunk_size=scaler)

        print(raster.shape)

        #
        times = np.arange(0, raster.shape[1]) * time_bin_size

        raster_more_learning = raster[more_learning, :]
        raster_less_learning = raster[less_learning, :]

        new_ml = MlMethodsOnePopulation()
        alpha = 1950

        r2_more_learning = []
        r2_less_learning = []

        for fit in range(nr_fits):
            random_seed = np.random.randint(0,1000,1)

            ml = new_ml.ridge_time_bin_progress(x=raster_more_learning, y=times, new_time_bin_size=time_bin_size,
                                                alpha_fitting=False, plotting=False, alpha=alpha, random_seed=random_seed)

            ll = new_ml.ridge_time_bin_progress(x=raster_less_learning, y=times, new_time_bin_size=time_bin_size,
                                                alpha_fitting=False, plotting=False, alpha=alpha, random_seed=random_seed)

            r2_more_learning.append(ml)
            r2_less_learning.append(ll)

        r2_more_learning = np.array(r2_more_learning)
        r2_less_learning = np.array(r2_less_learning)

        c = "white"


        res = np.vstack((r2_more_learning, r2_less_learning)).T

        bplot = plt.boxplot(res, positions=[1, 2], patch_artist=True,
                            labels=["More learning", "Less learning"],
                            boxprops=dict(color=c),
                            capprops=dict(color=c),
                            whiskerprops=dict(color=c),
                            flierprops=dict(color=c, markeredgecolor=c),
                            medianprops=dict(color=c),
                            )
        colors = ["yellow", 'blue']
        for patch, color in zip(bplot['boxes'], colors):
            patch.set_facecolor(color)
        plt.title("R2 VALUES OF RIDGE REGRESSION")
        plt.ylabel("R2 (15 SPLITS)")
        plt.grid(color="grey", axis="y")
        plt.show()

    # PRE cofiring and drift during sleep
    # ------------------------------------------------------------------------------------------------------------------

    def awake_cofiring_sleep_drift(self, time_bin_size=60):
        raster_pre = self.pre.get_raster()
        # make binary
        raster_pre[raster_pre > 0] = 1
        # raster_pre = raster_pre[:,:100]

        def count_co_firing(x,y):
            a = np.logical_and(x,y)
            return np.count_nonzero(a)

        nr_events = pairwise_distances(X=raster_pre, metric=count_co_firing)

        raster = []
        first = 0
        for l_s in self.long_sleep:
            duration = l_s.get_duration_sec()
            r = l_s.get_raster()
            raster.append(r)
            first += duration

        raster = np.hstack(raster)

        scaler = int(time_bin_size / self.params.time_bin_size)

        raster = down_sample_array_sum(x=raster, chunk_size=scaler)

        print(raster.shape)

        #
        times = np.arange(0, raster.shape[1]) * time_bin_size
        new_ml = MlMethodsOnePopulation()

        weights = new_ml.ridge_time_bin_progress(x=raster, y=times, new_time_bin_size=time_bin_size,
                                                alpha_fitting=True, plotting=False, return_weights=True)

        # compute all differences in weights
        weight_diff = pdist(np.expand_dims(weights, 1))

        weight_diff_abs = np.abs(weight_diff)

        weight_diff_norm = (weight_diff - np.mean(weight_diff))/np.std(weight_diff)

        weight_diff_norm_abs = np.abs(weight_diff_norm)

        nr_events_awake = upper_tri_without_diag(nr_events)

        plt.scatter(nr_events_awake, weight_diff_abs)
        plt.xlabel("Co-firing events awake")
        plt.ylabel("Regression weight distance")
        # plt.title("R = "+str(pearsonr(nr_events_awake, weight_diff_abs)))
        plt.show()

    def mode_occurence_pre_sleep_post(self):

        m_sleep, occurrences_sleep = self.phmm_mode_occurrence(part_to_analyze="nrem", data_length=1)

        m_pre, _ = self.pre.phmm_mode_occurrence()
        m_post, occurrences_post = self.post.phmm_mode_occurrence()

        plt.scatter(m_pre, occurrences_post)
        plt.xlabel("m_pre")
        plt.ylabel("occurrences sleep")
        plt.title(pearsonr(m_pre, occurrences_sleep))
        plt.show()


"""#####################################################################################################################
#       EXPLORATION FAMILIAR, PRE AND POST LEARNING CHEESEBOARD TASK
#####################################################################################################################"""


class ExplFamPrePostCheeseboardExplFam:
    """Class to compare PRE and POST"""

    def __init__(self, exp_fam_1, pre, post, exp_fam_2, params, session_params=None):
        self.params = params
        self.session_params = session_params
        self.cell_type = self.params.cell_type
        self.session_name = session_params.session_name

        # initialize each phase
        self.pre = pre
        self.post = post
        self.exp_fam_1 = exp_fam_1
        self.exp_fam_2 = exp_fam_2

    def rate_map_stability(self, spatial_resolution=5, nr_of_splits=3, cells_to_use="all", plotting=True):

        # get subsets
        with open(self.params.pre_proc_dir + "cell_classification/" +
                  self.session_name +"_"+self.params.stable_cell_method + ".pickle", "rb") as f:
            class_dic = pickle.load(f)

        if cells_to_use == "stable":
            cell_ids = class_dic["stable_cell_ids"].flatten()
        elif cells_to_use == "decreasing":
            cell_ids = class_dic["decrease_cell_ids"].flatten()
        elif cells_to_use == "increasing":
            cell_ids = class_dic["increase_cell_ids"].flatten()
        elif cells_to_use == "all":
            print(" --> using all cells")

        # get rate maps and occ maps from pre-probe, pre, post, post-probe
        pre_prob_rate_maps, pre_prob_occ_maps = \
            self.exp_fam_1.get_rate_maps_occ_maps_temporal_splits(spatial_resolution=spatial_resolution,
                                                                  nr_of_splits=nr_of_splits)
        env_dim = self.exp_fam_1.get_env_dim()
        post_prob_rate_maps, post_prob_occ_maps = \
            self.exp_fam_2.get_rate_maps_occ_maps_temporal_splits(env_dim=env_dim,
                                                                  spatial_resolution=spatial_resolution,
                                                                  nr_of_splits=nr_of_splits)
        pre_rate_maps, pre_occ_maps = \
            self.pre.get_rate_maps_occ_maps_temporal_splits(env_dim=env_dim, spatial_resolution=spatial_resolution,
                                                            nr_of_splits=nr_of_splits)
        post_rate_maps, post_occ_maps = \
            self.post.get_rate_maps_occ_maps_temporal_splits(env_dim=env_dim, spatial_resolution=spatial_resolution,
                                                             nr_of_splits=nr_of_splits)

        all_occ_maps = pre_prob_occ_maps+ pre_occ_maps+post_occ_maps+ post_prob_occ_maps
        all_occ_maps_arr = np.array(all_occ_maps)

        all_rate_maps = pre_prob_rate_maps + pre_rate_maps + post_rate_maps + post_prob_rate_maps

        # check which bins were visited in all maps
        nr_non_zero = np.count_nonzero(all_occ_maps_arr, axis=0)

        # set all good bins to 1
        good_bins = np.zeros((all_occ_maps_arr.shape[1], all_occ_maps_arr.shape[2]))
        good_bins[nr_non_zero==all_occ_maps_arr.shape[0]] = 1
        good_bins_rep = np.repeat(good_bins[:, :, np.newaxis], pre_prob_rate_maps[0].shape[2], axis=2)

        # only select good bins for rate maps
        map_similarity = np.zeros((all_occ_maps_arr.shape[0], all_occ_maps_arr.shape[0]))

        for id_1, rate_map_comp_1 in enumerate(all_rate_maps):
            for id_2, rate_map_comp_2 in enumerate(all_rate_maps):
                # compute correlation --> cdist computes all pairs: we only need mean of diagonal
                if not cells_to_use == "all":
                    rate_map_comp_1_good = rate_map_comp_1[good_bins == 1]
                    rate_map_comp_1_subset = rate_map_comp_1_good[:,cell_ids]
                    rate_map_comp_2_good = rate_map_comp_2[good_bins == 1]
                    rate_map_comp_2_subset = rate_map_comp_2_good[:,cell_ids]
                    a = 1 - cdist(rate_map_comp_1_subset, rate_map_comp_2_subset,
                                  metric="correlation")
                else:
                    a = 1-cdist(rate_map_comp_1[good_bins==1], rate_map_comp_2[good_bins==1], metric="correlation")
                map_similarity[id_1, id_2] = np.mean(np.diag(a))

        if plotting:
            # generate x/y label entries
            labels = []
            phases = np.array(["Exp.fam_before", "learn-PRE", "POST", "Exp.fam_after"])
            for phase in phases:
                for subdiv in range(nr_of_splits):
                    labels.append(phase+"_"+str(subdiv))

            # plt.figure(figsize=(6,5))
            plt.imshow(map_similarity, vmin=0, vmax=1)
            plt.yticks(np.arange(map_similarity.shape[0]), labels)
            plt.xticks(np.arange(map_similarity.shape[0]), labels, rotation='vertical')
            plt.xlim(-0.5, map_similarity.shape[0]-0.5)
            plt.ylim(-0.5, map_similarity.shape[0]-0.5)
            # plt.ylim(0, map_similarity.shape[0])
            a = plt.colorbar()
            a.set_label("Mean population vector correlation")
            plt.show()

        else:
            return map_similarity

    def compare_firing_rate_distributions(self, alpha=0.01):
        fir_exp_fam = self.exp_fam_1.get_raster()
        fir_pre = self.pre.get_raster()
        fir_post = self.post.get_raster()

        p_less_pre_post = []
        p_greater_pre_post = []
        p_two_sided_pre_post = []
        p_less_fam_pre = []
        p_greater_fam_pre = []
        p_two_sided_fam_pre = []
        for cell_id, (cell_fir_exp_fam, cell_fir_bef, cell_fir_aft) in enumerate(zip(fir_exp_fam, fir_pre, fir_post)):
            p_less_pre_post.append(mannwhitneyu(x=cell_fir_aft, y=cell_fir_bef, alternative="less")[1])
            p_greater_pre_post.append(mannwhitneyu(x=cell_fir_aft, y=cell_fir_bef, alternative="greater")[1])
            p_two_sided_pre_post.append(mannwhitneyu(x=cell_fir_aft, y=cell_fir_bef)[1])
            p_less_fam_pre.append(mannwhitneyu(x=cell_fir_bef, y=cell_fir_exp_fam, alternative="less")[1])
            p_greater_fam_pre.append(mannwhitneyu(x=cell_fir_bef, y=cell_fir_exp_fam, alternative="greater")[1])
            p_two_sided_fam_pre.append(mannwhitneyu(x=cell_fir_bef, y=cell_fir_exp_fam)[1])

        plt.scatter(np.arange(fir_post.shape[0]), p_two_sided_pre_post, color="green", label="pre_post")
        plt.scatter(np.arange(fir_post.shape[0]), p_two_sided_fam_pre, color="red", label="fam_pre")
        plt.legend()
        plt.show()
        print("HERE")

    def plot_rasters_contexts(self, down_sample_expl=False, cells_to_use="stable"):

            with open(self.params.pre_proc_dir + "cell_classification/" +
                  self.session_name + "_" + self.params.stable_cell_method + ".pickle", "rb") as f:
                class_dic = pickle.load(f)

            if cells_to_use == "stable":
                cell_ids = class_dic["stable_cell_ids"].flatten()
            elif cells_to_use == "decreasing":
                cell_ids = class_dic["decrease_cell_ids"].flatten()
            elif cells_to_use == "increasing":
                cell_ids = class_dic["increase_cell_ids"].flatten()

            raster_exp_fam_1 = self.exp_fam_1.get_raster()
            raster_pre = self.pre.get_raster(trials_to_use="all")
            raster_post = self.post.get_raster(trials_to_use="all")
            raster_exp_fam_2 = self.exp_fam_2.get_raster()

            if down_sample_expl:
                # down sample exploration familiar (much more data)
                raster_exp_fam_1 = raster_exp_fam_1[:, ::2]
                raster_exp_fam_2 = raster_exp_fam_2[:, ::2]

            all_data = np.hstack((raster_exp_fam_1, raster_pre, raster_post, raster_exp_fam_2))
            all_data = all_data[cell_ids, :]

            plt.figure(figsize=(15,5))
            plt.imshow(all_data, interpolation='nearest', aspect='auto')
            plt.vlines(raster_exp_fam_1.shape[1], 0, all_data.shape[0]-1, color="r")
            plt.vlines(raster_exp_fam_1.shape[1]+raster_pre.shape[1], 0, all_data.shape[0] - 1, color="r")
            plt.vlines(raster_exp_fam_1.shape[1] + raster_pre.shape[1] + raster_post.shape[1], 0, all_data.shape[0] - 1, color="r")
            plt.title(cells_to_use+" (EXP_FAM, PRE, POST, EXP_FAM)")
            plt.show()

    def distinguish_contexts(self, nr_splits=10, cells_to_use="stable"):

            with open(self.params.pre_proc_dir + "cell_classification/" +
                  self.session_name + "_" + self.params.stable_cell_method + ".pickle", "rb") as f:
                class_dic = pickle.load(f)

            cell_ids_stable = class_dic["stable_cell_ids"].flatten()
            cell_ids_decrease = class_dic["decrease_cell_ids"].flatten()
            cell_ids_increase = class_dic["increase_cell_ids"].flatten()

            raster_exp_fam_1 = self.exp_fam_1.get_raster()
            raster_pre = self.pre.get_raster(trials_to_use="all")
            raster_post = self.post.get_raster(trials_to_use="all")
            raster_exp_fam_2 = self.exp_fam_2.get_raster()

            # down sample exploration familiar (much more data)
            raster_exp_fam_1 = raster_exp_fam_1[:, ::2]
            raster_exp_fam_2 = raster_exp_fam_2[:, ::2]

            if cells_to_use == "stable":
                raster_exp_fam_1 = raster_exp_fam_1[cell_ids_stable,:]
                raster_exp_fam_2 = raster_exp_fam_2[cell_ids_stable, :]
                raster_post = raster_post[cell_ids_stable, :]
                raster_pre = raster_pre[cell_ids_stable, :]
            elif cells_to_use == "decreasing":
                raster_exp_fam_1 = raster_exp_fam_1[cell_ids_decrease,:]
                raster_exp_fam_2 = raster_exp_fam_2[cell_ids_decrease, :]
                raster_post = raster_post[cell_ids_decrease, :]
                raster_pre = raster_pre[cell_ids_decrease, :]
            elif cells_to_use == "increasing":
                raster_exp_fam_1 = raster_exp_fam_1[cell_ids_increase,:]
                raster_exp_fam_2 = raster_exp_fam_2[cell_ids_increase, :]
                raster_post = raster_post[cell_ids_increasee, :]
                raster_pre = raster_pre[cell_ids_increase, :]

            contexts = [raster_exp_fam_1, raster_pre, raster_post, raster_exp_fam_2]


            context_ids = [0, 1, 2, 3]

            mean_acc = np.zeros((4, 4))
            mean_acc[:] = np.nan

            for combination in itertools.combinations(context_ids, r=2):
                context_a = contexts[combination[0]]
                context_b = contexts[combination[1]]

                X = np.hstack((context_a, context_b)).T
                y = np.zeros(X.shape[0])
                y[:context_a.shape[1]] = 1

                train_index_list = []
                test_index_list = []

                sss = StratifiedShuffleSplit(n_splits=nr_splits, test_size=0.3, random_state=0)
                for split_id, (train_index, test_index) in enumerate(sss.split(X, y)):
                    train_index_list.append(train_index)
                    test_index_list.append(test_index)

                train_test_indices = list(zip(train_index_list, test_index_list))

                with mp.Pool(nr_splits) as p:
                    # compute results for different splits in parallel
                    multi_arg = partial(self.svm_with_test_train_index, X=X, y=y)
                    mean_acc_splits = p.map(multi_arg, train_test_indices)

                mean_acc[combination[0], combination[1]] = np.mean(np.array(mean_acc_splits))

            plt.imshow(mean_acc)
            plt.xticks([0, 1, 2, 3], ["exp_fam_1", "pre", "post", "exp_fam_2"])
            plt.yticks([0, 1, 2, 3], ["exp_fam_1", "pre", "post", "exp_fam_2"])
            plt.colorbar()
            plt.show()

    def distinguish_contexts_subsets(self, nr_splits=2, nr_cells_in_subset=None, nr_subsets=10,
                                     cells_to_use="decreasing", save_fig=False, min_nr_spikes_per_pv=2):
        with open(self.params.pre_proc_dir + "cell_classification/" +
                  self.session_name + "_" + self.params.stable_cell_method + ".pickle", "rb") as f:
            class_dic = pickle.load(f)

        cell_ids_stable = class_dic["stable_cell_ids"].flatten()
        cell_ids_decrease = class_dic["decrease_cell_ids"].flatten()
        cell_ids_increase = class_dic["increase_cell_ids"].flatten()
        if nr_cells_in_subset is None:
            nr_cells_in_subset = np.min(np.array([cell_ids_stable.shape[0], cell_ids_decrease.shape[0],
                                                  cell_ids_increase.shape[0]]))

        raster_exp_fam_1 = self.exp_fam_1.get_raster()
        raster_pre = self.pre.get_raster(trials_to_use="all")
        raster_post = self.post.get_raster(trials_to_use="all")
        raster_exp_fam_2 = self.exp_fam_2.get_raster()

        # down sample exploration familiar (much more data)
        raster_exp_fam_1 = raster_exp_fam_1[:, ::2]
        raster_exp_fam_2 = raster_exp_fam_2[:, ::2]

        mean_acc = np.zeros((4, 4, nr_subsets))
        mean_acc[:] = np.nan

        for subset_id in range(nr_subsets):

            if cells_to_use == "stable":
                cell_ids_subset = np.random.choice(a=cell_ids_stable, size=nr_cells_in_subset, replace=False)
            elif cells_to_use == "decreasing":
                cell_ids_subset = np.random.choice(a=cell_ids_decrease, size=nr_cells_in_subset, replace=False)
            elif cells_to_use == "increasing":
                cell_ids_subset = np.random.choice(a=cell_ids_increase, size=nr_cells_in_subset, replace=False)

            raster_exp_fam_1_subset = raster_exp_fam_1[cell_ids_subset, :]
            raster_exp_fam_2_subset = raster_exp_fam_2[cell_ids_subset, :]
            raster_post_subset = raster_post[cell_ids_subset, :]
            raster_pre_subset = raster_pre[cell_ids_subset, :]

            contexts = [raster_exp_fam_1_subset, raster_pre_subset, raster_post_subset, raster_exp_fam_2_subset]

            # filter population vectors that contain less than 2 spikes
            contexts_filtered = []
            for context in contexts:
                contexts_filtered.append(context[:,np.count_nonzero(context, axis=0)> min_nr_spikes_per_pv])

            contexts = contexts_filtered

            context_ids = [0, 1, 2, 3]

            for combination in itertools.combinations(context_ids, r=2):
                context_a = contexts[combination[0]]
                context_b = contexts[combination[1]]

                X = np.hstack((context_a, context_b)).T
                y = np.zeros(X.shape[0])
                y[:context_a.shape[1]] = 1

                train_index_list = []
                test_index_list = []

                sss = StratifiedShuffleSplit(n_splits=nr_splits, test_size=0.3, random_state=0)
                for split_id, (train_index, test_index) in enumerate(sss.split(X, y)):
                    train_index_list.append(train_index)
                    test_index_list.append(test_index)

                train_test_indices = list(zip(train_index_list, test_index_list))

                with mp.Pool(nr_splits) as p:
                    # compute results for different splits in parallel
                    multi_arg = partial(self.svm_with_test_train_index, X=X, y=y)
                    mean_acc_splits = p.map(multi_arg, train_test_indices)

                mean_acc[combination[0], combination[1], subset_id] = np.mean(np.array(mean_acc_splits))

        mean_mean_acc = np.mean(mean_acc, axis=2)

        plt.imshow(mean_mean_acc)
        plt.xticks([0, 1, 2, 3], ["exp_fam_1", "pre", "post", "exp_fam_2"])
        plt.yticks([0, 1, 2, 3], ["exp_fam_1", "pre", "post", "exp_fam_2"])
        plt.title(cells_to_use)
        a = plt.colorbar()
        a.set_label("Mean accuracy")

        if save_fig:
            plt.savefig(self.params.pre_proc_dir+"/temp_data/"+"svm_dist_env_"+self.session_name+"_"+cells_to_use+".png")
            plt.close()
        else:
            plt.show()

    def svm_with_test_train_index(self, train_test_indices, X, y):

        train_index = train_test_indices[0]
        test_index = train_test_indices[1]

        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = y[train_index], y[test_index]

        clf = make_pipeline(StandardScaler(), svm.SVC(gamma='auto', kernel="linear"))
        clf.fit(X_train, y_train)

        return clf.score(X_test, y_test)


"""#####################################################################################################################
#       SLEEP BEFORE AND AFTER PRE
#####################################################################################################################"""


class SleepBeforeSleep:
    """Class for long sleep"""

    def __init__(self, sleep_before, sleep, params, session_params):
        self.params = params
        self.session_params = session_params
        self.session_name = session_params.session_name

        self.sleep_before = sleep_before
        self.sleep = sleep

    def compute_likelihoods(self, cells_to_use="all"):
        # self.sleep_before.decode_activity_using_pre_post(template_type="phmm", part_to_analyze="all_swr")

        # compute likelihoods from sleep
        pre_model = self.sleep.session_params.default_pre_phmm_model
        self.sleep.decode_phmm_one_model(template_type="phmm", template_file_name=pre_model,cells_to_use=cells_to_use,
                                                part_to_analyze="all_swr", return_results=False, plot_for_control=False)

        # compute likelihoods from sleep_before
        self.sleep_before.decode_phmm_one_model(template_type="phmm", template_file_name=pre_model,cells_to_use=cells_to_use,
                                                part_to_analyze="all_swr", return_results=False)

    def compute_likelihoods_subsets(self, plotting=True, save_fig=False, use_max=True, cells_to_use="stable", split_sleep=True):
        if cells_to_use == "stable":
            pre_model = self.sleep_before.session_params.default_pre_phmm_model_stable
        elif cells_to_use == "decreasing":
            pre_model = self.sleep_before.session_params.default_pre_phmm_model_dec
        # get likelihoods from sleep_before
        self.sleep_before.decode_phmm_one_model_cell_subset(template_file_name=pre_model,
                                                                                      part_to_analyze="all_swr",
                                                                                      return_results=False,
                                                                                   cells_to_use=cells_to_use)

        # get likelihoods from sleep_before
        self.sleep.decode_phmm_one_model_cell_subset(template_file_name=pre_model,
                                                                                      part_to_analyze="all_swr",
                                                                                      return_results=False,
                                                                     cells_to_use = cells_to_use)

    def compare_likelihoods(self, plotting=True, save_fig=False, use_max=True, cells_to_use="all", split_sleep=True):
        # get likelihoods from sleep_before
        pre_model = self.sleep_before.session_params.default_pre_phmm_model
        likelihood_sleep_before, _, _, _ = self.sleep_before.decode_phmm_one_model(template_file_name=pre_model,
                                                                                      part_to_analyze="all_swr",
                                                                                      return_results=True,
                                                                                   cells_to_use=cells_to_use)

        # get likelihoods from sleep_before
        likelihood_sleep, _, _, _ = self.sleep.decode_phmm_one_model(template_file_name=pre_model,
                                                                                      part_to_analyze="all_swr",
                                                                                      return_results=True,
                                                                     cells_to_use = cells_to_use)

        likelihood_sleep_before = np.vstack(likelihood_sleep_before)
        likelihood_sleep = np.vstack(likelihood_sleep)

        if use_max:
            likelihood_sleep_before = np.max(likelihood_sleep_before, axis=1)
            likelihood_sleep = np.max(likelihood_sleep, axis=1)
        else:
            likelihood_sleep_before = likelihood_sleep_before.flatten()
            likelihood_sleep = likelihood_sleep.flatten()

        if split_sleep:
            likelihood_sleep_1 = likelihood_sleep[:int(likelihood_sleep.shape[0]/2)]
            likelihood_sleep_2 = likelihood_sleep[int(likelihood_sleep.shape[0]/2):]

            print("Sleep before vs. sleep 1")
            print(mannwhitneyu(likelihood_sleep_before, likelihood_sleep_1))
            print("Sleep before vs. sleep 2")
            print(mannwhitneyu(likelihood_sleep_before, likelihood_sleep_2))

        else:
            print(mannwhitneyu(likelihood_sleep_before, likelihood_sleep))

        if plotting or save_fig:

            if split_sleep:
                p_sleep_before = 1. * np.arange(likelihood_sleep_before.shape[0]) / ( likelihood_sleep_before.shape[0] - 1)
                p_sleep_1 = 1. * np.arange(likelihood_sleep_1.shape[0]) / (likelihood_sleep_1.shape[0] - 1)
                p_sleep_2 = 1. * np.arange(likelihood_sleep_2.shape[0]) / (likelihood_sleep_2.shape[0] - 1)
                if save_fig:
                    plt.style.use('default')
                    plt.close()
                plt.plot(np.sort(likelihood_sleep_before), p_sleep_before, color="greenyellow", label="Sleep before")
                plt.plot(np.sort(likelihood_sleep_1), p_sleep_1, color="lightgreen", label="Sleep_1")
                plt.plot(np.sort(likelihood_sleep_2), p_sleep_2, color="limegreen", label="Sleep_2")
                plt.gca().set_xscale("log")
                if use_max:
                    plt.xlabel("Max. likelihood per PV")
                else:
                    plt.xlabel("Likelihood per PV")
                plt.ylabel("CDF")
                plt.legend()
                if save_fig:
                    plt.rcParams['svg.fonttype'] = 'none'
                    plt.savefig("decoding_sleep_before_sleep_example_" + cells_to_use + ".svg", transparent="True")
                    plt.close()
                else:
                    plt.show()
            else:
                p_sleep_before = 1. * np.arange(likelihood_sleep_before.shape[0]) / (likelihood_sleep_before.shape[0] - 1)
                p_sleep = 1. * np.arange(likelihood_sleep.shape[0]) / (likelihood_sleep.shape[0] - 1)
                if save_fig:
                    plt.style.use('default')
                    plt.close()
                plt.plot(np.sort(likelihood_sleep_before), p_sleep_before, color="greenyellow", label="Sleep before")
                plt.plot(np.sort(likelihood_sleep), p_sleep, color="limegreen", label="Sleep")
                plt.gca().set_xscale("log")
                if use_max:
                    plt.xlabel("Max. likelihood per PV")
                else:
                    plt.xlabel("Likelihood per PV")
                plt.ylabel("CDF")
                plt.legend()
                if save_fig:
                    plt.rcParams['svg.fonttype'] = 'none'
                    plt.savefig("decoding_sleep_before_sleep_example_"+cells_to_use+".svg", transparent="True")
                    plt.close()
                else:
                    plt.show()


        else:
            if split_sleep:
                return likelihood_sleep_before, likelihood_sleep_1, likelihood_sleep_2
            else:
                return likelihood_sleep_before, likelihood_sleep

    def compare_likelihoods_subsets(self, plotting=True, save_fig=False, use_max=True, cells_to_use="stable",
                                    split_sleep=True, z_score=False):
        if cells_to_use == "stable":
            pre_model = self.sleep_before.session_params.default_pre_phmm_model_stable
        elif cells_to_use == "decreasing":
            pre_model = self.sleep_before.session_params.default_pre_phmm_model_dec
        # get likelihoods from sleep_before
        likelihood_sleep_before, _, _, _ = self.sleep_before.decode_phmm_one_model_cell_subset(template_file_name=pre_model,
                                                                                      part_to_analyze="all_swr",
                                                                                      return_results=True,
                                                                                   cells_to_use=cells_to_use)

        # get likelihoods from sleep_before
        likelihood_sleep, _, _, _ = self.sleep.decode_phmm_one_model_cell_subset(template_file_name=pre_model,
                                                                                      part_to_analyze="all_swr",
                                                                                      return_results=True,
                                                                     cells_to_use = cells_to_use)

        likelihood_sleep_before = np.vstack(likelihood_sleep_before)
        likelihood_sleep = np.vstack(likelihood_sleep)

        if use_max:
            likelihood_sleep_before = np.max(likelihood_sleep_before, axis=1)
            likelihood_sleep = np.max(likelihood_sleep, axis=1)
        else:
            likelihood_sleep_before = likelihood_sleep_before.flatten()
            likelihood_sleep = likelihood_sleep.flatten()

        if split_sleep:
            likelihood_sleep_1 = likelihood_sleep[:int(likelihood_sleep.shape[0]/2)]
            likelihood_sleep_2 = likelihood_sleep[int(likelihood_sleep.shape[0]/2):]

            # check if data needs to be z-scored
            if z_score:
                # combine all data to z-score
                all_likeli = np.hstack((likelihood_sleep_before, likelihood_sleep_1, likelihood_sleep_2))
                all_likeli_z = zscore(all_likeli)
                likelihood_sleep_before = all_likeli_z[:likelihood_sleep_before.shape[0]]
                likelihood_sleep_1 = all_likeli_z[likelihood_sleep_before.shape[0]:(likelihood_sleep_before.shape[0]+
                                                                                    likelihood_sleep_1.shape[0])]
                likelihood_sleep_2 = all_likeli_z[(likelihood_sleep_before.shape[0]+likelihood_sleep_1.shape[0]):]

            print("Sleep before vs. sleep 1")
            print(mannwhitneyu(likelihood_sleep_before, likelihood_sleep_1))
            print("Sleep before vs. sleep 2")
            print(mannwhitneyu(likelihood_sleep_before, likelihood_sleep_2))

        else:
            if z_score:
                # combine all data to z-score
                all_likeli = np.hstack((likelihood_sleep_before, likelihood_sleep))
                all_likeli_z = zscore(all_likeli)
                likelihood_sleep_before = all_likeli_z[:likelihood_sleep_before.shape[0]]
                likelihood_sleep = all_likeli_z[likelihood_sleep_before.shape[0]:]
            print(mannwhitneyu(likelihood_sleep_before, likelihood_sleep))

        if plotting or save_fig:

            if split_sleep:
                p_sleep_before = 1. * np.arange(likelihood_sleep_before.shape[0]) / ( likelihood_sleep_before.shape[0] - 1)
                p_sleep_1 = 1. * np.arange(likelihood_sleep_1.shape[0]) / (likelihood_sleep_1.shape[0] - 1)
                p_sleep_2 = 1. * np.arange(likelihood_sleep_2.shape[0]) / (likelihood_sleep_2.shape[0] - 1)
                if save_fig:
                    plt.style.use('default')
                    plt.close()
                plt.plot(np.sort(likelihood_sleep_before), p_sleep_before, color="greenyellow", label="Sleep before")
                plt.plot(np.sort(likelihood_sleep_1), p_sleep_1, color="lightgreen", label="Sleep_1")
                plt.plot(np.sort(likelihood_sleep_2), p_sleep_2, color="limegreen", label="Sleep_2")
                plt.gca().set_xscale("log")
                if use_max:
                    plt.xlabel("Max. likelihood per PV")
                else:
                    plt.xlabel("Likelihood per PV")
                plt.ylabel("CDF")
                plt.legend()
                if save_fig:
                    plt.rcParams['svg.fonttype'] = 'none'
                    plt.savefig("decoding_sleep_before_sleep_example_" + cells_to_use + ".svg", transparent="True")
                    plt.close()
                else:
                    plt.show()
            else:
                p_sleep_before = 1. * np.arange(likelihood_sleep_before.shape[0]) / (likelihood_sleep_before.shape[0] - 1)
                p_sleep = 1. * np.arange(likelihood_sleep.shape[0]) / (likelihood_sleep.shape[0] - 1)
                if save_fig:
                    plt.style.use('default')
                    plt.close()
                plt.plot(np.sort(likelihood_sleep_before), p_sleep_before, color="greenyellow", label="Sleep before")
                plt.plot(np.sort(likelihood_sleep), p_sleep, color="limegreen", label="Sleep")
                plt.gca().set_xscale("log")
                if use_max:
                    plt.xlabel("Max. likelihood per PV")
                else:
                    plt.xlabel("Likelihood per PV")
                plt.ylabel("CDF")
                plt.legend()
                if save_fig:
                    plt.rcParams['svg.fonttype'] = 'none'
                    plt.savefig("decoding_sleep_before_sleep_example_"+cells_to_use+".svg", transparent="True")
                    plt.close()
                else:
                    plt.show()


        else:
            if split_sleep:
                return likelihood_sleep_before, likelihood_sleep_1, likelihood_sleep_2
            else:
                return likelihood_sleep_before, likelihood_sleep

    def compare_max_post_probabilities(self, plotting=True, save_fig=False, use_max=True, cells_to_use="all"):
        # get likelihoods from sleep_before
        pre_model = self.sleep_before.session_params.default_pre_phmm_model
        likelihood_sleep_before, _, _, _ = self.sleep_before.decode_phmm_one_model(template_file_name=pre_model,
                                                                                   part_to_analyze="all_swr",
                                                                                   return_results=True,
                                                                                   cells_to_use=cells_to_use)

        # get likelihoods from sleep_before
        likelihood_sleep, _, _, _ = self.sleep.decode_phmm_one_model(template_file_name=pre_model,
                                                                     part_to_analyze="all_swr",
                                                                     return_results=True,
                                                                     cells_to_use=cells_to_use)

        likelihood_sleep_before = np.vstack(likelihood_sleep_before)
        likelihood_sleep = np.vstack(likelihood_sleep)

        # compute posterior probabilities
        posterior_prob_sleep_before = likelihood_sleep_before / np.sum(likelihood_sleep_before, axis=1,
                                                                       keepdims=True)
        posterior_prob_sleep = likelihood_sleep / np.sum(likelihood_sleep, axis=1, keepdims=True)

        max_posterior_prob_sleep_before = np.max(posterior_prob_sleep_before, axis=1)
        max_posterior_prob_sleep = np.max(posterior_prob_sleep, axis=1)

        # split sleep into sleep 1 and sleep 2
        max_posterior_prob_sleep_1 = max_posterior_prob_sleep[:int(max_posterior_prob_sleep.shape[0] / 2)]
        max_posterior_prob_sleep_2 = max_posterior_prob_sleep[int(max_posterior_prob_sleep.shape[0] / 2):]

        print("Sleep before vs. sleep 1")
        print(mannwhitneyu(max_posterior_prob_sleep_before, max_posterior_prob_sleep_1))
        print("Sleep before vs. sleep 2")
        print(mannwhitneyu(max_posterior_prob_sleep_before, max_posterior_prob_sleep_2))

        if plotting or save_fig:

            p_sleep_before = 1. * np.arange(max_posterior_prob_sleep_before.shape[0]) / (
                        max_posterior_prob_sleep_before.shape[0] - 1)
            p_sleep_1 = 1. * np.arange(max_posterior_prob_sleep_1.shape[0]) / (
                        max_posterior_prob_sleep_1.shape[0] - 1)
            p_sleep_2 = 1. * np.arange(max_posterior_prob_sleep_2.shape[0]) / (
                        max_posterior_prob_sleep_2.shape[0] - 1)
            if save_fig:
                plt.style.use('default')
                plt.close()
            plt.plot(np.sort(max_posterior_prob_sleep_before), p_sleep_before, color="greenyellow",
                     label="Sleep before")
            plt.plot(np.sort(max_posterior_prob_sleep_1), p_sleep_1, color="lightgreen", label="Sleep_1")
            plt.plot(np.sort(max_posterior_prob_sleep_2), p_sleep_2, color="limegreen", label="Sleep_2")
            plt.gca().set_xscale("log")

            plt.xlabel("Max. post. probability per PV (z-scored)")
            plt.ylabel("CDF")
            plt.legend()
            if save_fig:
                plt.rcParams['svg.fonttype'] = 'none'
                plt.savefig("decoding_sleep_before_sleep_example_" + cells_to_use + ".svg", transparent="True")
                plt.close()
            else:
                plt.show()
        else:
            return max_posterior_prob_sleep_before, max_posterior_prob_sleep_1, max_posterior_prob_sleep_2

    def likelihood_difference_subsets(self, plotting=True, save_fig=False, use_max=True, cells_to_use="stable",
                                    split_sleep=True, z_score=False):
        if cells_to_use == "stable":
            pre_model = self.sleep_before.session_params.default_pre_phmm_model_stable
        elif cells_to_use == "decreasing":
            pre_model = self.sleep_before.session_params.default_pre_phmm_model_dec
        # get likelihoods from sleep_before
        likelihood_sleep_before, _, _, _ = self.sleep_before.decode_phmm_one_model_cell_subset(template_file_name=pre_model,
                                                                                      part_to_analyze="all_swr",
                                                                                      return_results=True,
                                                                                   cells_to_use=cells_to_use)

        # get likelihoods from sleep_before
        likelihood_sleep, _, _, _ = self.sleep.decode_phmm_one_model_cell_subset(template_file_name=pre_model,
                                                                                      part_to_analyze="all_swr",
                                                                                      return_results=True,
                                                                     cells_to_use = cells_to_use)

        likelihood_sleep_before = np.vstack(likelihood_sleep_before)
        likelihood_sleep = np.vstack(likelihood_sleep)

        if split_sleep:
            likelihood_sleep_1 = likelihood_sleep[:int(likelihood_sleep.shape[0]/2)]
            likelihood_sleep_2 = likelihood_sleep[int(likelihood_sleep.shape[0]/2):]

            diff_sleep_sleep_1 = np.mean(likelihood_sleep_before, axis=0) - np.mean(likelihood_sleep_1, axis=0)
            diff_sleep_sleep_2 = np.mean(likelihood_sleep_before, axis=0) - np.mean(likelihood_sleep_2, axis=0)

        else:
            # diff_sleep_sleep = np.mean(likelihood_sleep_before, axis=0) - np.mean(likelihood_sleep, axis=0)
            diff_sleep_sleep = np.mean(likelihood_sleep_before, axis=0)/np.mean(likelihood_sleep, axis=0)
        if plotting or save_fig:

            if split_sleep:
                p_sleep_before = 1. * np.arange(likelihood_sleep_before.shape[0]) / ( likelihood_sleep_before.shape[0] - 1)
                p_sleep_1 = 1. * np.arange(likelihood_sleep_1.shape[0]) / (likelihood_sleep_1.shape[0] - 1)
                p_sleep_2 = 1. * np.arange(likelihood_sleep_2.shape[0]) / (likelihood_sleep_2.shape[0] - 1)
                if save_fig:
                    plt.style.use('default')
                    plt.close()
                plt.plot(np.sort(likelihood_sleep_before), p_sleep_before, color="greenyellow", label="Sleep before")
                plt.plot(np.sort(likelihood_sleep_1), p_sleep_1, color="lightgreen", label="Sleep_1")
                plt.plot(np.sort(likelihood_sleep_2), p_sleep_2, color="limegreen", label="Sleep_2")
                plt.gca().set_xscale("log")
                if use_max:
                    plt.xlabel("Max. likelihood per PV")
                else:
                    plt.xlabel("Likelihood per PV")
                plt.ylabel("CDF")
                plt.legend()
                if save_fig:
                    plt.rcParams['svg.fonttype'] = 'none'
                    plt.savefig("decoding_sleep_before_sleep_example_" + cells_to_use + ".svg", transparent="True")
                    plt.close()
                else:
                    plt.show()
            else:
                p_sleep_before = 1. * np.arange(likelihood_sleep_before.shape[0]) / (likelihood_sleep_before.shape[0] - 1)
                p_sleep = 1. * np.arange(likelihood_sleep.shape[0]) / (likelihood_sleep.shape[0] - 1)
                if save_fig:
                    plt.style.use('default')
                    plt.close()
                plt.plot(np.sort(likelihood_sleep_before), p_sleep_before, color="greenyellow", label="Sleep before")
                plt.plot(np.sort(likelihood_sleep), p_sleep, color="limegreen", label="Sleep")
                plt.gca().set_xscale("log")
                if use_max:
                    plt.xlabel("Max. likelihood per PV")
                else:
                    plt.xlabel("Likelihood per PV")
                plt.ylabel("CDF")
                plt.legend()
                if save_fig:
                    plt.rcParams['svg.fonttype'] = 'none'
                    plt.savefig("decoding_sleep_before_sleep_example_"+cells_to_use+".svg", transparent="True")
                    plt.close()
                else:
                    plt.show()


        else:
            if split_sleep:
                return diff_sleep_sleep_1, diff_sleep_sleep_2
            else:
                return diff_sleep_sleep


"""#####################################################################################################################
#       SLEEP BEFORE, PRE AND SLEEP AFTER PRE
#####################################################################################################################"""


class SleepBeforePreSleep:
    """Class for long sleep"""

    def __init__(self, sleep_before, pre, sleep, params, session_params):
        self.params = params
        self.session_params = session_params
        self.session_name = session_params.session_name

        self.sleep_before = sleep_before
        self.sleep = sleep
        self.pre = pre
        self.sleep_before_sleep = SleepBeforeSleep(sleep_before=sleep_before, sleep=sleep, params=params,
                                                   session_params=session_params)

    def pre_play_learning_phmm_modes(self, trials_to_use_for_decoding=None, n_smoothing=500, cells_to_use="stable", plotting=False):
            
        # get likelihoods from awake decoding
        posterior_prob, seq = self.pre.learning_phmm_modes_activation(trials_to_use_for_decoding=trials_to_use_for_decoding,
                                                                 cells_to_use=cells_to_use)

        # get likelihood ratio sleep_before/sleep_after
        likeli_ratio_sleep = self.sleep_before_sleep.likelihood_difference_subsets(split_sleep=False, plotting=False,
                                                                                   cells_to_use=cells_to_use)

        # compute average posterior probabilities for first 20% and last 20%
        post_prob_first_20 = np.mean(posterior_prob[:int(posterior_prob.shape[0]*0.2),:], axis=0)
        post_prob_last_20 = np.mean(posterior_prob[int(posterior_prob.shape[0] * 0.8):, :], axis=0)

        pre_modulation = post_prob_first_20/post_prob_last_20

        if plotting:
            print(pearsonr(pre_modulation, likeli_ratio_sleep)[0])
            plt.scatter(likeli_ratio_sleep, pre_modulation)
            plt.ylabel("post_prob_first_20/post_prob_last_20")
            plt.xlabel("likeli_sleep_before/likeli_sleep_after")
            plt.yscale("symlog")
            plt.xscale("symlog")
            plt.title(cells_to_use)
            # plt.gca().set_aspect('equal', 'box')
            plt.show()
            #
            #
            # # select modes that are active during sleep before
            # post_prob_stronger_before = post_prob[:, likeli_ratio_sleep > 1]
            # post_prob_stronger_before_max = np.max(post_prob_stronger_before, axis=1)
            # plt.plot(moving_average(post_prob_stronger_before_max, n=500))
            # plt.xlabel("Time bin (PRE)")
            # plt.ylabel("Max post. prob")
            # plt.title(cells_to_use +" cells:\nModes that are more reactivated in sleep BEFORE")
            # plt.show()
            #
            # post_prob_stronger_before_mean = np.mean(post_prob_stronger_before, axis=1)
            # plt.plot(moving_average(post_prob_stronger_before_mean, n=n_smoothing))
            # plt.xlabel("Time bin (PRE)")
            # plt.ylabel("Mean post-prob")
            # plt.title(cells_to_use +" cells:\nModes that are more reactivated in sleep BEFORE")
            # plt.show()
            #
            # post_prob_stronger_before_z = zscore(post_prob[:, likeli_ratio_sleep > 1], axis=0)
            # # plt.plot(likeli_stronger_before_z)
            # # plt.show()
            #
            # post_prob_stronger_before_z_mean = np.mean(post_prob_stronger_before_z, axis=1)
            # plt.plot(moving_average(post_prob_stronger_before_z_mean, n=n_smoothing))
            # plt.xlabel("Time bin (PRE)")
            # plt.ylabel("Mean of z-scored post-prob")
            # plt.title(cells_to_use +" cells:\nModes that are more reactivated in sleep BEFORE")
            # plt.show()
            #
            # # select modes that are active during sleep after
            # post_prob_stronger_after = post_prob[:, likeli_ratio_sleep < 1]
            #
            # # plt.plot(likeli_stronger_after)
            # # plt.show()
            #
            # post_prob_stronger_after_z = zscore(post_prob[:, likeli_ratio_sleep < 1], axis=0)
            # # plt.plot(likeli_stronger_after_z)
            # # plt.show()
            #
            # post_prob_stronger_after_z_mean = np.mean(post_prob_stronger_after_z, axis=1)
            # plt.plot(moving_average(post_prob_stronger_after_z_mean, n=n_smoothing))
            # plt.xlabel("Time bin (PRE)")
            # plt.ylabel("Mean of z-scored post. probs")
            # plt.title(cells_to_use +" cells:\nModes that are more reactivated in sleep AFTER")
            # plt.show()
            #
            # post_prob_stronger_after_mean = np.mean(post_prob_stronger_after, axis=1)
            # plt.plot(moving_average(post_prob_stronger_after_mean, n=n_smoothing))
            # plt.xlabel("Time bin (PRE)")
            # plt.ylabel("Mean post-prob")
            # plt.title(cells_to_use +" cells:\nModes that are more reactivated in sleep AFTER")
            # plt.show()
            #
            # # select modes that are active during sleep after
            # post_prob_stronger_after = post_prob[:, likeli_ratio_sleep < 1]
            # post_prob_stronger_after_max = np.max(post_prob_stronger_after, axis=1)
            # plt.plot(moving_average(post_prob_stronger_after_max, n=500))
            # plt.xlabel("Time bin (PRE)")
            # plt.ylabel("Max post. prob")
            # plt.title(cells_to_use +" cells:\nModes that are more reactivated in sleep AFTER")
            # plt.show()

        else:
            return pre_modulation, likeli_ratio_sleep

    def plot_locations_preplay_replay_modes(self, cells_to_use="stable", which_modes="replay"):
        # get likelihood ratio sleep_before/sleep_after
        likeli_ratio_sleep = self.sleep_before_sleep.likelihood_difference_subsets(split_sleep=False, plotting=False,
                                                                                   cells_to_use=cells_to_use)

        if which_modes == "preplay":
            modes = np.argwhere(likeli_ratio_sleep > 1)
        elif which_modes == "replay":
            modes = np.argwhere(likeli_ratio_sleep < 1)
        for mode_id in modes:
            self.pre.plot_phmm_mode_spatial(cells_to_use=cells_to_use, mode_id=mode_id)


"""#####################################################################################################################
#   EXPLORATION NOVEL AND FAMILIAR
#####################################################################################################################"""


class PreProbPrePostPostProb:
    """Class to compare PRE and POST"""

    def __init__(self, pre_probe, pre, post, post_probe, params, session_params=None):
        self.params = params
        self.session_params = session_params
        self.cell_type = self.params.cell_type
        self.session_name = session_params.session_name

        # initialize each phase
        self.pre = pre
        self.post = post
        self.pre_probe = pre_probe
        self.post_probe = post_probe

    def rate_map_stability(self, spatial_resolution=5, nr_of_splits=3, cells_to_use="all", plotting=True):

        # get subsets
        with open(self.params.pre_proc_dir + "cell_classification/" +
                  self.session_name +"_"+self.params.stable_cell_method + ".pickle", "rb") as f:
            class_dic = pickle.load(f)

        if cells_to_use == "stable":
            cell_ids = class_dic["stable_cell_ids"].flatten()
        elif cells_to_use == "decreasing":
            cell_ids = class_dic["decrease_cell_ids"].flatten()
        elif cells_to_use == "increasing":
            cell_ids = class_dic["increase_cell_ids"].flatten()
        elif cells_to_use == "all":
            print(" --> using all cells")

        # get rate maps and occ maps from pre-probe, pre, post, post-probe
        pre_prob_rate_maps, pre_prob_occ_maps = \
            self.pre_probe.get_rate_maps_occ_maps_temporal_splits(spatial_resolution=spatial_resolution,
                                                                  nr_of_splits=nr_of_splits)
        env_dim = self.pre_probe.get_env_dim()
        post_prob_rate_maps, post_prob_occ_maps = \
            self.post_probe.get_rate_maps_occ_maps_temporal_splits(env_dim=env_dim,
                                                                  spatial_resolution=spatial_resolution,
                                                                  nr_of_splits=nr_of_splits)
        # exclude first trial of PRE (learning)
        pre_rate_maps, pre_occ_maps = \
            self.pre.get_rate_maps_occ_maps_temporal_splits(env_dim=env_dim, spatial_resolution=spatial_resolution,
                                                            nr_of_splits=nr_of_splits)
        post_rate_maps, post_occ_maps = \
            self.post.get_rate_maps_occ_maps_temporal_splits(env_dim=env_dim, spatial_resolution=spatial_resolution,
                                                             nr_of_splits=nr_of_splits)

        all_occ_maps = pre_prob_occ_maps+ pre_occ_maps+post_occ_maps+ post_prob_occ_maps
        all_occ_maps_arr = np.array(all_occ_maps)

        all_rate_maps = pre_prob_rate_maps + pre_rate_maps + post_rate_maps + post_prob_rate_maps

        # check which bins were visited in all maps
        nr_non_zero = np.count_nonzero(all_occ_maps_arr, axis=0)

        # set all good bins to 1
        good_bins = np.zeros((all_occ_maps_arr.shape[1], all_occ_maps_arr.shape[2]))
        good_bins[nr_non_zero==all_occ_maps_arr.shape[0]] = 1
        good_bins_rep = np.repeat(good_bins[:, :, np.newaxis], pre_prob_rate_maps[0].shape[2], axis=2)

        # only select good bins for rate maps
        map_similarity = np.zeros((all_occ_maps_arr.shape[0], all_occ_maps_arr.shape[0]))

        for id_1, rate_map_comp_1 in enumerate(all_rate_maps):
            for id_2, rate_map_comp_2 in enumerate(all_rate_maps):
                # compute correlation --> cdist computes all pairs: we only need mean of diagonal
                if not cells_to_use == "all":
                    rate_map_comp_1_good = rate_map_comp_1[good_bins == 1]
                    rate_map_comp_1_subset = rate_map_comp_1_good[:,cell_ids]
                    rate_map_comp_2_good = rate_map_comp_2[good_bins == 1]
                    rate_map_comp_2_subset = rate_map_comp_2_good[:,cell_ids]
                    a = 1 - cdist(rate_map_comp_1_subset, rate_map_comp_2_subset,
                                  metric="correlation")
                else:
                    a = 1-cdist(rate_map_comp_1[good_bins==1], rate_map_comp_2[good_bins==1], metric="correlation")
                map_similarity[id_1, id_2] = np.mean(np.diag(a))

        if plotting:
            # generate x/y label entries
            labels = []
            phases = np.array(["pre-probe", "learn-PRE", "POST", "post-probe"])
            for phase in phases:
                for subdiv in range(nr_of_splits):
                    labels.append(phase+"_"+str(subdiv))

            # plt.figure(figsize=(6,5))
            plt.imshow(map_similarity, vmin=0, vmax=1)
            plt.yticks(np.arange(map_similarity.shape[0]), labels)
            plt.xticks(np.arange(map_similarity.shape[0]), labels, rotation='vertical')
            plt.xlim(-0.5, map_similarity.shape[0]-0.5)
            plt.ylim(-0.5, map_similarity.shape[0]-0.5)
            # plt.ylim(0, map_similarity.shape[0])
            a = plt.colorbar()
            a.set_label("Mean population vector correlation")
            plt.show()

        else:
            return map_similarity


"""#####################################################################################################################
#   EXPLORATION NOVEL AND FAMILIAR
#####################################################################################################################"""


class ExplorationNovelFamiliar:
    """Class to compare PRE and POST"""

    def __init__(self, nov, fam, params):
        self.params = params

        self.nov = nov
        self.fam = fam

    def exp_fam_exp_novel(self):
        # --------------------------------------------------------------------------------------------------------------
        # compares activity of novel and familiar exploration --> e.g. through PCA
        #
        # parameters:   -
        # --------------------------------------------------------------------------------------------------------------

        X_f = self.fam.get_raster()
        X_n = self.nov.get_raster()

        ml_fam = MlMethodsOnePopulation(act_map=X_f, params=self.params)
        exp_var_fam = ml_fam.apply_pca()
        exp_var_fam_cumsum = np.cumsum(exp_var_fam)

        ml_nov = MlMethodsOnePopulation(act_map=X_n, params=self.params)
        exp_var_nov = ml_nov.apply_pca()
        exp_var_nov_cumsum = np.cumsum(exp_var_nov)

        plt.plot(exp_var_fam_cumsum, label="FAMILIAR")
        plt.plot(exp_var_nov_cumsum, label="NOVEL")
        plt.grid()
        plt.legend()
        plt.show()

    # Poisson HMM
    # ------------------------------------------------------------------------------------------------------------------

    def poisson_exploration_fam_nov(self, file_name_fam, file_name_nov):
        # --------------------------------------------------------------------------------------------------------------
        # loads poisson hmm model from exploration familiar and exploration novel
        #
        # parameters:   - file_name_fam, string: file that contains the model that was trained on exploration familiar
        #               - file_name_nov, string: file that contains the model that was trained on exploration novel
        # returns:      -
        # --------------------------------------------------------------------------------------------------------------
        print(" - EVALUATING POISSON HMM TRAINED ON SLEEP AND FIT TO AWAKE DATA")

        means_f, std_modes_f, mode_freq_f, env_f, trans_mat_f, state_trans_f, mode_lambda_f = \
            self.fam.fit_poisson_hmm(file_name=file_name_fam, plot_awake_fit=False)

        means_n, std_modes_n, mode_freq_n, env_n, trans_mat_n, state_trans_n, mode_lambda_n = \
            self.nov.fit_poisson_hmm(file_name=file_name_nov, plot_awake_fit=False)

        sim_across = np.zeros((mode_lambda_n.shape[0], mode_lambda_f.shape[0]))
        sim_within_n = np.zeros((mode_lambda_n.shape[0], mode_lambda_n.shape[0]))
        sim_within_f = np.zeros((mode_lambda_f.shape[0], mode_lambda_f.shape[0]))

        for i, lam_n in enumerate(mode_lambda_n):
            for j, lam_f in enumerate(mode_lambda_f):
                sim_across[i, j] = distance.cosine(lam_n, lam_f)

        plt.imshow(sim_across, interpolation='nearest', aspect='auto')
        plt.colorbar()
        plt.title("novel")
        plt.show()

        for i, lam_n in enumerate(mode_lambda_n):
            mode_lambda_wo = np.delete(mode_lambda_n, i, axis=0)
            for j, lam_f in enumerate(mode_lambda_wo):
                sim_within_n[i, j] = distance.cosine(lam_n, lam_f)

        for i, lam_n in enumerate(mode_lambda_f):
            mode_lambda_wo = np.delete(mode_lambda_f, i, axis=0)
            for j, lam_f in enumerate(mode_lambda_wo):
                sim_within_f[i, j] = distance.cosine(lam_n, lam_f)

        plt.hist(sim_across.flatten(), color="b")
        plt.hist(sim_within_n.flatten(), color="r", alpha=0.5)
        plt.hist(sim_within_f.flatten(), color="gray", alpha=0.5)
        plt.show()

        plt.imshow(mode_lambda_n.T, interpolation='nearest', aspect='auto')
        plt.title("novel")
        plt.show()

        plt.imshow(mode_lambda_f.T, interpolation='nearest', aspect='auto')
        plt.title("familiar")
        plt.show()

        exit()

        constrained_means = means[:, std < 35]
        fig, ax = plt.subplots()
        ax.scatter(means[0, :], means[1, :], c="grey", label="ALL MEANS")
        ax.scatter(constrained_means[0, :], constrained_means[1, :], c="red", label="SPATIALLY CONSTRAINED")
        ax.add_artist(env)
        ax.set_ylim(40, 220)
        ax.set_xlim(0, 175)
        ax.set_aspect("equal")
        plt.xlabel("X")
        plt.ylabel("Y")
        plt.legend()
        plt.show()

    def phmm_spatial_assignment(self, file_name_pop_1, file_name_pop_2):
        # --------------------------------------------------------------------------------------------------------------
        # analysis awake pHMM fits of two experiment phases (e.g. exploration familiar vs. exploration novel)
        #
        # args:         - file_name_pop_1, str: file name containing pHMM model for population 1
        #               - file_name_pop_2, str: file name containing pHMM model for population 2

        #
        # returns:      -
        # --------------------------------------------------------------------------------------------------------------
        means_f, std_f, mode_freq_f, env_f, transmat_f, _, _ = \
            self.fam.fit_poisson_hmm(file_name=file_name_pop_1, plot_awake_fit=False)

        means_n, std_n, mode_freq_n, env_n, transmat_n, _, _ = \
            self.nov.fit_poisson_hmm(file_name=file_name_pop_2, plot_awake_fit=False)

        plt.hist(std_f, color="r", label="FAM", bins=int(means_f.shape[1]/3))
        plt.hist(std_n, color="b", label="NOV", alpha=0.5, bins=int(means_n.shape[1]/3))
        plt.xlabel("STD. / CM")
        plt.ylabel("COUNT")
        plt.legend()
        plt.title("SPATIALLY CONSTRAINS OF MODES")
        plt.show()


"""#####################################################################################################################
#   EXPLORATION AND POST-SLEEP
#####################################################################################################################"""


class ExplorationAndSleep:

    def __init__(self, exploration, sleep, params):
        self.params = params

        self.sleep = sleep
        self.exp = exploration

    def poisson_train_sleep_fit_awake(self, file_name):
        # --------------------------------------------------------------------------------------------------------------
        # loads poisson hmm model that was trained on sleep and fits it to awake data
        #
        # parameters:   - file_name, string: file that contains the model that was trained on sleep data
        #
        # returns:      -
        # --------------------------------------------------------------------------------------------------------------
        print(" - EVALUATING POISSON HMM TRAINED ON SLEEP AND FIT TO AWAKE DATA")

        means, std, mode_freq, env = self.exp.fit_poisson_hmm(file_name=file_name, plot_awake_fit=False)

        constrained_means = means[:, std < 35]
        fig, ax = plt.subplots()
        ax.scatter(means[0, :], means[1, :], c="grey", label="ALL MEANS")
        ax.scatter(constrained_means[0, :], constrained_means[1, :], c="red", label="SPATIALLY CONSTRAINED")
        ax.add_artist(env)
        ax.set_ylim(40, 220)
        ax.set_xlim(0, 175)
        ax.set_aspect("equal")
        plt.xlabel("X")
        plt.ylabel("Y")
        plt.legend()
        plt.show()
        exit()

        raster_sleep = self.sleep.get_raster().T

        with open(self.params.pre_proc_dir+"ML/" + file_name, "rb") as file:
            model = pickle.load(file)

        state_sequence = model.predict(raster_sleep)

        std_state_sequence = std_modes[state_sequence]

        plt.plot(std_state_sequence[:800])
        plt.show()

        exit()

        state_seq_sh = state_sequence[:10000]

        means_to_plot = means[:, state_seq_sh]

        fig, ax = plt.subplots()
        ax.scatter(means_to_plot[0, :], means_to_plot[1, :], c="grey")

        import matplotlib.cm as cm
        colors = cm.cool(np.linspace(0, 1, state_seq_sh.shape[0] - 1))
        for i, c in zip(range(0, state_seq_sh.shape[0] - 1), colors):
            ax.plot(means_to_plot[0, i:i + 2], means_to_plot[1, i:i + 2], color=c)
        # plt.title(title)
        # ax.scatter(mds[:, 0], mds[:, 1], color="grey")
        ax.scatter(means_to_plot[0, 0], means_to_plot[1, 0], color="white", marker="x", label="start", zorder=200)
        ax.scatter(means_to_plot[0, -1], means_to_plot[1, -1], color="white", label="end", zorder=200)

        ax.add_artist(env)
        ax.set_ylim(40, 220)
        ax.set_xlim(0, 175)
        ax.set_aspect("equal")
        plt.xlabel("X")
        plt.ylabel("Y")
        plt.legend()
        plt.show()

        print(state_sequence.shape)

        exit()

    def poisson_train_awake_fit_sleep(self, poisson_model_file):
        # --------------------------------------------------------------------------------------------------------------
        # loads poisson hmm model that was trained on awake data and fits it to awake data --> to look at reactivations
        #
        # parameters:   - poisson_model_file, string: file that contains the model that was trained on awake data
        #
        # returns:      -
        # --------------------------------------------------------------------------------------------------------------

        seq, nr_modes = self.sleep.fit_poisson_hmm(file_name=poisson_model_file)
        X = self.sleep.get_raster()

        # order modes by occurance
        a, b = np.unique(seq, return_counts=True)

        occ_ind = np.flip(np.argsort(b))
        mode_dec_occ = a[occ_ind]
        occ = b[occ_ind]
        occ_norm = occ / np.max(b)
        means, std, mode_freq, env, transmat, _, _ = \
            self.exp.fit_poisson_hmm(file_name=poisson_model_file, plot_awake_fit=False)

        means_dec_occ = means[:, mode_dec_occ]
        std_dec_occ = std[mode_dec_occ]

        labels = np.round((occ * self.params.time_bin_size) / 60, 3).astype(str).tolist()
        occ_norm = occ_norm * 200
        fig, ax = plt.subplots()

        ax.scatter(means_dec_occ[0, 1:-1], means_dec_occ[1, 1:-1], c="red", s=occ_norm[1:-1], alpha=0.5)
        ax.scatter(means_dec_occ[0, 0], means_dec_occ[1, 0], c="red", s=occ_norm[0], alpha=0.5,
                   label=labels[0] + " min")
        ax.scatter(means_dec_occ[0, -10], means_dec_occ[1, -10], c="red", s=occ_norm[-10], alpha=0.5,
                   label=labels[-1] + " min")
        ax.add_artist(env)
        ax.set_ylim(40, 220)
        ax.set_xlim(0, 175)
        ax.set_aspect("equal")
        plt.legend()
        plt.title("DURATION AND LOCATION OF REACTIVATED MODE")
        plt.xlabel("X")
        plt.ylabel("Y")
        plt.show()

        window_size = 10

        bins_per_window = int(window_size / self.params.time_bin_size)

        nr_windows = int(X.shape[1] / bins_per_window)

        mode_prob = np.zeros((nr_modes, nr_windows))
        for i in range(nr_windows):
            a, b = np.unique(seq[i * bins_per_window:(i + 1) * bins_per_window], return_counts=True)
            mode_prob[a, i] = b / bins_per_window

        # adjust weight of mode that was never reactivated --> set to 1
        mode_prob[np.where(~mode_prob.any(axis=1))[0], :] = 1

        max_mode_prob = np.max(mode_prob, axis=1)
        mode_prob_norm = mode_prob / mode_prob.max(axis=1)[:, None]

        plt.imshow(mode_prob_norm, interpolation='nearest', aspect='auto')
        plt.title("MODE FREQUENCY - NORMALIZED - " + str(window_size) + "s WINDOW")
        plt.ylabel("MODE ID")
        plt.xlabel("WINDOW ID")
        a = plt.colorbar()
        a.set_label("MODE FREQUENCY - NORMALIZED")

        # compute weighted average
        windows = np.tile(np.arange(nr_windows).T, nr_modes).reshape((nr_modes, nr_windows))

        weighted_av = np.average(windows, axis=1, weights=mode_prob)

        a = (windows - weighted_av[:, None]) ** 2

        weighted_std = np.sqrt(np.average(a, axis=1, weights=mode_prob))

        plt.scatter(weighted_av, np.arange(nr_modes), c="r", s=1, label="WEIGHTED AVERAGE")
        plt.legend()
        plt.show()

        plt.scatter(weighted_av, weighted_std)
        plt.title("MODE OCCURENCE: WEIGHTED AVERAGE & WEIGHTED STD")
        plt.ylabel("WEIGHTED STD")
        plt.xlabel("WEIGHTED AVERAGE")
        plt.show()

        entro = []
        # compute entropy
        for i in mode_prob_norm:
            entro.append(entropy(i))

        entro = np.array(entro)

        plt.hist(entro)
        plt.title("ENTROPY")
        plt.xlabel("ENTROPY")
        plt.ylabel("COUNT")
        plt.show()

        # find mode to split into high and low entropy
        dist_mode = mode_of_dist(entro)[0]
        dist_mode = 3.8

        high_entropy_modes = np.squeeze(np.argwhere(entro > dist_mode), 1)
        low_entropy_modes = np.squeeze(np.argwhere(entro < dist_mode), 1)
        #
        # means, std_modes, mode_freq, env, trans_mat, state_sequence, mode_lambda = \
        #     self.exploration_fam.load_fit_poisson_hmm(file_name=poisson_model_file)
        #

        means, std, mode_freq, env, transmat, _, _ = \
            self.exploration_novel.load_fit_poisson_hmm(file_name=poisson_model_file, plot_awake_fit=False)

        r = pearsonr(entro, std)
        plt.scatter(entro, std)
        plt.ylabel("SPATIAL COVERAGE (STD) - BEHAVIOR")
        plt.xlabel("ENTROPY - SLEEP")
        plt.title(str(r))
        plt.show()

        plt.subplot(2, 1, 1)
        plt.hist(std[high_entropy_modes], density=True, color="red", label="HIGH ENTR.")
        plt.hist(std[low_entropy_modes], density=True, color="blue", alpha=0.5, label="LOW ENTR.")
        plt.ylabel("DENSITY")
        plt.xlabel("STD / cm")
        plt.legend()
        means_hi_ent = means[:, high_entropy_modes]

        # compute distance between means --> correlate with transition matrix

        means_lo_ent = means[:, low_entropy_modes]

        # compute distance between means --> correlate with transition matrix
        plt.subplot(2, 1, 2)

        dist_mat = np.zeros((means_hi_ent.shape[1], means_hi_ent.shape[1]))

        for i, mean_1 in enumerate(means_hi_ent.T):
            for j, mean_2 in enumerate(means_hi_ent.T):
                dist_mat[i, j] = np.linalg.norm(mean_1 - mean_2)
        plt.hist(upper_tri_without_diag(dist_mat).flatten(), density=True, color="red")

        dist_mat = np.zeros((means_lo_ent.shape[1], means_lo_ent.shape[1]))

        for i, mean_1 in enumerate(means_lo_ent.T):
            for j, mean_2 in enumerate(means_lo_ent.T):
                dist_mat[i, j] = np.linalg.norm(mean_1 - mean_2)

        plt.hist(upper_tri_without_diag(dist_mat).flatten(), density=True, alpha=0.5, color="blue")

        plt.ylabel("DENSITY")
        plt.xlabel("DISTANCE BETWEEN MEANS / cm")
        plt.show()

        high_ent_means = means[:, high_entropy_modes]
        low_ent_means = means[:, low_entropy_modes]
        fig, ax = plt.subplots()
        ax.scatter(means[0, :], means[1, :], c="grey", label="ALL MEANS")
        ax.scatter(high_ent_means[0, :], high_ent_means[1, :], c="red", label="HIGH ENT. MEANS")
        ax.scatter(low_ent_means[0, :], low_ent_means[1, :], c="blue", label="LOW ENT. MEANS")
        ax.add_artist(env)
        ax.set_ylim(40, 220)
        ax.set_xlim(0, 175)
        ax.set_aspect("equal")
        plt.xlabel("X")
        plt.ylabel("Y")
        plt.legend()
        plt.show()

        exit()

        for mode in high_entropy_modes:
            self.exploration_fam.phmm_single_mode_details(mode_id=mode, poisson_model_file=poisson_model_file)


"""#####################################################################################################################
#   COMPARING SLEEP AFTER NOVEL AND FAMILIAR EXPLORATION
#####################################################################################################################"""


class SleepNovelFamiliar:
    """Class to compare PRE and POST"""

    def __init__(self, nov, fam, params):
        self.params = params

        self.nov = nov
        self.fam = fam

    def poisson_reactivation_fam_nov(self, file_phmm_fam, file_phmm_nov):
        # --------------------------------------------------------------------------------------------------------------
        # loads poisson hmm model that was trained on awake data before and model that was trained on awake data after
        # and fits it to sleep data --> to look at reactivations
        #
        # parameters:   - file_name, string: file that contains the model that was trained on awake data
        #
        # returns:      -
        # --------------------------------------------------------------------------------------------------------------

        seq_f, nr_modes_f = self.fam.fit_poisson_hmm(file_name=file_phmm_fam)
        X_f = self.fam.get_raster()

        seq_n, nr_modes_n = self.nov.fit_poisson_hmm(file_name=file_phmm_nov)
        X_n = self.nov.get_raster()

        _, time_react_fam = np.unique(seq_f, return_counts=True)
        _, time_react_nov = np.unique(seq_n, return_counts=True)

        time_react_fam = time_react_fam * self.params.time_bin_size
        time_react_nov = time_react_nov * self.params.time_bin_size

        window_size = 10
        bins_per_window = int(window_size / self.params.time_bin_size)

        nr_windows = int(X_f.shape[1] / bins_per_window)

        mode_prob = np.zeros((nr_modes_f, nr_windows))
        for i in range(nr_windows):
            a, b = np.unique(seq_f[i * bins_per_window:(i + 1) * bins_per_window], return_counts=True)
            mode_prob[a, i] = b / bins_per_window

        # adjust weight of mode that was never reactivated --> set to 1
        mode_prob[np.where(~mode_prob.any(axis=1))[0], :] = 1

        max_mode_prob = np.max(mode_prob, axis=1)
        mode_prob_norm = mode_prob / mode_prob.max(axis=1)[:, None]

        # compute entropy
        entro_f = []
        # compute entropy
        for i in mode_prob_norm:
            entro_f.append(entropy(i))
        entro_f = np.array(entro_f)

        plt.hist(entro_f, density=True, bins=20, color="orange", label="FAMILIAR")

        plt.vlines(np.median(entro_f), 0, 0.5, colors="red", label="MEDIAN FAMILIAR")
        nr_windows = int(X_n.shape[1] / bins_per_window)

        mode_prob = np.zeros((nr_modes_n, nr_windows))
        for i in range(nr_windows):
            a, b = np.unique(seq_n[i * bins_per_window:(i + 1) * bins_per_window], return_counts=True)
            mode_prob[a, i] = b / bins_per_window

        # adjust weight of mode that was never reactivated --> set to 1
        mode_prob[np.where(~mode_prob.any(axis=1))[0], :] = 1

        mode_prob_norm = mode_prob / mode_prob.max(axis=1)[:, None]

        # compute entropy
        entro_n = []
        # compute entropy
        for i in mode_prob_norm:
            entro_n.append(entropy(i))
        entro_n = np.array(entro_n)

        plt.vlines(np.median(entro_n), 0, 0.5, colors="blue", label="MEDIAN NOVEL")

        plt.hist(entro_n, density=True, bins=20, alpha=0.5, label="NOVEL")
        plt.legend()
        plt.xlabel("ENTROPY")
        plt.ylabel("DENSITY")
        plt.title(str(mannwhitneyu(entro_f, entro_n)))
        plt.show()

"""#####################################################################################################################
#   ALL DATA
#####################################################################################################################"""


class AllData:
    """Class to compare PRE and POST"""
    def __init__(self, session_params, params):
        # --------------------------------------------------------------------------------------------------------------
        # creates SelectedData object
        #
        # args:     - session_name, str: session name, there needs to be a parameter file in parameter_files/ with the
        #             same name
        #           - experiment_phase, str: defines which phase of the experiment is supposed to be used
        #             (e.g. EXPLORATION_NOVEL, SLEEP_NOVEL)
        #           - cell_type, list of strings: which type of cell(s) to use (e.g. ["p1", "pe"]
        #           - pre_proc_dir, str: directory where pre-processed data is stored
        #
        # --------------------------------------------------------------------------------------------------------------

        # --------------------------------------------------------------------------------------------------------------
        # load parameter file
        # --------------------------------------------------------------------------------------------------------------
        self.session_params = session_params
        self.params = params

        # --------------------------------------------------------------------------------------------------------------
        # import parameters that describe the data
        # --------------------------------------------------------------------------------------------------------------

        self.data_dir = self.session_params.data_params_dictionary["data_dir"]
        self.cell_type = self.params.cell_type
        self.pre_proc_dir = self.params.pre_proc_dir
        self.session_name = self.session_params.session_name

    @staticmethod
    def get_cell_id(data_dir, session_name, cell_type):
        # --------------------------------------------------------------------------------------------------------------
        # returns cell IDs from .des file for the selected cell type
        #
        # args:         - data_dir, str: directory where data folders are stored (directory containig all mjc.. folders)
        #               - session_name, str: "e.g. mjc163_2_0104"
        #               - cell type, str: can be on of the following (updated: 16.11.2020)
        #
        #                   - u1: unidentified?
        #                   - p1: pyramidal cells of the HPC
        #                   - p2 - p3: pyramidal cells of the PFC
        #                   - b1: interneurons of HPC
        #                   - b2 - b3: interneurons of HPC
        #                   - pe: pyramidal cells MEC
        #
        # returns:      - cell_ids, list: list containing cell ids
        # --------------------------------------------------------------------------------------------------------------

        with open(data_dir + "/" + session_name + "/" + session_name[1:] + ".des") as f:
            des = f.read()
        des = des.splitlines()
        # offset by 2 entries
        cell_ids = [i + 2 for i in range(len(des)) if des[i] == cell_type]

        return cell_ids

    @staticmethod
    def assign_spikes_and_features(cell_ids, clu, res, fet, cell_spike_times_dic, cell_features_dic, cell_id_offset):
        # --------------------------------------------------------------------------------------------------------------
        # loads spike times and features for each cell and writes times (in 20kHz resolution) to dictionary
        #
        # args:     - cell_IDs, list: list containing cell ids of cells that are supposed to be used
        #           - clu, np.array: array containing entry <-> cell_id associations
        #           - res, np.array: spike times (at 20kHz resolution
        #
        # returns:  - data, dictionary:     data["cell" + cell_ID] --> for each cell one np.array with spike times in
        #                                   20kHz resolution
        #
        # --------------------------------------------------------------------------------------------------------------

        # go through all cell ids that were provided
        for cell_ID in cell_ids:

            # need to offset clu by number of cells found before

            # find all entries of the cell_ID in the clu list
            entries_cell = np.where(clu + cell_id_offset == cell_ID)

            # append entries from res file (data is shifted by -1 with respect to clu list)
            ind_res_file = entries_cell[0] - 1
            ind_fet_file = ind_res_file

            # select spikes using found indices
            cell_spikes = res[ind_res_file]

            # select features using found indices
            features = fet[ind_fet_file, :]

            # only write to dictionary if there is actually data
            if cell_spikes.shape[0] > 0:

                cell_spike_times_dic["cell" + str(cell_ID)] = cell_spikes
                cell_features_dic["cell" + str(cell_ID)] = features

            # if there is no more data for cell_ids --> return and move to next tetrode
            else:
                return

    @staticmethod
    def assign_spikes(cell_ids, clu, res, cell_spike_times_dic, cell_id_offset):
        # --------------------------------------------------------------------------------------------------------------
        # loads spike times and features for each cell and writes times (in 20kHz resolution) to dictionary
        #
        # args:     - cell_IDs, list: list containing cell ids of cells that are supposed to be used
        #           - clu, np.array: array containing entry <-> cell_id associations
        #           - res, np.array: spike times (at 20kHz resolution
        #
        # returns:  - data, dictionary:     data["cell" + cell_ID] --> for each cell one np.array with spike times in
        #                                   20kHz resolution
        #
        # --------------------------------------------------------------------------------------------------------------

        # go through all cell ids that were provided
        for cell_ID in cell_ids:

            # need to offset clu by number of cells found before

            # find all entries of the cell_ID in the clu list
            entries_cell = np.where(clu + cell_id_offset == cell_ID)

            # append entries from res file (data is shifted by -1 with respect to clu list)
            ind_res_file = entries_cell[0] - 1
            ind_fet_file = ind_res_file

            # select spikes using found indices
            cell_spikes = res[ind_res_file]

            # only write to dictionary if there is actually data
            if cell_spikes.shape[0] > 0:

                cell_spike_times_dic["cell" + str(cell_ID)] = cell_spikes

            # if there is no more data for cell_ids --> return and move to next tetrode
            else:
                return

    @staticmethod
    def assign_spikes_and_waveforms(cell_ids, clu, res, cell_spike_times_dic, cell_waveform_dic, spk, cell_id_offset):
        # --------------------------------------------------------------------------------------------------------------
        # loads spike times and features for each cell and writes times (in 20kHz resolution) to dictionary
        #
        # args:     - cell_IDs, list: list containing cell ids of cells that are supposed to be used
        #           - clu, np.array: array containing entry <-> cell_id associations
        #           - res, np.array: spike times (at 20kHz resolution
        #
        # returns:  - data, dictionary:     data["cell" + cell_ID] --> for each cell one np.array with spike times in
        #                                   20kHz resolution
        #
        # --------------------------------------------------------------------------------------------------------------

        # go through all cell ids that were provided
        for cell_ID in cell_ids:

            # need to offset clu by number of cells found before

            # find all entries of the cell_ID in the clu list
            entries_cell = np.where(clu + cell_id_offset == cell_ID)

            # append entries from res file (data is shifted by -1 with respect to clu list)
            ind_res_file = entries_cell[0] - 1

            # select spikes using found indices
            cell_spikes = res[ind_res_file]

            # wave forms
            waveforms = spk[ind_res_file, :, :]

            # only write to dictionary if there is actually data
            if cell_spikes.shape[0] > 0:

                cell_spike_times_dic["cell" + str(cell_ID)] = cell_spikes

                cell_waveform_dic["cell" + str(cell_ID)] = waveforms

            # if there is no more data for cell_ids --> return and move to next tetrode
            else:
                return

    def load_spikes_and_features(self):

        # check how many tetrodes there are
        with open(self.data_dir + "/" + self.session_name + "/" + self.session_name + ".par") as f:
            for i, line in enumerate(f):
                if i == 2:
                    nr_tetrodes = np.int(line.split(sep=" ")[0])
                if i > 2:
                    break

        print("Loading .res and .fet data ...")

        # get cell ids
        cell_ids_orig = self.get_cell_id(data_dir=self.data_dir, session_name=self.session_name, cell_type=self.cell_type)
        cell_ids = cell_ids_orig

        cell_spike_times_dic = {}
        cell_features_dic = {}
        cell_id_offset = 0

        # if we take not the first tetrode, we have to offset the cluster ids
        for tetrode in range(1, nr_tetrodes + 1):
            print(" - tetrode " + str(tetrode))
            # load cluster IDs (from .clu) and times of spikes (from .res)
            clu = read_integers(
                self.data_dir + "/" + self.session_name + "/" + self.session_name[1:] + ".clu." + str(tetrode))
            res = read_integers(
                self.data_dir + "/" + self.session_name + "/" + self.session_name[1:] + ".res." + str(tetrode))
            fet = read_arrays(
                self.data_dir + "/" + self.session_name + "/" + self.session_name[1:] + ".fet." + str(tetrode),
                skip_first_row=True)

            self.assign_spikes_and_features(clu=clu, res=res, fet=fet, cell_ids=cell_ids,
                                          cell_features_dic=cell_features_dic, cell_spike_times_dic=cell_spike_times_dic,
                                          cell_id_offset=cell_id_offset)
            # need to add all cells from tetrode
            cell_id_offset += max(clu) - 1
            # need to remove the matched cell ids from the cell_ids list
            cell_ids = cell_ids_orig[len(cell_spike_times_dic):]

        return cell_features_dic, cell_spike_times_dic

    def load_spikes(self):
        # check how many tetrodes there are
        with open(self.data_dir + "/" + self.session_name + "/" + self.session_name + ".par") as f:
            for i, line in enumerate(f):
                if i == 2:
                    nr_tetrodes = np.int(line.split(sep=" ")[0])
                if i > 2:
                    break

        print("Loading .res data ...")

        # get cell ids
        cell_ids_orig = self.get_cell_id(data_dir=self.data_dir, session_name=self.session_name, cell_type=self.cell_type)
        cell_ids = cell_ids_orig

        cell_spike_times_dic = {}
        cell_id_offset = 0

        # if we take not the first tetrode, we have to offset the cluster ids
        for tetrode in range(1, nr_tetrodes + 1):
            print(" - tetrode " + str(tetrode))
            # load cluster IDs (from .clu) and times of spikes (from .res)
            clu = read_integers(
                self.data_dir + "/" + self.session_name + "/" + self.session_name[1:] + ".clu." + str(tetrode))
            res = read_integers(
                self.data_dir + "/" + self.session_name + "/" + self.session_name[1:] + ".res." + str(tetrode))

            self.assign_spikes(clu=clu, res=res, cell_ids=cell_ids, cell_spike_times_dic=cell_spike_times_dic,
                                          cell_id_offset=cell_id_offset)
            # need to add all cells from tetrode
            cell_id_offset += max(clu) - 1
            # need to remove the matched cell ids from the cell_ids list
            cell_ids = cell_ids_orig[len(cell_spike_times_dic):]

        return cell_spike_times_dic

    def load_spikes_and_waveforms(self, nr_tetrodes=16):
        # get cell ids
        cell_ids_orig = self.get_cell_id(data_dir=self.data_dir, session_name=self.session_name,
                                         cell_type=self.cell_type)
        cell_ids = cell_ids_orig

        cell_spike_times_dic = {}
        cell_waveform_dic = {}
        cell_id_offset = 0

        for tetrode in range(1, nr_tetrodes + 1):
            print(" - tetrode " + str(tetrode))
            clu = read_integers(
                self.data_dir + "/" + self.session_name + "/" + self.session_name[1:] + ".clu." + str(tetrode))
            res = read_integers(
                self.data_dir + "/" + self.session_name + "/" + self.session_name[1:] + ".res." + str(tetrode))
            # load cluster IDs (from .clu) and times of spikes (from .res)
            spk = np.fromfile(
                self.data_dir + "/" + self.session_name + "/" + self.session_name[1:] + ".spk." + str(tetrode),
                dtype=np.int16)
            with open(self.data_dir + "/" + self.session_name + "/" + self.session_name[1:] + ".par." + str(
                    tetrode)) as f:
                for i, line in enumerate(f):
                    if i == 0:
                        nr_channels = np.int(line.split(sep=" ")[1])
                    if i == 4:
                        nr_tet = np.int(line.split(sep=" ")[0])
                    if i > 4:
                        break
            spk = spk.reshape([-1, nr_tet, nr_channels])

            self.assign_spikes_and_waveforms(cell_ids=cell_ids, res=res, clu=clu, spk=spk,
                                             cell_spike_times_dic=cell_spike_times_dic,
                                             cell_waveform_dic=cell_waveform_dic, cell_id_offset=cell_id_offset)
            # need to add all cells from tetrode
            cell_id_offset += max(clu) - 1
            # need to remove the matched cell ids from the cell_ids list
            cell_ids = cell_ids_orig[len(cell_spike_times_dic):]

        return cell_spike_times_dic, cell_waveform_dic

    def per_cell_drift(self, features, spike_times, last_spike):

        # compute mean/std for first 1/10 of data
        nr_chunks = 10
        chunk_start = 0
        chunk_end = (1 / nr_chunks) * last_spike

        # compute mean and std for the chunk_start
        mean_initial = np.mean(features[np.logical_and(chunk_start < spike_times,spike_times < chunk_end)], axis=0)
        std_initial = np.std(features[np.logical_and(chunk_start < spike_times,spike_times < chunk_end)], axis=0)

        chunk_start = 0.9 * last_spike
        chunk_end = last_spike

        # compute mean and std for the chunk_start
        mean_last = np.mean(features[np.logical_and(chunk_start < spike_times,spike_times < chunk_end)], axis=0)
        std_last = np.std(features[np.logical_and(chunk_start < spike_times,spike_times < chunk_end)], axis=0)

        z_score = (mean_last - mean_initial) / std_initial

        return z_score

    def assess_stability_subsets(self, plotting=True):

        nr_features_to_use = 10

        with open(self.params.pre_proc_dir + "cell_classification/" +
                  self.session_name + "_" + self.params.stable_cell_method + ".pickle",
                  "rb") as f:
            class_dic = pickle.load(f)

        stable = class_dic["stable_cell_ids"]
        dec = class_dic["decrease_cell_ids"]
        inc = class_dic["increase_cell_ids"]

        # load features and spike times
        cell_feature_dic, cell_spike_times_dic = self.load_spikes_and_features()

        # find last spike
        last_spike = 0
        for _, cell_fir in cell_spike_times_dic.items():
            if max(cell_fir) > 0:
                last_spike = max(cell_fir)

        # get keys from dictionary and get correct order
        cell_names = []
        for key in cell_spike_times_dic.keys():
            cell_names.append(key[4:])
        cell_names = np.array(cell_names).astype(int)
        cell_names.sort()

        stable_cell_z = np.zeros((stable.shape[0], nr_features_to_use))

        for i, cell_id in enumerate(stable):

            features = cell_feature_dic["cell"+str(cell_names[cell_id])]
            spike_times = cell_spike_times_dic["cell" + str(cell_names[cell_id])]

            z = self.per_cell_drift(features=features, spike_times=spike_times, last_spike=last_spike)

            stable_cell_z[i,:] = z[:nr_features_to_use]

        dec_cell_z = np.zeros((dec.shape[0], nr_features_to_use))

        for i, cell_id in enumerate(dec):

            features = cell_feature_dic["cell"+str(cell_names[cell_id])]
            spike_times = cell_spike_times_dic["cell" + str(cell_names[cell_id])]

            z = self.per_cell_drift(features=features, spike_times=spike_times, last_spike=last_spike)

            dec_cell_z[i,:] = z[:nr_features_to_use]

        inc_cell_z = np.zeros((inc.shape[0], nr_features_to_use))

        for i, cell_id in enumerate(inc):

            features = cell_feature_dic["cell"+str(cell_names[cell_id])]
            spike_times = cell_spike_times_dic["cell" + str(cell_names[cell_id])]

            z = self.per_cell_drift(features=features, spike_times=spike_times, last_spike=last_spike)

            inc_cell_z[i,:] = z[:nr_features_to_use]

        all_inc_z = inc_cell_z.flatten()
        all_dec_z = dec_cell_z.flatten()
        all_stable_z = stable_cell_z.flatten()

        if plotting:

            p_stable = 1. * np.arange(all_stable_z.shape[0]) / (all_stable_z.shape[0] - 1)
            p_inc = 1. * np.arange(all_inc_z.shape[0]) / (all_inc_z.shape[0] - 1)
            p_dec = 1. * np.arange(all_dec_z.shape[0]) / (all_dec_z.shape[0] - 1)

            plt.plot(np.sort(all_stable_z), p_stable, color="violet", label="stable")
            plt.plot(np.sort(all_dec_z), p_dec, color="turquoise", label="dec")
            plt.plot(np.sort(all_inc_z), p_inc, color="orange", label="inc")
            plt.ylabel("cdf")
            plt.xlabel("(mean_last_10%-mean_first_10%)/std_first_10% \n all features")
            plt.legend()
            plt.show()

            plt.plot(np.sort(np.abs(all_stable_z)), p_stable, color="violet", label="stable")
            plt.plot(np.sort(np.abs(all_dec_z)), p_dec, color="turquoise", label="dec")
            plt.plot(np.sort(np.abs(all_inc_z)), p_inc, color="orange", label="inc")
            plt.ylabel("cdf")
            plt.xlabel("abs((mean_last_10%-mean_first_10%)/std_first_10%) \n all features")
            plt.legend()
            plt.show()
        else:
            return all_stable_z, all_dec_z, all_inc_z

    def assess_stability(self, plotting=True, nr_features_to_use = 10):

        # load features and spike times
        cell_feature_dic, cell_spike_times_dic = self.load_spikes_and_features()

        # compute mean/std of reference distribution (first hour) for each cell/feature
        per_cell_mean = []
        per_cell_std = []
        for (_, features_cell), (_, spike_times) in zip(cell_feature_dic.items(), cell_spike_times_dic.items()):
            per_cell_mean.append(np.mean(features_cell[spike_times <  (20e3 * 60 * 60), :nr_features_to_use], axis=0))
            per_cell_std.append(np.std(features_cell[spike_times <  (20e3 * 60 * 60), :nr_features_to_use], axis=0))

        cell_res = []
        # go through all cells and compute distance
        for p_c_mean, p_c_std, (_, features_cell), (_, spike_times) in zip(per_cell_mean, per_cell_std,
                                                                           cell_feature_dic.items(),
                                                                           cell_spike_times_dic.items()):
            per_hour_res = []
            for i_subplot, hour in enumerate([2, 8, 16, 23]):
                cell_val = features_cell[np.logical_and(hour * 20e3 * 60 * 60 < spike_times,
                                                         spike_times < (hour + 1) * 20e3 * 60 * 60), :nr_features_to_use]
                per_hour_res.append((cell_val-p_c_mean)/p_c_std)
            cell_res.append(per_hour_res)

        cell_res_array = np.vstack(cell_res)
        # put all results for one time window together
        all_hours_res_all_cells = []
        for hour in range(len(cell_res[0])):
            per_hour_res_all_cells = []
            for res in cell_res:
                per_hour_res_all_cells.extend(res[hour].flatten())
            a = np.array(per_hour_res_all_cells)
            all_hours_res_all_cells.append(a[~np.isnan(a)])

        if plotting:

            c = "white"
            bplot = plt.boxplot(all_hours_res_all_cells, positions=[1, 2, 3, 4], patch_artist=True,
                                labels=["2h-3h", "8h-9h", "16h-17h", "23h-24h"],
                                boxprops=dict(color=c),
                                capprops=dict(color=c),
                                whiskerprops=dict(color=c),
                                flierprops=dict(color=c, markeredgecolor=c),
                                medianprops=dict(color=c), showfliers=False
                                )
            plt.ylabel("(feature-mean_feature_1h)/std_feature_1h)")
            plt.show()
        else:
            return all_hours_res_all_cells

    def plot_waveforms(self, nr_tetrodes=16, cells_to_plot="stable"):

        with open(self.params.pre_proc_dir + "cell_classification/" +
                  self.session_name + "_" + self.params.stable_cell_method + ".pickle",
                  "rb") as f:
            class_dic = pickle.load(f)

        if cells_to_plot == "stable":
            subset = class_dic["stable_cell_ids"]
        elif cells_to_plot == "dec":
            subset = class_dic["decrease_cell_ids"]
        elif cells_to_plot == "inc":
            subset = class_dic["increase_cell_ids"]

        cell_spike_times_dic, cell_waveform_dic = self.load_spikes_and_waveforms()

        # get keys from dictionary and get correct order
        cell_names = []
        for key in cell_spike_times_dic.keys():
            cell_names.append(key[4:])
        cell_names = np.array(cell_names).astype(int)
        cell_names.sort()

        for e, cell_id in enumerate(subset):

            waveforms = cell_waveform_dic["cell" + str(cell_names[cell_id])]

            example = waveforms[::100, :, 0]
            for i in range(example.shape[0]):
                plt.plot(example[i, :], color="lightgray")
            plt.ylabel("Amplitude")
            plt.xlabel("Time")
            plt.title("Waveform - cell "+str(cell_id))
            plt.show()

    def plot_waveforms_single_cells(self, cell_id, electrode, save_fig=False, y_min=None, y_max=None):

        scale_bar_y = 200
        scale_bar_x = 5

        cell_spike_times_dic, cell_waveform_dic = self.load_spikes_and_waveforms()

        # find last spike
        # last_spike = 0
        # for _, cell_fir in cell_spike_times_dic.items():
        #     if max(cell_fir) > 0:
        #         last_spike = max(cell_fir)
        #
        # dur_exp_h = last_spike * (1/20e3)/60/60

        # get keys from dictionary and get correct order
        cell_names = []
        for key in cell_spike_times_dic.keys():
            cell_names.append(key[4:])
        cell_names = np.array(cell_names).astype(int)
        cell_names.sort()

        waveforms = cell_waveform_dic["cell" + str(cell_names[cell_id])]
        spike_times = cell_spike_times_dic["cell" + str(cell_names[cell_id])]
        all_mean_waveforms = []
        for i_subplot, hour in enumerate([0, 8, 16, 23]):
            wf_within_hour = waveforms[
                np.logical_and(hour * 20e3 * 60 * 60 < spike_times, spike_times < (hour + 1) * 20e3 * 60 * 60)]
            mean = np.mean(wf_within_hour, axis=0)
            # check if one or several (up to 4 electrodes) of the tetrode
            if isinstance(electrode, list):
                mean_smoothed = []
                for el_id in electrode:
                    mean_smoothed.append(moving_average(a=mean[:, el_id], n=1))
            else:
                mean_smoothed = moving_average(a=mean[:, electrode], n=1)
            # example = wf_within_hour[::10, :, 0]
            # for i in range(example.shape[0]):
            #     plt.plot(example[i, :], color="lightgray")
            all_mean_waveforms.append(mean_smoothed)

        # determine min and max y values for plotting
        if y_min is None and y_max is None:
            y_min = np.min(np.hstack(all_mean_waveforms).flatten())
            y_max = np.max(np.hstack(all_mean_waveforms).flatten())
            y_min -= np.max([-1 * y_min, y_max]) * 0.05
            y_max += np.max([-1 * y_min, y_max]) * 0.05

        # plotting
        # --------------------------------------------------------------------------------------------------------------

        if save_fig:
            plt.style.use('default')
        plt.figure(figsize=(13, 3))
        cmap = matplotlib.cm.get_cmap('viridis')
        colors_to_plot = cmap(np.linspace(0, 1, 4))
        for i_subplot, (hour, mean_smoothed) in enumerate(zip([0, 8, 16, 23], all_mean_waveforms)):
            plt.subplot(1, 5, i_subplot + 1)
            if isinstance(mean_smoothed, list):
                for el_res in mean_smoothed:
                    plt.plot(el_res, color=colors_to_plot[i_subplot])
            else:
                plt.plot(mean_smoothed, color=colors_to_plot[i_subplot])
            if i_subplot == 0:
                plt.ylabel("Amplitude")
                plt.title("1st hour")
                plt.hlines(0.9 * y_min + scale_bar_y, 0, scale_bar_x, colors="k", linewidth=2)
                plt.vlines(scale_bar_x, 0.9 * y_min, 0.9 * y_min + scale_bar_y, colors="k", linewidth=2)
                plt.gca().get_xaxis().set_ticks([])
            else:
                plt.title(str(hour) + "th hour")
                plt.gca().get_yaxis().set_ticks([])
                plt.gca().get_xaxis().set_ticks([])
            plt.ylim(y_min, y_max)
        plt.subplot(1, 5, 5)
        for i, wf in enumerate(all_mean_waveforms):
            if isinstance(wf, list):
                for el_res in wf:
                    plt.plot(el_res, color=colors_to_plot[i])
            else:
                plt.plot(wf, color=colors_to_plot[i])
        plt.title("Superimposed")
        plt.ylim(y_min, y_max)
        plt.gca().get_yaxis().set_ticks([])
        plt.gca().get_xaxis().set_ticks([])
        if save_fig:
            plt.tight_layout()
            plt.rcParams['svg.fonttype'] = 'none'
            plt.savefig("waveform_example_cell_" + str(cell_id) + str(electrode)+".svg", transparent="True")
            plt.close()
        else:
            plt.show()

    def gamma_phase_preference_analysis(self):

        cell_spike_dic = self.load_spikes()
        print("HERE")

"""#####################################################################################################################
#   CLASS FOR MULTIPLE PHASES AND ONE POPULATION --> OBSOLETE
#####################################################################################################################"""

# TODO: double check and delete
class MultPhasesOnePopulation:
    """Base class for multiple phase analysis"""

    def __init__(self, data_obj, params):
        self.params = params
        self.cell_type = data_obj.get_cell_type()
        self.long_sleep = []
        self.learning_cheeseboard = []

        # initialize each phase
        for phase_id, phase in enumerate(data_obj.data_description):
            # check if extended data (incl. LFP)  needs to be loaded
            if params.data_to_use == "std":
                data_dic = data_obj.get_standard_data()[phase_id]
            elif params.data_to_use == "ext":
                data_dic = data_obj.get_extended_data()[phase_id]

            # generate a copy of params --> need to change the experiment_phase_id (do not want it to contain
            # list with all experiment_phase_ids, but only with the current one)
            params_copy = copy.deepcopy(params)
            params_copy.experiment_phase_id = [params_copy.experiment_phase_id[phase_id]]
            params_copy.experiment_phase = [params_copy.experiment_phase[phase_id]][0]

            if phase == "sleep_familiar":
                self.sleep_fam = Sleep([data_dic], data_obj.get_cell_type(), params_copy)
            elif phase == "sleep_novel":
                self.sleep_novel = Sleep([data_dic], data_obj.get_cell_type(), params_copy)
            elif phase == "exploration_familiar":
                self.exploration_fam = Exploration(data_dic, data_obj.get_cell_type(), params_copy)
            elif phase == "exploration_novel":
                self.exploration_novel = Exploration(data_dic, data_obj.get_cell_type(), params_copy)
            elif "sleep_long" in phase:
                self.long_sleep.append(Sleep([data_dic], data_obj.get_cell_type(), params_copy))
            elif "learning_cheeseboard" in phase:
                self.learning_cheeseboard.append(Cheeseboard([data_dic], data_obj.get_cell_type(), params_copy))

    def check_sleep(self):
        speed = np.zeros(0)
        for sleep in self.long_sleep:
            np.hstack(speed, sleep.get_speed())

    def distinguish_cells_sleep_1(self):
        raster = []
        for l_s in self.long_sleep:
            raster.append(l_s.get_raster())

        raster = np.hstack(raster)

        m = []

        for cell_id, cell_firing in enumerate(raster):
            smooth_firing = moving_average(a=cell_firing, n=1000)
            smooth_firing /= np.max(smooth_firing)
            # compute regression
            coef = np.polyfit(np.arange(smooth_firing.shape[0])*self.params.time_bin_size, smooth_firing, 1)
            m.append(coef[0])
            # if abs(coef[0]) < 5e-6:
            #     stable_cells_k_means.append(cell_id)
            # # if abs(coef[0]) < 2e-6:
            #     poly1d_fn = np.poly1d(coef)
            #     plt.plot(smooth_firing)
            #     plt.plot(np.arange(smooth_firing.shape[0]), poly1d_fn(np.arange(smooth_firing.shape[0])), '--w')
            #     plt.title(coef)
            #     plt.show()

        m = np.array(m)
        # find stable cells: < median --> not a good measure, but for the time being
        stable_cells = np.squeeze(np.argwhere(np.abs(m) < np.median(np.abs(m))))

        plt.hist(np.abs(np.array(m)), bins=80)
        plt.xlabel("M")
        plt.ylabel("DENSITY")
        print(np.median(np.array(np.abs(m))))
        plt.show()

        np.save(self.params.pre_proc_dir+"stable_cells_k_means/"+self.params.session_name+"_regression", np.array(stable_cells))

    def distinguish_cells_sleep_2(self):

        raster = []
        for l_s in self.long_sleep:
            raster.append(l_s.get_raster())

        raster = np.hstack(raster)

        raster_1 = raster[:, :int(raster.shape[1]/10)]
        raster_2 = raster[:, -int(raster.shape[1]/10):]

        p_val = []
        for cell_id, (cell_fir_bef, cell_fir_aft) in enumerate(zip(raster_1, raster_2)):
            p_val.append(stats.ks_2samp(cell_fir_aft, cell_fir_bef)[1])

        p_val = np.array(p_val)
        stable_cells = np.squeeze(np.argwhere(p_val > 0.01))
        plt.hist(p_val, bins=50)
        plt.show()

        np.save(self.params.pre_proc_dir+"stable_cells_k_means/"+self.params.session_name+"_ks_sleep", np.array(stable_cells))

    def distinguish_cells_sleep_3(self):

        raster = []
        for l_s in self.long_sleep:
            raster.append(l_s.get_raster())

        raster = np.hstack(raster)

        raster_1 = raster[:, :int(raster.shape[1]/10)]
        raster_2 = raster[:, -int(raster.shape[1]/10):]

        fir_diff = []
        for cell_id, (cell_fir_bef, cell_fir_aft) in enumerate(zip(raster_1, raster_2)):
            fir_diff.append(np.mean(cell_fir_aft)-np.mean(cell_fir_bef))

        fir_diff = np.abs(np.array(fir_diff))
        stable_cells = np.squeeze(np.argwhere(fir_diff < 0.025))
        plt.hist(fir_diff, bins=80)
        plt.show()

        np.save(self.params.pre_proc_dir+"stable_cells_k_means/"+self.params.session_name+"_sleep_fir_diff", np.array(stable_cells))

    def distinguish_cells_sleep_4(self, nr_clusters=3):

        # get rasters
        raster = []
        first = 0
        for l_s in self.long_sleep:
            duration = l_s.get_duration_sec()
            r, t = l_s.get_spike_binned_raster(return_estimated_times=True)
            raster.append(r)
            first += duration
        raster = np.hstack(raster)

        # down sample raster
        raster_ds = down_sample_array_mean(x=raster, chunk_size=5000)
        raster_ds = raster_ds / np.max(raster_ds, axis=1, keepdims=True)

        # smooth down sampled rasters
        raster_ds_smoothed = []

        for cell_arr in raster_ds:
            s = moving_average(a=cell_arr, n=50)
            s = s/np.max(s)
            raster_ds_smoothed.append(s)

        raster_ds_smoothed = np.array(raster_ds_smoothed)

        # k-means clustering
        kmeans = KMeans(n_clusters=nr_clusters).fit(X=raster_ds_smoothed)
        k_labels = kmeans.labels_

        stable = []
        increase = []
        decrease = []
        # find clusters with constant/decreasing/increasing firing rates
        # compare firing during first 20% with firing during last 20%
        for cl_id in np.unique(k_labels):
            cl_data = raster_ds_smoothed[k_labels == cl_id, :]
            diff = np.mean(cl_data[:, -int(0.2 * cl_data.shape[1]):].flatten()) - np.mean(
                cl_data[:, :int(0.2 * cl_data.shape[1])].flatten())
            if diff < -0.09:
                decrease.append(cl_id)
            elif diff > 0.09:
                increase.append(cl_id)
            else:
                stable.append(cl_id)

        decrease = np.array(decrease)
        increase = np.array(increase)
        stable = np.array(stable)

        print("STABLE CLUSTER IDS: "+str(stable))
        print("INCREASING CLUSTER IDS: " + str(increase))
        print("DECREASING CLUSTER IDS: " + str(decrease))

        stable_cell_ids = []
        decrease_cell_ids = []
        increase_cell_ids = []
        for stable_cluster_id in stable:
            stable_cell_ids.append(np.where(k_labels==stable_cluster_id)[0])
        for dec_cluster_id in decrease:
            decrease_cell_ids.append(np.where(k_labels==dec_cluster_id)[0])
        for inc_cluster_id in increase:
            increase_cell_ids.append(np.where(k_labels==inc_cluster_id)[0])

        stable_cell_ids = np.hstack(stable_cell_ids)
        decrease_cell_ids = np.hstack(decrease_cell_ids)
        increase_cell_ids = np.hstack(increase_cell_ids)

        # create dictionary with labels

        cell_class_dic = {
            "stable_cell_ids": stable_cell_ids,
            "decrease_cell_ids": decrease_cell_ids,
            "increase_cell_ids": increase_cell_ids
        }

        with open(self.params.pre_proc_dir+"cell_classification/"+self.params.session_name+"_k_means.pickle", "wb") as f:
            pickle.dump(cell_class_dic, f, pickle.HIGHEST_PROTOCOL)

        k_labels_sorted = k_labels.argsort()

        fig = plt.figure(figsize=(8,6))
        gs = fig.add_gridspec(6, 20)

        ax1 = fig.add_subplot(gs[:, 0])
        ax1.set_title("CLUSTER \n IDs")
        ax2 = fig.add_subplot(gs[:, 3:-2])
        ax3 = fig.add_subplot(gs[:, -1:])

        # plotting

        ax1.imshow(np.expand_dims(k_labels[k_labels_sorted], 1), aspect="auto", cmap="tab10")
        ax1.axis("off")
        rate_map = ax2.imshow(raster_ds_smoothed[k_labels_sorted,:], interpolation='nearest', aspect='auto')
        ax2.set_xlabel("BIN ID")
        ax2.set_ylabel("CELLS SORTED")
        a = plt.colorbar(rate_map, cax=ax3)
        a.set_label("#SPIKES / NORMALIZED")
        plt.show()


        exit()

        # new_ml = MlMethodsOnePopulation()
        # weights = new_ml.ridge_time_bin_progress(x=raster, y=times, new_time_bin_size=str(self.params.spikes_per_bin)+" SPIKE")
        # sort according to weights
        # weights_sorted = weights.argsort()

        # new labels to sort according to increase/decrease
        # decreasing labels: 30+
        # stable labels: 50+
        # increasing labels: 80+

        new_labels_1 = np.zeros(raster_ds_smoothed.shape[0])

        for cl_id in decrease:
            new_labels_1[k_labels == cl_id] = 30+cl_id

        for cl_id in increase:
            new_labels_1[k_labels == cl_id] = 80+cl_id

        for cl_id in stable:
            new_labels_1[k_labels == cl_id] = 50+cl_id

        # have smaller numbers again
        combined_k_labels_sorted = new_labels_1.argsort()

        a = np.unique(new_labels_1)
        a = np.sort(a)
        for c_id, n_id in zip(a, np.arange(a.shape[0])):
            new_labels_1[new_labels_1 == c_id] = n_id

        # new_labels = np.zeros(raster_ds_smoothed.shape[0])
        # for cl_id in decrease:
        #     new_labels[k_labels == cl_id] = 1
        #
        # for cl_id in increase:
        #     new_labels[k_labels == cl_id] = 2

        combined_k_labels_sorted = new_labels_1.argsort()


        fig = plt.figure(figsize=(8,6))
        gs = fig.add_gridspec(6, 20)

        ax1 = fig.add_subplot(gs[:, 0])
        ax1.set_title("CLUSTERS")
        ax2 = fig.add_subplot(gs[:, 3:-2])
        ax3 = fig.add_subplot(gs[:, -1:])

        # plotting

        ax1.imshow(np.expand_dims(new_labels_1[combined_k_labels_sorted], 1), aspect="auto", cmap="tab10")
        ax1.axis("off")
        rate_map = ax2.imshow(raster_ds_smoothed[combined_k_labels_sorted,:], interpolation='nearest', aspect='auto')
        ax2.set_xlabel("BIN ID")
        ax2.set_ylabel("CELLS SORTED")
        a = plt.colorbar(rate_map, cax=ax3)
        a.set_label("#SPIKES / NORMALIZED")
        ax2.set_title("COMBINED CLUSTERS")
        plt.savefig("ex1.pdf")
        plt.show()

    def distinguish_cells_awake(self):
        # check if cells have similar firing rates looking at before and after sleep sessions

        lcb_1_raster = self.learning_cheeseboard[0].get_raster()
        lcb_2_raster = self.learning_cheeseboard[1].get_raster()

        p_val = []
        for cell_id, (cell_fir_bef, cell_fir_aft) in enumerate(zip(lcb_1_raster, lcb_2_raster)):
            p_val.append(stats.mannwhitneyu(cell_fir_aft, cell_fir_bef)[1])

        p_val = np.array(p_val)
        stable_cells = np.squeeze(np.argwhere(p_val > 0.01))
        plt.hist(p_val, bins=50)
        plt.show()

        np.save(self.params.pre_proc_dir+"stable_cells_k_means/"+self.params.session_name+"_mann_whitney", np.array(stable_cells))

    """#################################################################################################################
    #  remapping cheeseboard
    #################################################################################################################"""
    def remapping_cheeseboard(self):
        lcb_1_raster = self.learning_cheeseboard[0].get_raster()
        lcb_2_raster = self.learning_cheeseboard[1].get_raster()

        # compute correlation matrices to check remapping
        lcb_1_corr_mat = np.corrcoef(lcb_1_raster)
        lcb_2_corr_mat = np.corrcoef(lcb_2_raster)

        plt.imshow(lcb_1_corr_mat)
        plt.show()
        plt.imshow(lcb_2_corr_mat)
        plt.show()

        print("HERE")

    def pre_post_firing_rates(self, spatial_resolution=10):
        lcb_1_raster = self.learning_cheeseboard[0].get_raster()
        lcb_2_raster = self.learning_cheeseboard[1].get_raster()

        if self.params.stable_cell_method == "k_means":
            # load only stable cells
            with open(self.params.pre_proc_dir + "cell_classification/" +
                      self.params.session_name + "_k_means.pickle", "rb") as f:
                class_dic = pickle.load(f)

            stable_cells = class_dic["stable_cell_ids"]
            inc_cells = class_dic["increase_cell_ids"]
            dec_cells = class_dic["decrease_cell_ids"]



        # compute mean firing rates
        mean_pre = np.mean(lcb_1_raster, axis=1)
        mean_post = np.mean(lcb_2_raster, axis=1)

        rate_maps_pre = self.learning_cheeseboard[0].get_rate_maps(spatial_resolution=spatial_resolution)
        rate_maps_post = self.learning_cheeseboard[1].get_rate_maps(spatial_resolution=spatial_resolution)
        occ_map_pre = self.learning_cheeseboard[0].get_occ_map(spatial_resolution=spatial_resolution)
        occ_map_post = self.learning_cheeseboard[1].get_occ_map(spatial_resolution=spatial_resolution)
        prob_occ_pre = occ_map_pre.flatten() / occ_map_pre.sum()
        prob_occ_post = occ_map_post.flatten() / occ_map_post.sum()

        sparsity_pre = []
        skaggs_info_pre = []

        for rate_map_pre in rate_maps_pre.T:
            # compute sparsity
            sparse_cell = np.round(np.mean(rate_map_pre.flatten()) ** 2 / np.mean(np.square(rate_map_pre.flatten())), 4)

            # compute Skagg's information criterium
            skaggs_info_cell = np.round(np.sum(np.nan_to_num(prob_occ_pre * (rate_map_pre / rate_map_pre.mean()).flatten()*
                                               np.log((rate_map_pre / rate_map_pre.mean()).flatten()))), 4)
            skaggs_info_sec = np.round(np.sum(np.nan_to_num(prob_occ_pre * (rate_map_pre / rate_map_pre.mean()).flatten()*
                                               np.log((rate_map_pre / rate_map_pre.mean()).flatten())))*rate_map_pre.mean(), 4)
            sparsity_pre.append(sparse_cell)
            skaggs_info_pre.append(skaggs_info_sec)

        skaggs_info_pre = np.array(skaggs_info_pre)
        sparsity_pre = np.array(sparsity_pre)

        sparsity_post = []
        skaggs_info_post = []

        for rate_map_post in rate_maps_post.T:
            # compute sparsity
            sparse_cell = np.round(np.mean(rate_map_post.flatten()) ** 2 / np.mean(np.square(rate_map_post.flatten())), 4)

            # compute Skagg's information criterium
            skaggs_info_cell = np.round(np.sum(np.nan_to_num(prob_occ_post * (rate_map_post / rate_map_post.mean()).flatten()*
                                               np.log((rate_map_post/ rate_map_post.mean()).flatten()))), 4)
            skaggs_info_sec = np.round(np.sum(np.nan_to_num(prob_occ_post * (rate_map_post / rate_map_post.mean()).flatten()*
                                               np.log((rate_map_post/ rate_map_post.mean()).flatten())))*rate_map_post.mean(), 4)
            sparsity_post.append(sparse_cell)
            skaggs_info_post.append(skaggs_info_sec)

        skaggs_info_post = np.array(skaggs_info_post)
        sparsity_post = np.array(sparsity_post)

        max_x = 0.65

        plt.legend()
        plt.close()
        plt.figure(figsize=(15,5))
        plt.subplot(1,3,1)
        plt.title("DECREASING")
        plt.ylabel("DENSITY")
        plt.hist(sparsity_pre[dec_cells], color="grey", label="PRE", density=True, edgecolor='blue')
        plt.hist(sparsity_post[dec_cells], color="#a0c4e4", label="POST", density=True, alpha=0.8)
        plt.legend()
        plt.xlabel("SPARSITY")
        plt.xlim(0, max_x)
        plt.subplot(1,3,2)
        plt.title("INCREASING")
        plt.hist(sparsity_pre[inc_cells], color="grey", label="PRE", density=True, alpha=0.8, edgecolor='red')
        plt.hist(sparsity_post[inc_cells], color="#f7959c", label="POST", density=True, alpha=0.8)
        plt.legend()
        plt.xlabel("SPARSITY")
        plt.xlim(0, max_x)
        plt.subplot(1,3,3)
        plt.title("STABLE")
        plt.hist(sparsity_pre[stable_cells], color="grey",label="PRE", density=True, alpha=0.7, edgecolor='yellow')
        plt.hist(sparsity_post[stable_cells], color="#ffdba1", label="POST", density=True, alpha=0.8)
        plt.legend()
        plt.xlim(0,max_x)
        plt.xlabel("SPARSITY")
        plt.show()


        max_x = 12

        plt.legend()
        plt.close()
        plt.figure(figsize=(15,5))
        plt.subplot(1,3,1)
        plt.title("DECREASING")
        plt.ylabel("DENSITY")
        plt.hist(skaggs_info_pre[dec_cells], color="grey", label="PRE", density=True, edgecolor='blue')
        plt.hist(skaggs_info_post[dec_cells], color="#a0c4e4", label="POST", density=True, alpha=0.8)
        plt.legend()
        plt.xlabel("SKAGGS INFO")
        # plt.xlim(0, max_x)
        plt.subplot(1,3,2)
        plt.title("INCREASING")
        plt.hist(skaggs_info_pre[inc_cells], color="grey", label="PRE", density=True, alpha=0.8, edgecolor='red')
        plt.hist(skaggs_info_post[inc_cells], color="#f7959c", label="POST", density=True, alpha=0.8)
        plt.legend()
        plt.xlabel("SKAGGS INFO")
        # plt.xlim(0, max_x)
        plt.subplot(1,3,3)
        plt.title("STABLE")
        plt.hist(skaggs_info_pre[stable_cells], color="grey",label="PRE", density=True, alpha=0.7, edgecolor='yellow')
        plt.hist(skaggs_info_post[stable_cells], color="#ffdba1", label="POST", density=True, alpha=0.8)
        plt.legend()
        # plt.xlim(0,max_x)
        plt.xlabel("SKAGGS INFO")
        plt.show()



        # for i in range(20):
        #     plt.imshow(rate_maps_pre[:, :, i]), plt.title(str(skaggs_info_pre[i])+"_"+str(sparsity_pre[i])), \
        #     plt.colorbar(), plt.show()


        env_dim_pre = self.learning_cheeseboard[0].get_env_dim()
        rate_maps_post = self.learning_cheeseboard[1].get_rate_maps(spatial_resolution= spatial_resolution, env_dim=env_dim_pre)
        prob_occ = occ_map_pre.flatten() / occ_map_pre.sum()

        remapping = []

        for pre, post in zip(rate_maps_pre.T, rate_maps_post.T):
            remapping.append(pearsonr(pre.flatten(), post.flatten())[0])

        remapping = np.array(remapping)
        plt.hist(remapping[stable_cells], color="#a0c4e4", label="DECREASING", density=True, edgecolor='blue')
        _,x1 = plt.gca().get_xlim()
        plt.hist(remapping[inc_cells], color="#f7959c", label="INCREASING", density=True, alpha=0.8, edgecolor='red')
        _, x2 = plt.gca().get_xlim()
        plt.hist(remapping[dec_cells], color="#ffdba1",label="STABLE", density=True, alpha=0.7, edgecolor='yellow')
        _, x3 = plt.gca().get_xlim()
        max_x = max(x1,x2,x3)
        plt.title("RATE MAP OVERLAP")
        plt.xlabel("PEARSON(PRE, POST)")
        plt.legend()
        plt.close()
        plt.subplot(3,1,1)        # remapping = []
        #
        # for pre, post in zip(rate_maps_pre.T, rate_maps_post.T):
        #     remapping.append(pearsonr(pre.flatten(), post.flatten())[0])
        #
        # remapping = np.array(remapping)
        plt.hist(remapping[stable_cells], color="#a0c4e4", label="DECREASING", density=True, edgecolor='blue', bins=20)
        plt.title("REMAPPING")
        plt.legend()
        plt.xlim(0, max_x)
        plt.subplot(3,1,2)
        plt.hist(remapping[inc_cells], color="#f7959c", label="INCREASING", density=True, alpha=0.8, edgecolor='red', bins=20)
        plt.legend()
        plt.xlim(0, max_x)
        plt.ylabel("DENSITY")
        plt.subplot(3,1,3)
        plt.hist(remapping[dec_cells], color="#ffdba1",label="STABLE", density=True, alpha=0.7, edgecolor='yellow', bins=20)
        plt.legend()
        plt.xlim(0,max_x)
        plt.xlabel("PEARSON(PRE,POST)")

        plt.show()


        plt.hist(sparsity_pre[stable_cells], color="#a0c4e4", label="DECREASING", density=True, edgecolor='blue')
        _,x1 = plt.gca().get_xlim()
        plt.hist(sparsity_pre[inc_cells], color="#f7959c", label="INCREASING", density=True, alpha=0.8, edgecolor='red')
        _, x2 = plt.gca().get_xlim()
        plt.hist(sparsity_pre[dec_cells], color="#ffdba1",label="STABLE", density=True, alpha=0.7, edgecolor='yellow')
        _, x3 = plt.gca().get_xlim()
        max_x = max(x1,x2,x3)
        plt.title("SPARSITY")
        plt.legend()
        plt.close()
        plt.subplot(3,1,1)
        plt.hist(sparsity_pre[stable_cells], color="#a0c4e4", label="DECREASING", density=True, edgecolor='blue', bins=20)
        plt.title("SPATIAL INFORMATION - PRE")
        plt.legend()
        plt.xlim(0, max_x)
        plt.subplot(3,1,2)
        plt.hist(sparsity_pre[inc_cells], color="#f7959c", label="INCREASING", density=True, alpha=0.8, edgecolor='red', bins=20)
        plt.legend()
        plt.xlim(0, max_x)
        plt.ylabel("DENSITY")
        plt.subplot(3,1,3)
        plt.hist(sparsity_pre[dec_cells], color="#ffdba1",label="STABLE", density=True, alpha=0.7, edgecolor='yellow', bins=20)
        plt.legend()
        plt.xlim(0,max_x)
        plt.xlabel("SPARSITY")

        plt.show()

        plt.hist(sparsity_post[stable_cells], color="#a0c4e4", label="DECREASING", density=True, edgecolor='blue')
        _,x1 = plt.gca().get_xlim()
        plt.hist(sparsity_post[inc_cells], color="#f7959c", label="INCREASING", density=True, alpha=0.8, edgecolor='red')
        _,x2 = plt.gca().get_xlim()
        plt.hist(sparsity_post[dec_cells], color="#ffdba1",label="STABLE", density=True, alpha=0.7, edgecolor='yellow')
        _,x3 = plt.gca().get_xlim()
        max_x = max(x1,x2,x3)
        plt.title("SPARSITY")
        plt.legend()
        plt.close()
        plt.subplot(3,1,1)
        plt.hist(sparsity_post[stable_cells], color="#a0c4e4", label="DECREASING", density=True, edgecolor='blue', bins=20)
        plt.title("SPATIAL INFORMATION - POST")
        plt.legend()
        plt.xlim(0, max_x)
        plt.subplot(3,1,2)
        plt.hist(sparsity_post[inc_cells], color="#f7959c", label="INCREASING", density=True, alpha=0.8, edgecolor='red', bins=20)
        plt.legend()
        plt.xlim(0, max_x)
        plt.ylabel("DENSITY")
        plt.subplot(3,1,3)
        plt.hist(sparsity_post[dec_cells], color="#ffdba1",label="STABLE", density=True, alpha=0.7, edgecolor='yellow', bins=20)
        plt.legend()
        plt.xlim(0, max_x)
        plt.xlabel("SPARSITY")
        plt.show()
        plt.hist(skaggs_info_pre[stable_cells], color="#a0c4e4", label="DECREASING", density=True, edgecolor='blue')
        _,x1 = plt.gca().get_xlim()
        plt.hist(skaggs_info_pre[inc_cells], color="#f7959c", label="INCREASING", density=True, alpha=0.8, edgecolor='red')
        _,x2 = plt.gca().get_xlim()
        plt.hist(skaggs_info_pre[dec_cells], color="#ffdba1",label="STABLE", density=True, alpha=0.7, edgecolor='yellow')
        _,x3 = plt.gca().get_xlim()
        max_x = max(x1,x2,x3)
        plt.legend()
        plt.close()
        plt.title("SKAGGS INFORMATION")
        plt.legend()
        plt.close()
        plt.subplot(3,1,1)
        plt.hist(skaggs_info_pre[stable_cells], color="#a0c4e4", label="DECREASING", density=True, edgecolor='blue', bins=20)
        plt.title("SPATIAL INFORMATION - PRE")
        plt.legend()
        plt.xlim(0, max_x)
        plt.subplot(3,1,2)
        plt.hist(skaggs_info_pre[inc_cells], color="#f7959c", label="INCREASING", density=True, alpha=0.8, edgecolor='red', bins=20)
        plt.legend()
        plt.xlim(0, max_x)
        plt.ylabel("DENSITY")
        plt.subplot(3,1,3)
        plt.hist(skaggs_info_pre[dec_cells], color="#ffdba1",label="STABLE", density=True, alpha=0.7, edgecolor='yellow', bins=20)
        plt.legend()
        plt.xlim(0, max_x)
        plt.xlabel("SKAGGS INFO")
        plt.show()

        plt.hist(skaggs_info_post[stable_cells], color="#a0c4e4", label="DECREASING", density=True, edgecolor='blue')
        _, x1 = plt.gca().get_xlim()
        plt.hist(skaggs_info_post[inc_cells], color="#f7959c", label="INCREASING", density=True, alpha=0.8,
                 edgecolor='red')
        _, x2 = plt.gca().get_xlim()
        plt.hist(skaggs_info_post[dec_cells], color="#ffdba1", label="STABLE", density=True, alpha=0.7,
                 edgecolor='yellow')
        _, x3 = plt.gca().get_xlim()
        max_x = max(x1, x2, x3)
        plt.legend()
        plt.close()
        plt.title("SKAGGS INFORMATION")
        plt.legend()
        plt.close()
        plt.subplot(3, 1, 1)
        plt.hist(skaggs_info_post[stable_cells], color="#a0c4e4", label="DECREASING", density=True, edgecolor='blue', bins=20)
        plt.title("SPATIAL INFORMATION - POST")
        plt.legend()
        plt.xlim(0, max_x)
        plt.subplot(3, 1, 2)
        plt.hist(skaggs_info_post[inc_cells], color="#f7959c", label="INCREASING", density=True, alpha=0.8,
                 edgecolor='red', bins=20)
        plt.legend()
        plt.xlim(0, max_x)
        plt.ylabel("DENSITY")
        plt.subplot(3, 1, 3)
        plt.hist(skaggs_info_post[dec_cells], color="#ffdba1", label="STABLE", density=True, alpha=0.7,
                 edgecolor='yellow', bins=20)
        plt.legend()
        plt.xlim(0, max_x)
        plt.xlabel("SKAGGS INFO")
        plt.show()

        exit()






        # compute mean firing rates
        mean_pre = np.mean(lcb_1_raster, axis=1)
        mean_post = np.mean(lcb_2_raster, axis=1)

        plt.scatter(mean_pre[stable_cells], mean_post[stable_cells], c="#ffdba1", label="STABLE")
        plt.scatter(mean_pre[inc_cells], mean_post[inc_cells], c="#f7959c", label="INCREASING")
        plt.scatter(mean_pre[dec_cells], mean_post[dec_cells], c="#a0c4e4", label="DECREASING")
        plt.xlabel("MEAN FIRING RATE PRE")
        plt.ylabel("MEAN FIRING RATE POST")
        plt.legend()
        plt.show()

        plt.hist((mean_post[dec_cells]- mean_pre[dec_cells])/(mean_post[dec_cells]+ mean_pre[dec_cells]),
                 color="#a0c4e4", label="DECREASING", density=True, edgecolor='blue')
        plt.hist((mean_post[inc_cells] - mean_pre[inc_cells])/(mean_post[inc_cells] + mean_pre[inc_cells]),
                 color="#f7959c", label="INCREASING", density=True, alpha=0.8, edgecolor='red')
        plt.hist((mean_post[stable_cells] - mean_pre[stable_cells])/(mean_post[stable_cells] +
                                                                     mean_pre[stable_cells]), color="#ffdba1",
                 label="STABLE", density=True, alpha=0.7, edgecolor='yellow')

        matplotlib.rcParams['mathtext.default'] = 'regular'

        matplotlib.rcParams['text.usetex'] = True
        plt.xlabel(r'$\frac{\overline{firing\_rate}_{POST} - '
                   r'\overline{firing\_rate}_{PRE}}{\overline{firing\_rate}_{POST} + \overline{firing\_rate}_{PRE}}$')
        plt.ylabel("DENSITY")
        plt.ylim(0,1.4)
        plt.legend(loc="upper left")
        plt.show()


    """#################################################################################################################
    #  novel vs. familiar
    #################################################################################################################"""

    def exp_fam_exp_novel(self):
        # --------------------------------------------------------------------------------------------------------------
        # compares activity of novel and familiar exploration --> e.g. through PCA
        #
        # parameters:   -
        # --------------------------------------------------------------------------------------------------------------

        X_f = self.exploration_fam.get_raster()
        X_n = self.exploration_novel.get_raster()

        ml_fam = MlMethodsOnePopulation(act_map=X_f, params=self.params)
        exp_var_fam = ml_fam.apply_pca()
        exp_var_fam_cumsum = np.cumsum(exp_var_fam)

        ml_nov = MlMethodsOnePopulation(act_map=X_n, params=self.params)
        exp_var_nov = ml_nov.apply_pca()
        exp_var_nov_cumsum = np.cumsum(exp_var_nov)

        plt.plot(exp_var_fam_cumsum, label="FAMILIAR")
        plt.plot(exp_var_nov_cumsum, label="NOVEL")
        plt.grid()
        plt.legend()
        plt.show()

    """#################################################################################################################
    #   treeHMM
    #################################################################################################################"""

    def generate_tree_hmm_input_data(self, file_name):
        # --------------------------------------------------------------------------------------------------------------
        # generates binary input data for TreeHMM model
        #
        # args:
        #
        # --------------------------------------------------------------------------------------------------------------
        exp_raster = self.exploration_fam.get_raster()
        sleep_raster = self.sleep_fam.get_raster()
        # self.sleep_fam.save_transformed_spike_times(self.params.pre_proc_dir+"TreeHMM/input_files")
        # exit()
        len_sleep = sleep_raster.shape[1]
        combined = np.hstack((sleep_raster, exp_raster))
        plt.imshow(combined,  interpolation='nearest', aspect='auto')
        plt.vlines(len_sleep, 0, sleep_raster.shape[0]-1, colors="r", linewidth=1)
        plt.title("SLEEP | EXPLORATION")
        plt.show()

        np.save(self.params.pre_proc_dir+"TreeHMM/input_files/"+file_name, combined)

        print("SEPARATOR FOR DATA AT: "+str(len_sleep))

    def tree_hmm_mode_spatial_estimate_from_data(self, file_name):
        # --------------------------------------------------------------------------------------------------------------
        # function to investigate spatial content of modes using data --> uses P to assign population vectors to modes
        # and uses these population vectors to compute mean firing rate and correlation structure
        #
        # args: - file_name, string: input file name (output of TreeHMM)
        # --------------------------------------------------------------------------------------------------------------
        data_base = pickle.load(open(file_name, "rb"), encoding='latin1')
        sleep_raster = self.sleep_fam.get_transformed_raster()
        pfs_matrix = self.exploration_fam.place_field_similarity()
        # remove diagonal
        pfs_matrix = pfs_matrix[~np.eye(pfs_matrix.shape[0], dtype=bool)].reshape(pfs_matrix.shape[0], -1)
        len_sleep = sleep_raster.shape[1]

        # get probability of each mode for each population vector
        P = data_base["P"]
        nr_modes = P.shape[1]

        mode_array = []
        # find most probable mode for each time bin
        for time_bin in P:
            mode_array.append(np.argmax(time_bin))

        mode_array = np.array(mode_array)

        all_modes_mean = []
        all_modes_corr = []

        unique, counts = np.unique(mode_array, return_counts=True)

        # collect all population vectors from one mode and compute mean firing and correlations
        for mode_id in range(nr_modes):
            pop_vec_mode = sleep_raster[:, mode_array == mode_id]
            all_modes_mean.append(np.mean(pop_vec_mode, axis=1))
            # compute correlation matrice from samples
            A = np.nan_to_num(np.corrcoef(pop_vec_mode))
            # remove diagonal
            A = A[~np.eye(A.shape[0], dtype=bool)].reshape(A.shape[0], -1)
            plt.imshow(A, interpolation='nearest', aspect='auto')
            plt.colorbar()
            plt.show()

            all_modes_corr.append(A)

        all_modes_corr = np.array(all_modes_corr)

        pfs_corr = []
        # correlate correlation matrix from each mode with place field similarity
        for corr_mode in all_modes_corr:
            pfs_corr.append(pearsonr(corr_mode.flatten(), pfs_matrix.flatten()))

        print(pfs_corr)

        exit()



        all_modes_mean = np.array(all_modes_mean)
        all_modes_mean = np.nan_to_num(all_modes_mean.T)

        plt.imshow(all_modes_mean,  interpolation='nearest', aspect='auto')
        plt.colorbar()
        plt.show()

        rate_maps_orig = np.array(self.exploration_fam.get_rate_maps())
        rate_maps = np.reshape(rate_maps_orig, (rate_maps_orig.shape[0], rate_maps_orig.shape[1]*rate_maps_orig.shape[2]))

        res = np.zeros((all_modes_mean.shape[1], rate_maps.shape[1]))

        # compute correlation between mean prob. vectors for each mode and all spatially binned population vectors
        for mode_id, mode in enumerate(all_modes_mean.T):
            for spat_bin_id, spat_bin in enumerate(rate_maps.T):
                res[mode_id, spat_bin_id] = pearsonr(mode, spat_bin)[0]
                # res[mode_id, spat_bin_id] = 1-distance.cosine(mode, spat_bin)

        plt.imshow(res,  interpolation='nearest', aspect='auto')
        plt.xlabel("SPATIAL BINS")
        plt.ylabel("MODE ID")
        a = plt.colorbar()
        a.set_label("PEARSON CORRELATION")
        plt.title("CORRELATION: MEAN FIRING POP. VEC. EXPLORATION \n - CELL FIRING PROB. VEC. PER MODE")
        plt.show()

    def tree_hmm_mode_spatial_from_params(self, file_name):
        # --------------------------------------------------------------------------------------------------------------
        # function to investigate spatial content of modes using m and J for each mode (Ising model)
        #
        # args: - file_name, string: input file name (output of TreeHMM)
        # --------------------------------------------------------------------------------------------------------------
        data_base = pickle.load(open(file_name, "rb"), encoding='latin1')

        all_modes_m = []
        all_modes_corr = []
        # get high firing cells from mean firing prob from each mode using "params"
        for mode_id in range(len(data_base["params"])):
            # get mean firing rates from modes
            mean_fir_rates = data_base["params"][mode_id]["m"]
            all_modes_m.append(mean_fir_rates)
            corr_struct = data_base["params"][mode_id]["J"]
            all_modes_corr.append(corr_struct)

        all_modes_m = np.squeeze(np.array(all_modes_m), axis=2).T

        rate_maps_orig = np.array(self.exploration_fam.get_rate_maps())

        rate_maps = np.reshape(rate_maps_orig, (rate_maps_orig.shape[0], rate_maps_orig.shape[1]*rate_maps_orig.shape[2]))

        res = np.zeros((all_modes_m.shape[1], rate_maps.shape[1]))

        # compute correlation between mean prob. vectors for each mode and all spatially binned population vectors
        for mode_id, mode in enumerate(all_modes_m.T):
            for spat_bin_id, spat_bin in enumerate(rate_maps.T):
                # only consider significant correlations
                if pearsonr(mode, spat_bin)[1] < 0.05:
                    res[mode_id, spat_bin_id] = pearsonr(mode, spat_bin)[0]
                else:
                    res[mode_id, spat_bin_id] = 0
                # res[mode_id, spat_bin_id] = 1-distance.cosine(mode, spat_bin)

        # compute sparsity from shuffled data
        # idea: take all correlation values from all modes --> sample from it and compute sparsity
        all_corr_values = res.flatten()
        nr_draws = 10000
        sparsity_null = []
        for i in range(nr_draws):
            sample = np.random.choice(all_corr_values, res.shape[1])
            sparsity_null.append(compute_sparsity(sample))

        spar_null_95_perc = np.percentile(sparsity_null,95)
        plt.hist(sparsity_null)
        plt.vlines(spar_null_95_perc, 0, 250, colors="r")
        plt.show()

        res = np.nan_to_num(res)

        sparsity_per_mode = []
        spat_sel_modes = []
        # check sparsity of each row as a measure of being spatially constrained
        for mode_map in res:
            map_reshaped = np.reshape(mode_map, (rate_maps_orig.shape[1], rate_maps_orig.shape[2]))
            sparsity_per_mode.append(compute_sparsity(mode_map))
            # if sparsity is greater than of the shuffled data --> spatially selective
            if compute_sparsity(mode_map) > spar_null_95_perc:
                spat_sel_modes.append(mode_map)

            # plt.imshow(map_reshaped, interpolation='nearest', aspect='auto')
            # plt.title(compute_sparsity(mode_map))
            # plt.xlabel("SPATIAL BINS")
            # plt.ylabel("MODE ID")
            # a = plt.colorbar()
            # a.set_label("PEARSON CORRELATION")
            # # plt.title("CORRELATION: MEAN FIRING POP. VEC. EXPLORATION \n - CELL FIRING PROB. PER MODE")
            # plt.show()

        spat_sel_modes = np.array(spat_sel_modes)
        avg_spat_sel_modes = np.reshape(np.mean(spat_sel_modes, axis=0),(rate_maps_orig.shape[1], rate_maps_orig.shape[2]))
        plt.imshow(avg_spat_sel_modes, interpolation='nearest', aspect='equal')
        a = plt.colorbar()
        a.set_label("AVG. CORR. m-VECTOR MODE <> RATE MAP \n"
                    "FOR ALL SPATIALLY CONSTRAINED MODES")
        plt.title("COVERAGE OF SPATIALLY CONSTRAINED MODES")
        plt.xlabel("X")
        plt.ylabel("Y")
        plt.show()

        plt.hist(sparsity_per_mode)
        plt.vlines(np.percentile(sparsity_null,95), 0, 10, colors="r", label="95% percentile, shuffled data")
        plt.xlabel("SPARSITY")
        plt.ylabel("#MODES")
        plt.title("SPARSITY OF SIGNIF. CORRELATION VALUES \n (ROWS OF ABOVE MATRIX)")
        plt.legend()
        plt.show()

        print(sparsity_per_mode)
        exit()

        plt.imshow(res,  interpolation='nearest', aspect='auto')
        plt.xlabel("SPATIAL BINS")
        plt.ylabel("MODE ID")
        a = plt.colorbar()
        a.set_label("PEARSON CORRELATION")
        plt.title("CORRELATION: MEAN FIRING POP. VEC. EXPLORATION \n - CELL FIRING PROB. PER MODE")
        plt.show()

        for mode_id, _ in enumerate(all_modes_m.T):
            res_1 = np.reshape(res[mode_id,:], (rate_maps_orig.shape[1], rate_maps_orig.shape[2]))
            plt.imshow(res_1,  interpolation='nearest', aspect='auto')
            plt.title(str(mode_id))
            plt.colorbar()
            plt.show()

    def tree_hmm_fit(self, file_name, perc_threshold=0.5, prob_threshold=0.1):
        # --------------------------------------------------------------------------------------------------------------
        # analyses identified modes
        #
        # args: - file_name, string: input file name (output of TreeHMM)
        #       - perc_threshold, float: threshold for cells with high firing probability (threshold * max prob value)
        #       - prob_threshold, float:    threshold for cells with high firing probability (max prob value) to filter
        #                                   low prob.
        # --------------------------------------------------------------------------------------------------------------
        # self.sleep_fam.evaluate_tree_hmm_fit(file_name=file_name)

        ind_high_firing_cells, all_modes_m = self.sleep_fam.tree_hmm_high_firing_cells(file_name=file_name,
                                                                        perc_threshold=perc_threshold,
                                                                        prob_threshold=prob_threshold, plotting=True)

        rate_maps = self.exploration_fam.get_rate_maps()

        corr_coefficients = []

        for mode in range(all_modes_m.shape[1]):

            pfs_matrix = self.exploration_fam.place_field_similarity(plotting=False)
            upper_diag_pcf_matrix = upper_tri_without_diag(pfs_matrix)

            mode_m = np.expand_dims(all_modes_m[:,mode], axis=1)
            sq = mode_m @ mode_m.T
            firing_product_upper_diag = upper_tri_without_diag(sq)
            to_delete = np.argwhere(firing_product_upper_diag < 0.001)

            firing_product_upper_diag = np.delete(firing_product_upper_diag, to_delete)
            upper_diag_pcf_matrix = np.delete(upper_diag_pcf_matrix, to_delete)
            try:
                corr_coefficients.append(pearsonr(firing_product_upper_diag, upper_diag_pcf_matrix)[0])
            except:
                continue
            # firing_product_upper_diag = firing_product_upper_diag(firing_product_upper_diag != 0)
            # upper_diag_pcf_matrix = upper_diag_pcf_matrix(firing_product_upper_diag != 0)
        #
        # plt.scatter(upper_diag_pcf_matrix, firing_product_upper_diag)
        # plt.show()
        print(corr_coefficients)
        plt.hist(corr_coefficients, density=True)
        plt.xlabel("PEARSON R: JOINT FIRING PROBABILITY - PFS")
        plt.ylabel("DENSITY")
        plt.show()

        mode_spec_pfs = []
        within_all_modes_pfs = []
        modes_without_hi_fi_cells = []

        for mode_id in range(len(ind_high_firing_cells)):

            cells_in_mode = ind_high_firing_cells[mode_id]

            within_mode_pfs = []
            other_cell_ids = cells_in_mode.copy()

            if len(cells_in_mode) == 0:
                modes_without_hi_fi_cells.append(mode_id)

            # find all entries in place similarity matrix of high firing cells within mode
            for cell_id in cells_in_mode:
                print(cell_id)
                other_cell_ids = np.delete(other_cell_ids, np.argwhere(other_cell_ids == cell_id))
                if other_cell_ids.shape[0] == 0:
                    break
                within_mode_pfs.extend(pfs_matrix[cell_id, other_cell_ids])

            mode_spec_pfs.append(within_mode_pfs)
            within_all_modes_pfs.extend(within_mode_pfs)
            # plt.hist(upper_diag_pcf_matrix, density=True)
            #
            # plt.hist(within_mode_pfs, density=True)
            # plt.show()
        plt.figure(figsize=[10, 6])
        plt.subplot(2,1,1)
        import statistics
        for i, mode_val in enumerate(mode_spec_pfs):
            print(mode_val)
            if len(mode_val) == 1:
                plt.scatter(i, mode_val, color="yellow", label="TWO CELLS")
            elif len(mode_val) > 1:
                plt.scatter(i, statistics.median(mode_val), color="blue", label="MANY CELLS - MEDIAN")
            elif len(mode_val) == 0:
                plt.vlines(i, -0.01, 0.01, colors="red", label="ONE CELL")

        # color all modes that do not have even one high firing cells
        plt.vlines(modes_without_hi_fi_cells, -0.01, 0.01, colors="magenta", label="NO HI FI CELL")

        handles, labels = plt.gca().get_legend_handles_labels()
        by_label = OrderedDict(zip(labels, handles))
        plt.legend(by_label.values(), by_label.keys())
        plt.xlabel("CLUSTER ID")
        plt.ylabel("PLACE FIELD SIMILARITY")
        plt.title("CELLS WITH HIGH FIRING PROBABILITY PER CLUSTER (PERC./PROB. THRESH: "+str(perc_threshold)+", "+
                                                                                             str(prob_threshold)+")")
        plt.subplot(2, 1, 2)
        plt.hist(upper_diag_pcf_matrix, histtype='stepfilled', alpha=0.8, density=True, bins=40, color="#990000",
                 label="ALL PFS VALUES")

        plt.hist(within_all_modes_pfs, histtype='stepfilled', alpha=0.3, density=True, bins=40, color="yellow",
                 label="VALUES WITHIN MODES")
        plt.legend()
        plt.title("PLACE FIELD SIMILARITY")
        plt.xlabel("PEARSON R")
        plt.ylabel("DENSITY")
        plt.show()


        exit()
        for mode_id in range(len(ind_high_firing_cells)):
            cells_in_mode = ind_high_firing_cells[mode_id]
            rate_maps = np.array(rate_maps)
            avg_rate_map = np.mean(rate_maps[cells_in_mode, :, :], axis=0)
            plt.imshow(avg_rate_map)
            plt.title("MODE "+str(mode_id)+", NR. OF HIGH FIRING PROB. CELLS: "+str(cells_in_mode.shape[0]))
            plt.show()

        exit()

    """#################################################################################################################
    #  poisson hmm
    #################################################################################################################"""

    def poisson_train_sleep_fit_awake(self, file_name):
        # --------------------------------------------------------------------------------------------------------------
        # loads poisson hmm model that was trained on sleep and fits it to awake data
        #
        # parameters:   - file_name, string: file that contains the model that was trained on sleep data
        #
        # returns:      -
        # --------------------------------------------------------------------------------------------------------------
        print(" - EVALUATING POISSON HMM TRAINED ON SLEEP AND FIT TO AWAKE DATA")

        means, std, mode_freq, env = self.exploration_fam.fit_poisson_hmm(file_name=file_name, plot_awake_fit=False)

        constrained_means = means[:, std < 35]
        fig, ax = plt.subplots()
        ax.scatter(means[0, :], means[1, :], c="grey", label="ALL MEANS")
        ax.scatter(constrained_means[0, :], constrained_means[1, :], c="red", label="SPATIALLY CONSTRAINED")
        ax.add_artist(env)
        ax.set_ylim(40, 220)
        ax.set_xlim(0, 175)
        ax.set_aspect("equal")
        plt.xlabel("X")
        plt.ylabel("Y")
        plt.legend()
        plt.show()
        exit()

        raster_sleep = self.sleep_fam.get_raster().T

        with open(self.params.pre_proc_dir+"ML/" + file_name, "rb") as file:
            model = pickle.load(file)

        state_sequence = model.predict(raster_sleep)

        std_state_sequence = std_modes[state_sequence]

        plt.plot(std_state_sequence[:800])
        plt.show()

        exit()

        state_seq_sh = state_sequence[:10000]

        means_to_plot = means[:, state_seq_sh]

        fig, ax = plt.subplots()
        ax.scatter(means_to_plot[0, :], means_to_plot[1, :], c="grey")

        import matplotlib.cm as cm
        colors = cm.cool(np.linspace(0, 1, state_seq_sh.shape[0] - 1))
        for i, c in zip(range(0, state_seq_sh.shape[0] - 1), colors):
            ax.plot(means_to_plot[0, i:i + 2], means_to_plot[1, i:i + 2], color=c)
        # plt.title(title)
        # ax.scatter(mds[:, 0], mds[:, 1], color="grey")
        ax.scatter(means_to_plot[0, 0], means_to_plot[1, 0], color="white", marker="x", label="start", zorder=200)
        ax.scatter(means_to_plot[0, -1], means_to_plot[1, -1], color="white", label="end", zorder=200)

        ax.add_artist(env)
        ax.set_ylim(40, 220)
        ax.set_xlim(0, 175)
        ax.set_aspect("equal")
        plt.xlabel("X")
        plt.ylabel("Y")
        plt.legend()
        plt.show()

        print(state_sequence.shape)

        exit()

    def poisson_exploration_fam_nov(self, file_name_fam, file_name_nov):
        # --------------------------------------------------------------------------------------------------------------
        # loads poisson hmm model from exploration familiar and exploration novel
        #
        # parameters:   - file_name_fam, string: file that contains the model that was trained on exploration familiar
        #               - file_name_nov, string: file that contains the model that was trained on exploration novel
        # returns:      -
        # --------------------------------------------------------------------------------------------------------------
        print(" - EVALUATING POISSON HMM TRAINED ON SLEEP AND FIT TO AWAKE DATA")

        means_f, std_modes_f, mode_freq_f, env_f, trans_mat_f, state_trans_f, mode_lambda_f = \
            self.exploration_fam.fit_poisson_hmm(file_name=file_name_fam, plot_awake_fit=False)

        means_n, std_modes_n, mode_freq_n, env_n, trans_mat_n, state_trans_n, mode_lambda_n = \
            self.exploration_novel.fit_poisson_hmm(file_name=file_name_nov, plot_awake_fit=False)

        sim_across = np.zeros((mode_lambda_n.shape[0], mode_lambda_f.shape[0]))
        sim_within_n = np.zeros((mode_lambda_n.shape[0], mode_lambda_n.shape[0]))
        sim_within_f = np.zeros((mode_lambda_f.shape[0], mode_lambda_f.shape[0]))

        for i, lam_n in enumerate(mode_lambda_n):
            for j, lam_f in enumerate(mode_lambda_f):
                sim_across[i, j] = distance.cosine(lam_n, lam_f)

        plt.imshow(sim_across, interpolation='nearest', aspect='auto')
        plt.colorbar()
        plt.title("novel")
        plt.show()

        for i, lam_n in enumerate(mode_lambda_n):
            mode_lambda_wo = np.delete(mode_lambda_n, i, axis=0)
            for j, lam_f in enumerate(mode_lambda_wo):
                sim_within_n[i, j] = distance.cosine(lam_n, lam_f)

        for i, lam_n in enumerate(mode_lambda_f):
            mode_lambda_wo = np.delete(mode_lambda_f, i, axis=0)
            for j, lam_f in enumerate(mode_lambda_wo):
                sim_within_f[i, j] = distance.cosine(lam_n, lam_f)

        plt.hist(sim_across.flatten(), color="b")
        plt.hist(sim_within_n.flatten(), color="r", alpha=0.5)
        plt.hist(sim_within_f.flatten(), color="gray", alpha=0.5)
        plt.show()

        plt.imshow(mode_lambda_n.T, interpolation='nearest', aspect='auto')
        plt.title("novel")
        plt.show()

        plt.imshow(mode_lambda_f.T, interpolation='nearest', aspect='auto')
        plt.title("familiar")
        plt.show()

        exit()

        constrained_means = means[:, std < 35]
        fig, ax = plt.subplots()
        ax.scatter(means[0, :], means[1, :], c="grey", label="ALL MEANS")
        ax.scatter(constrained_means[0, :], constrained_means[1, :], c="red", label="SPATIALLY CONSTRAINED")
        ax.add_artist(env)
        ax.set_ylim(40, 220)
        ax.set_xlim(0, 175)
        ax.set_aspect("equal")
        plt.xlabel("X")
        plt.ylabel("Y")
        plt.legend()
        plt.show()

    def poisson_train_awake_fit_sleep(self, poisson_model_file):
        # --------------------------------------------------------------------------------------------------------------
        # loads poisson hmm model that was trained on awake data and fits it to awake data --> to look at reactivations
        #
        # parameters:   - poisson_model_file, string: file that contains the model that was trained on awake data
        #
        # returns:      -
        # --------------------------------------------------------------------------------------------------------------

        seq, nr_modes = self.sleep_novel.fit_poisson_hmm(file_name=poisson_model_file)
        X = self.sleep_novel.get_raster()

        # order modes by occurance
        a, b = np.unique(seq, return_counts=True)

        occ_ind = np.flip(np.argsort(b))
        mode_dec_occ = a[occ_ind]
        occ = b[occ_ind]
        occ_norm = occ / np.max(b)
        means, std, mode_freq, env, transmat, _, _ = \
            self.exploration_novel.fit_poisson_hmm(file_name=poisson_model_file, plot_awake_fit=False)

        means_dec_occ = means[:, mode_dec_occ]
        std_dec_occ = std[mode_dec_occ]

        labels = np.round((occ * self.params.time_bin_size) / 60, 3).astype(str).tolist()
        occ_norm = occ_norm * 200
        fig, ax = plt.subplots()

        ax.scatter(means_dec_occ[0, 1:-1], means_dec_occ[1, 1:-1], c="red", s=occ_norm[1:-1], alpha=0.5)
        ax.scatter(means_dec_occ[0, 0], means_dec_occ[1, 0], c="red", s=occ_norm[0], alpha=0.5,
                   label=labels[0] + " min")
        ax.scatter(means_dec_occ[0, -10], means_dec_occ[1, -10], c="red", s=occ_norm[-10], alpha=0.5,
                   label=labels[-1] + " min")
        ax.add_artist(env)
        ax.set_ylim(40, 220)
        ax.set_xlim(0, 175)
        ax.set_aspect("equal")
        plt.legend()
        plt.title("DURATION AND LOCATION OF REACTIVATED MODE")
        plt.xlabel("X")
        plt.ylabel("Y")
        plt.show()

        window_size = 10

        bins_per_window = int(window_size / self.params.time_bin_size)

        nr_windows = int(X.shape[1] / bins_per_window)

        mode_prob = np.zeros((nr_modes, nr_windows))
        for i in range(nr_windows):
            a, b = np.unique(seq[i * bins_per_window:(i + 1) * bins_per_window], return_counts=True)
            mode_prob[a, i] = b / bins_per_window

        # adjust weight of mode that was never reactivated --> set to 1
        mode_prob[np.where(~mode_prob.any(axis=1))[0], :] = 1

        max_mode_prob = np.max(mode_prob, axis=1)
        mode_prob_norm = mode_prob / mode_prob.max(axis=1)[:, None]

        plt.imshow(mode_prob_norm, interpolation='nearest', aspect='auto')
        plt.title("MODE FREQUENCY - NORMALIZED - " + str(window_size) + "s WINDOW")
        plt.ylabel("MODE ID")
        plt.xlabel("WINDOW ID")
        a = plt.colorbar()
        a.set_label("MODE FREQUENCY - NORMALIZED")

        # compute weighted average
        windows = np.tile(np.arange(nr_windows).T, nr_modes).reshape((nr_modes, nr_windows))

        weighted_av = np.average(windows, axis=1, weights=mode_prob)

        a = (windows - weighted_av[:, None]) ** 2

        weighted_std = np.sqrt(np.average(a, axis=1, weights=mode_prob))

        plt.scatter(weighted_av, np.arange(nr_modes), c="r", s=1, label="WEIGHTED AVERAGE")
        plt.legend()
        plt.show()

        plt.scatter(weighted_av, weighted_std)
        plt.title("MODE OCCURENCE: WEIGHTED AVERAGE & WEIGHTED STD")
        plt.ylabel("WEIGHTED STD")
        plt.xlabel("WEIGHTED AVERAGE")
        plt.show()

        entro = []
        # compute entropy
        for i in mode_prob_norm:
            entro.append(entropy(i))

        entro = np.array(entro)

        plt.hist(entro)
        plt.title("ENTROPY")
        plt.xlabel("ENTROPY")
        plt.ylabel("COUNT")
        plt.show()

        # find mode to split into high and low entropy
        dist_mode = mode_of_dist(entro)[0]
        dist_mode = 3.8

        high_entropy_modes = np.squeeze(np.argwhere(entro > dist_mode), 1)
        low_entropy_modes = np.squeeze(np.argwhere(entro < dist_mode), 1)
        #
        # means, std_modes, mode_freq, env, trans_mat, state_sequence, mode_lambda = \
        #     self.exploration_fam.load_fit_poisson_hmm(file_name=poisson_model_file)
        #

        means, std, mode_freq, env, transmat, _, _ = \
            self.exploration_novel.load_fit_poisson_hmm(file_name=poisson_model_file, plot_awake_fit=False)

        r = pearsonr(entro, std)
        plt.scatter(entro, std)
        plt.ylabel("SPATIAL COVERAGE (STD) - BEHAVIOR")
        plt.xlabel("ENTROPY - SLEEP")
        plt.title(str(r))
        plt.show()

        plt.subplot(2, 1, 1)
        plt.hist(std[high_entropy_modes], density=True, color="red", label="HIGH ENTR.")
        plt.hist(std[low_entropy_modes], density=True, color="blue", alpha=0.5, label="LOW ENTR.")
        plt.ylabel("DENSITY")
        plt.xlabel("STD / cm")
        plt.legend()
        means_hi_ent = means[:, high_entropy_modes]

        # compute distance between means --> correlate with transition matrix

        means_lo_ent = means[:, low_entropy_modes]

        # compute distance between means --> correlate with transition matrix
        plt.subplot(2, 1, 2)

        dist_mat = np.zeros((means_hi_ent.shape[1], means_hi_ent.shape[1]))

        for i, mean_1 in enumerate(means_hi_ent.T):
            for j, mean_2 in enumerate(means_hi_ent.T):
                dist_mat[i, j] = np.linalg.norm(mean_1 - mean_2)
        plt.hist(upper_tri_without_diag(dist_mat).flatten(), density=True, color="red")

        dist_mat = np.zeros((means_lo_ent.shape[1], means_lo_ent.shape[1]))

        for i, mean_1 in enumerate(means_lo_ent.T):
            for j, mean_2 in enumerate(means_lo_ent.T):
                dist_mat[i, j] = np.linalg.norm(mean_1 - mean_2)

        plt.hist(upper_tri_without_diag(dist_mat).flatten(), density=True, alpha=0.5, color="blue")

        plt.ylabel("DENSITY")
        plt.xlabel("DISTANCE BETWEEN MEANS / cm")
        plt.show()

        high_ent_means = means[:, high_entropy_modes]
        low_ent_means = means[:, low_entropy_modes]
        fig, ax = plt.subplots()
        ax.scatter(means[0, :], means[1, :], c="grey", label="ALL MEANS")
        ax.scatter(high_ent_means[0, :], high_ent_means[1, :], c="red", label="HIGH ENT. MEANS")
        ax.scatter(low_ent_means[0, :], low_ent_means[1, :], c="blue", label="LOW ENT. MEANS")
        ax.add_artist(env)
        ax.set_ylim(40, 220)
        ax.set_xlim(0, 175)
        ax.set_aspect("equal")
        plt.xlabel("X")
        plt.ylabel("Y")
        plt.legend()
        plt.show()

        exit()

        for mode in high_entropy_modes:
            self.exploration_fam.phmm_single_mode_details(mode_id=mode, poisson_model_file=poisson_model_file)

    def poisson_reactivation_fam_nov(self, file_phmm_fam, file_phmm_nov):
        # --------------------------------------------------------------------------------------------------------------
        # loads poisson hmm model that was trained on awake data before and model that was trained on awake data after
        # and fits it to sleep data --> to look at reactivations
        #
        # parameters:   - file_name, string: file that contains the model that was trained on awake data
        #
        # returns:      -
        # --------------------------------------------------------------------------------------------------------------

        seq_f, nr_modes_f = self.sleep_fam.fit_poisson_hmm(file_name=file_phmm_fam)
        X_f = self.sleep_fam.get_raster()

        seq_n, nr_modes_n = self.sleep_novel.fit_poisson_hmm(file_name=file_phmm_nov)
        X_n = self.sleep_novel.get_raster()

        _, time_react_fam = np.unique(seq_f, return_counts=True)
        _, time_react_nov = np.unique(seq_n, return_counts=True)

        time_react_fam = time_react_fam * self.params.time_bin_size
        time_react_nov = time_react_nov * self.params.time_bin_size

        window_size = 10
        bins_per_window = int(window_size / self.params.time_bin_size)

        nr_windows = int(X_f.shape[1] / bins_per_window)

        mode_prob = np.zeros((nr_modes_f, nr_windows))
        for i in range(nr_windows):
            a, b = np.unique(seq_f[i * bins_per_window:(i + 1) * bins_per_window], return_counts=True)
            mode_prob[a, i] = b / bins_per_window

        # adjust weight of mode that was never reactivated --> set to 1
        mode_prob[np.where(~mode_prob.any(axis=1))[0], :] = 1

        max_mode_prob = np.max(mode_prob, axis=1)
        mode_prob_norm = mode_prob / mode_prob.max(axis=1)[:, None]

        # compute entropy
        entro_f = []
        # compute entropy
        for i in mode_prob_norm:
            entro_f.append(entropy(i))
        entro_f = np.array(entro_f)

        plt.hist(entro_f, density=True, bins=20, color="orange", label="FAMILIAR")

        plt.vlines(np.median(entro_f), 0, 0.5, colors="red", label="MEDIAN FAMILIAR")
        nr_windows = int(X_n.shape[1] / bins_per_window)

        mode_prob = np.zeros((nr_modes_n, nr_windows))
        for i in range(nr_windows):
            a, b = np.unique(seq_n[i * bins_per_window:(i + 1) * bins_per_window], return_counts=True)
            mode_prob[a, i] = b / bins_per_window

        # adjust weight of mode that was never reactivated --> set to 1
        mode_prob[np.where(~mode_prob.any(axis=1))[0], :] = 1

        mode_prob_norm = mode_prob / mode_prob.max(axis=1)[:, None]

        # compute entropy
        entro_n = []
        # compute entropy
        for i in mode_prob_norm:
            entro_n.append(entropy(i))
        entro_n = np.array(entro_n)

        plt.vlines(np.median(entro_n), 0, 0.5, colors="blue", label="MEDIAN NOVEL")

        plt.hist(entro_n, density=True, bins=20, alpha=0.5, label="NOVEL")
        plt.legend()
        plt.xlabel("ENTROPY")
        plt.ylabel("DENSITY")
        plt.title(str(mannwhitneyu(entro_f, entro_n)))
        plt.show()

    def phmm_spatial_assignment(self, file_name_pop_1, file_name_pop_2):
        # --------------------------------------------------------------------------------------------------------------
        # analysis awake pHMM fits of two experiment phases (e.g. exploration familiar vs. exploration novel)
        #
        # args:         - file_name_pop_1, str: file name containing pHMM model for population 1
        #               - file_name_pop_2, str: file name containing pHMM model for population 2

        #
        # returns:      -
        # --------------------------------------------------------------------------------------------------------------
        means_f, std_f, mode_freq_f, env_f, transmat_f, _, _ = \
            self.exploration_fam.fit_poisson_hmm(file_name=file_name_pop_1, plot_awake_fit=False)

        means_n, std_n, mode_freq_n, env_n, transmat_n, _, _ = \
            self.exploration_novel.fit_poisson_hmm(file_name=file_name_pop_2, plot_awake_fit=False)

        plt.hist(std_f, color="r", label="FAM", bins=int(means_f.shape[1]/3))
        plt.hist(std_n, color="b", label="NOV", alpha=0.5, bins=int(means_n.shape[1]/3))
        plt.xlabel("STD. / CM")
        plt.ylabel("COUNT")
        plt.legend()
        plt.title("SPATIALLY CONSTRAINS OF MODES")
        plt.show()

    def cross_val_poisson_hmm_cb_pre_post(self, cl_ar=np.arange(1, 100, 10), cell_selection="all"):
        # --------------------------------------------------------------------------------------------------------------
        # cross validation of poisson hmm fits to data
        #
        # args:     - cl_ar, range object: #clusters to fit to data
        # --------------------------------------------------------------------------------------------------------------

        print(" - CROSS-VALIDATING POISSON HMM ON CHEESEBOARD: PRE AND POST --> OPTIMAL #MODES ...")

        # get pre and post raster data
        pre_raster = self.learning_cheeseboard[0].get_raster()
        post_raster = self.learning_cheeseboard[1].get_raster()

        raster = np.hstack((pre_raster, post_raster))

        if cell_selection == "stable":
            # load only stable cells
            with open(
                    self.params.pre_proc_dir + "cell_classification/" + self.params.session_name + "_k_means.pickle",
                    "rb") as f:
                class_dic = pickle.load(f)

            stable_ids = class_dic["stable_cell_ids"]

            raster = raster[stable_ids, :]

            folder_name = self.params.session_name + "_pre_post_cb_stable" + self.cell_type

        elif cell_selection == "non_stable":

            # load only stable cells
            with open(
                    self.params.pre_proc_dir + "cell_classification/" + self.params.session_name + "_k_means.pickle",
                    "rb") as f:
                class_dic = pickle.load(f)

            stable_ids = class_dic["stable_cell_ids"]

            raster = np.delete(raster, stable_ids, axis=0)

            folder_name = self.params.session_name + "_pre_post_cb_non_stable" + self.cell_type

        elif cell_selection == "all":

            folder_name = self.params.session_name + "_pre_post_cb_" + self.cell_type

        nr_cores = 12

        new_ml = MlMethodsOnePopulation(params=self.params)
        new_ml.parallelize_cross_val_model(nr_cluster_array=cl_ar, nr_cores=nr_cores, model_type="POISSON_HMM",
                                           raster_data=raster, folder_name=folder_name)
        new_ml.cross_val_view_results(folder_name=folder_name)

    def view_cross_val_results_cb_pre_post(self, first_data_point_to_plot=None, cell_selection="all"):
        # --------------------------------------------------------------------------------------------------------------
        # views cross validation results
        #
        # args:     - model_type, string: which type of model ("POISSON_HMM")
        #           - custom_splits, bool: whether custom splits were used for cross validation
        # --------------------------------------------------------------------------------------------------------------
        if cell_selection == "stable":
            folder_name = self.params.session_name +"_pre_post_cb_stable"+self.cell_type
        elif cell_selection == "non_stable":
            folder_name = self.params.session_name +"_pre_post_cb_non_stable"+self.cell_type
        elif cell_selection == "all":
            folder_name = self.params.session_name +"_pre_post_cb_"+self.cell_type

        new_ml = MlMethodsOnePopulation(params=self.params)
        new_ml.cross_val_view_results(folder_name=folder_name, first_data_point_to_plot=first_data_point_to_plot)

    def fit_poisson_hmm_cb_pre_post(self, nr_modes, cell_selection="all"):
        # --------------------------------------------------------------------------------------------------------------
        # fits poisson hmm to data
        #
        # args:     - nr_modes, int: #clusters to fit to data
        #           - file_identifier, string: string that is added at the end of file for identification
        # --------------------------------------------------------------------------------------------------------------

        print(" - FITTING POISSON HMM WITH "+str(nr_modes)+" MODES ...\n")

        # get pre and post raster data
        pre_raster = self.learning_cheeseboard[0].get_raster()
        post_raster = self.learning_cheeseboard[1].get_raster()

        raster = np.hstack((pre_raster, post_raster))

        if cell_selection == "stable":
            # load only stable cells
            with open(
                    self.params.pre_proc_dir + "cell_classification/" + self.params.session_name + "_k_means.pickle",
                    "rb") as f:
                class_dic = pickle.load(f)

            stable_ids = class_dic["stable_cell_ids"]

            raster = raster[stable_ids, :]
            file_name = self.params.session_name + "_pre_post_cb_stable_" + \
                        self.cell_type + "_" + str(nr_modes) + "_modes"

        elif cell_selection == "non_stable":
            # load only stable cells
            with open(
                    self.params.pre_proc_dir + "cell_classification/" + self.params.session_name + "_k_means.pickle",
                    "rb") as f:
                class_dic = pickle.load(f)

            stable_ids = class_dic["stable_cell_ids"]

            raster = np.delete(raster, stable_ids, axis=0)
            file_name = self.params.session_name + "_pre_post_cb_non_stable_" + \
                        self.cell_type + "_" + str(nr_modes) + "_modes"

        elif cell_selection == "all":
            file_name = self.params.session_name + "_pre_post_cb_" + self.cell_type + "_" + str(nr_modes) + "_modes"

        model = PoissonHMM(n_components=nr_modes)
        model.fit(raster.T)
        model.set_time_bin_size(time_bin_size=self.params.time_bin_size)

        with open(self.params.pre_proc_dir+"phmm/"+file_name+".pkl", "wb") as file: pickle.dump(model, file)

        print("  - ... DONE!\n")

    def evaluate_poisson_hmm_cb_pre_post(self, nr_modes):
        # --------------------------------------------------------------------------------------------------------------
        # fits poisson hmm to data and evaluates the goodness of the model by comparing basic statistics (avg. firing
        # rate, correlation values, k-statistics) between real data and data sampled from the model
        #
        # args:     - nr_modes, int: #clusters to fit to data
        #           - load_from_file, bool: whether to load model from file or to fit model again
        # --------------------------------------------------------------------------------------------------------------

        print(" - EVALUATING POISSON HMM FIT (BASIC STATISTICS) ...")

        # get pre and post raster data
        pre_raster = self.learning_cheeseboard[0].get_raster()
        post_raster = self.learning_cheeseboard[1].get_raster()

        raster = np.hstack((pre_raster, post_raster))

        nr_time_bins = raster.shape[1]
        # X = X[:, :1000]

        file_name = self.params.session_name + "_pre_post_cb_" + self.cell_type+"_"+str(nr_modes)+"_modes"

        # check if model file exists already --> otherwise fit model again
        if os.path.isfile(self.params.pre_proc_dir+"phmm/" + file_name + ".pkl"):
            print("- LOADING PHMM MODEL FROM FILE\n")
            with open(self.params.pre_proc_dir+"phmm/" + file_name + ".pkl", "rb") as file:
                model = pickle.load(file)
        else:
            print("- PHMM MODEL FILE NOT FOUND --> FITTING PHMM TO DATA\n")
            model = PoissonHMM(n_components=nr_modes)
            model.fit(raster.T)

        samples, sequence = model.sample(nr_time_bins)
        samples = samples.T

        evaluate_clustering_fit(real_data=raster, samples=samples, binning="TEMPORAL_SPIKE",
                                   time_bin_size=0.1, plotting=True)

    def analyze_poisson_hmm_cb_pre_post(self, nr_modes, cell_selection = "all"):
        # --------------------------------------------------------------------------------------------------------------
        # fits poisson hmm to data and evaluates the goodness of the model by comparing basic statistics (avg. firing
        # rate, correlation values, k-statistics) between real data and data sampled from the model
        #
        # args:     - nr_modes, int: #clusters to fit to data
        #           - load_from_file, bool: whether to load model from file or to fit model again
        # --------------------------------------------------------------------------------------------------------------

        print(" - EVALUATING POISSON HMM FIT (BASIC STATISTICS) ...")

        # get pre and post raster data
        pre_raster = self.learning_cheeseboard[0].get_raster()
        post_raster = self.learning_cheeseboard[1].get_raster()

        if cell_selection == "stable":
            # load only stable cells
            with open(
                    self.params.pre_proc_dir + "cell_classification/" + self.params.session_name + "_k_means.pickle",
                    "rb") as f:
                class_dic = pickle.load(f)

            stable_ids = class_dic["stable_cell_ids"]

            pre_raster = pre_raster[stable_ids, :]
            post_raster = post_raster[stable_ids, :]

            file_name = self.params.session_name + "_pre_post_cb_stable_" + self.cell_type+"_"+str(nr_modes)+"_modes"

        elif cell_selection == "non_stable":
            # load only stable cells
            with open(
                    self.params.pre_proc_dir + "cell_classification/" + self.params.session_name + "_k_means.pickle",
                    "rb") as f:
                class_dic = pickle.load(f)

            stable_ids = class_dic["stable_cell_ids"]

            pre_raster = np.delete(pre_raster, stable_ids, axis=0)
            post_raster = np.delete(post_raster, stable_ids, axis=0)

            file_name = self.params.session_name + "_pre_post_cb_non_stable_" + self.cell_type+"_"+str(nr_modes)+"_modes"


        elif cell_selection == "all":
            file_name = self.params.session_name + "_pre_post_cb_" + self.cell_type+"_"+str(nr_modes)+"_modes"

        raster = np.hstack((pre_raster, post_raster))

        # check if model file exists already --> otherwise fit model again
        if os.path.isfile(self.params.pre_proc_dir+"phmm/" + file_name + ".pkl"):
            print("- LOADING PHMM MODEL FROM FILE\n")
            with open(self.params.pre_proc_dir+"phmm/" + file_name + ".pkl", "rb") as file:
                model = pickle.load(file)
        else:
            print("- PHMM MODEL FILE NOT FOUND --> FITTING PHMM TO DATA\n")
            model = PoissonHMM(n_components=nr_modes)
            model.fit(raster.T)

        # get #occurences modes for pre and post
        pre_modes, pre_counts = np.unique(model.predict(pre_raster.T), return_counts=True)
        pre_counts = (pre_counts / np.sum(pre_counts)) * 100

        # get #occurences modes for pre and post
        post_modes, post_counts = np.unique(model.predict(post_raster.T), return_counts=True)
        post_counts = (post_counts / np.sum(post_counts)) * 100

        # # get lambdas and look at their similarity
        #
        # lambdas = model.means_
        #
        # D = pairwise_distances(lambdas, metric="euclidean")
        # plt.imshow(D)
        # plt.colorbar()
        # plt.show()
        #
        # plt.hist(D.flatten())
        # plt.show()
        # thresh = 0.2
        #
        # lambdas_bin = lambdas.copy()
        # lambdas_bin[lambdas_bin >= thresh] = 1
        # lambdas_bin[lambdas_bin < thresh] = 0
        # lambdas_bin=lambdas_bin.astype(bool)
        # plt.imshow(lambdas_bin)
        # plt.show()
        # D = pairwise_distances(lambdas_bin, metric="jaccard")
        # plt.imshow(D)
        # plt.colorbar()
        # plt.show()
        #
        #
        # exit()

        lines = []
        for i in range(pre_counts.shape[0]):
            pair = [(pre_modes[i], 0), (pre_modes[i], pre_counts[i])]
            lines.append(pair)

        linecoll = matcoll.LineCollection(lines, colors="violet")
        fig, ax = plt.subplots()

        ax.add_collection(linecoll)

        lines = []
        for i in range(post_counts.shape[0]):
            pair = [(post_modes[i], 0), (post_modes[i], post_counts[i])]
            lines.append(pair)

        linecoll = matcoll.LineCollection(lines, colors="lightskyblue")

        ax.add_collection(linecoll)

        plt.scatter(pre_modes, pre_counts, c="violet", label="PRE")
        plt.scatter(post_modes, post_counts, c="lightskyblue", alpha=0.8, label="POST")
        plt.legend()
        plt.xlabel("MODE ID")
        plt.ylabel("MODE OCCURENCE (%)")
        plt.show()

        print("HERE")

    """#################################################################################################################
    #  memory drift
    #################################################################################################################"""

    # support functions
    # ------------------------------------------------------------------------------------------------------------------

    @staticmethod
    def compute_values_from_probabilities(pre_prob_list, post_prob_list, pre_prob_z_list, post_prob_z_list):
        # per event results
        event_pre_post_ratio = []
        event_pre_post_ratio_z = []
        event_pre_prob = []
        event_post_prob = []
        event_len_seq = []

        # per population vector results
        pop_vec_pre_post_ratio = []
        pre_seq_list = []
        pre_seq_list_z = []
        post_seq_list = []
        pre_seq_list_prob = []
        post_seq_list_prob = []
        pop_vec_post_prob = []
        pop_vec_pre_prob = []

        # go trough all events
        for pre_array, post_array, pre_array_z, post_array_z in zip(pre_prob_list, post_prob_list, pre_prob_z_list,
                                                                    post_prob_z_list):
            # make sure that there is any data for the current SWR
            if pre_array.shape[0] > 0:
                pre_sequence = np.argmax(pre_array, axis=1)
                pre_sequence_z = np.argmax(pre_array_z, axis=1)
                pre_sequence_prob = np.max(pre_array, axis=1)
                post_sequence = np.argmax(post_array, axis=1)
                post_sequence_prob = np.max(post_array, axis=1)
                pre_seq_list_z.extend(pre_sequence_z)
                pre_seq_list.extend(pre_sequence)
                post_seq_list.extend(post_sequence)
                pre_seq_list_prob.extend(pre_sequence_prob)
                post_seq_list_prob.extend(post_sequence_prob)

                # check how likely observed sequence is considering transitions from model (awake behavior)
                event_len_seq.append(pre_sequence.shape[0])

                # per SWR computations
                # ----------------------------------------------------------------------------------------------
                # arrays: [nr_pop_vecs_per_SWR, nr_time_spatial_time_bins]
                # get maximum value per population vector and take average across the SWR
                if pre_array.shape[0] > 0:
                    # save pre and post probabilities
                    event_pre_prob.append(np.mean(np.max(pre_array, axis=1)))
                    event_post_prob.append(np.mean(np.max(post_array, axis=1)))
                    # compute ratio by picking "winner" mode by first comparing z scored probabilities
                    # then the probability of the most over expressed mode (highest z-score) is used
                    pre_sequence_z = np.argmax(pre_array_z, axis=1)
                    prob_pre_z = np.mean(pre_array[:, pre_sequence_z])
                    post_sequence_z = np.argmax(post_array_z, axis=1)
                    prob_post_z = np.mean(post_array[:, post_sequence_z])
                    event_pre_post_ratio_z.append((prob_post_z - prob_pre_z) / (prob_post_z + prob_pre_z))

                    # compute ratio using probabilites
                    prob_pre = np.mean(np.max(pre_array, axis=1))
                    prob_post = np.mean(np.max(post_array, axis=1))
                    event_pre_post_ratio.append((prob_post - prob_pre) / (prob_post + prob_pre))
                else:
                    event_pre_prob.append(np.nan)
                    event_post_prob.append(np.nan)
                    event_pre_post_ratio.append(np.nan)

                # per population vector computations
                # ----------------------------------------------------------------------------------------------
                # compute per population vector similarity score
                prob_post = np.max(post_array, axis=1)
                prob_pre = np.max(pre_array, axis=1)
                pop_vec_pre_post_ratio.extend((prob_post - prob_pre) / (prob_post + prob_pre))

                if pre_array.shape[0] > 0:
                    pop_vec_pre_prob.extend(np.max(pre_array, axis=1))
                    pop_vec_post_prob.extend(np.max(post_array, axis=1))
                else:
                    pop_vec_pre_prob.extend([np.nan])
                    pop_vec_post_prob.extend([np.nan])

        pop_vec_pre_prob = np.array(pop_vec_pre_prob)
        pop_vec_post_prob = np.array(pop_vec_post_prob)
        pop_vec_pre_post_ratio = np.array(pop_vec_pre_post_ratio)
        pre_seq_list = np.array(pre_seq_list)

        return event_pre_post_ratio, event_pre_post_ratio_z, event_pre_prob, event_post_prob, event_len_seq,\
               pop_vec_pre_post_ratio, pre_seq_list, pre_seq_list_z, post_seq_list, pre_seq_list_prob, \
               post_seq_list_prob, pop_vec_post_prob, pop_vec_pre_prob

    def save_spike_rasters(self, part_to_analyze, file_name):
        """
        save constant #spike rasters for either REM, SWR, or NREM

        :param part_to_analyze: which sleep epoch to return ("REM","NREM")
        :type part_to_analyze: str
        :param file_name: where to save
        :type file_name: str
        """
        all_rasters = []
        for l_s in self.long_sleep:
            event_spike_rasters, event_spike_window_lenghts=l_s.get_event_spike_rasters(part_to_analyze=part_to_analyze)
            all_rasters.extend(event_spike_rasters)
        with open(file_name, "wb") as fp:  # Pickling
            pickle.dump(all_rasters, fp)

    def memory_drift_long_sleep_get_results(self, template_type, part_to_analyze, pop_vec_threshold, measure="normalized_ratio",
                                            pre_file_name=None, post_file_name=None, only_stable_cells=False):

        first = 0
        pre_prob_list = []
        post_prob_list = []
        event_times_list = []
        for i, l_s in enumerate(self.long_sleep):
            if not i:
                default_pre_phmm_model, default_post_phmm_model,_ ,_ = l_s.get_pre_post_templates()
            duration = l_s.get_duration_sec()
            pre_prob, post_prob, ev_t, _ = l_s.decode_activity_using_pre_post(template_type=template_type,
                                                                              pre_file_name=pre_file_name,
                                                                              post_file_name=post_file_name,
                                                                              part_to_analyze=part_to_analyze,
                                                                              only_stable_cells=only_stable_cells)
            pre_prob_list.extend(pre_prob)
            post_prob_list.extend(post_prob)
            event_times_list.extend(ev_t + first)
            first += duration

        pre_prob_arr = np.vstack(pre_prob_list)
        post_prob_arr = np.vstack(post_prob_list)
        if measure == "normalized_ratio":
            result_all = (np.max(post_prob_arr, axis=1) - np.max(pre_prob_arr, axis=1)) / \
                        (np.max(pre_prob_arr, axis=1) + np.max(post_prob_arr, axis=1))
        elif measure == "log_like_ratio":
            result_all = np.log(np.max(post_prob_arr, axis=1)/np.max(pre_prob_arr, axis=1))
        elif measure == "model_evidence":
            if template_type == "phmm":
                # load pHMM models
                with open(self.params.pre_proc_dir + "phmm/" + default_pre_phmm_model + '.pkl', 'rb') as f:
                    model_pre_dic = pickle.load(f)
                stationary_dist_pre = model_pre_dic.get_stationary_distribution()
                with open(self.params.pre_proc_dir + "phmm/" + default_post_phmm_model + '.pkl', 'rb') as f:
                    model_post_dic = pickle.load(f)
                stationary_dist_post = model_post_dic.get_stationary_distribution()

                pre_prob = np.sum(pre_prob_arr * stationary_dist_pre, axis=1)
                post_prob = np.sum(post_prob_arr * stationary_dist_post, axis=1)
                result_all = np.log(post_prob/pre_prob)

        # assign array chunks to events again (either SWR or rem phases)
        length_per_event = [x.shape[0] for x in pre_prob_list]
        result_per_event = []
        first = 0
        for swr_id in range(len(length_per_event)):
            result_per_event.append(result_all[first:first + length_per_event[swr_id]])
            first += length_per_event[swr_id]

        # delete swr that are too short (< 2 population vectors)
        short_nrem_events_to_delete = np.where(np.array(length_per_event) < pop_vec_threshold)[0]
        for index in sorted(short_nrem_events_to_delete, reverse=True):
            del result_per_event[index]
            del length_per_event[index]
            del event_times_list[index]

        event_times = np.vstack(event_times_list)

        # compute length in seconds
        duration_event_in_s = event_times[:, 1] - event_times[:, 0]

        return result_per_event, event_times, length_per_event, duration_event_in_s, pre_prob_arr, post_prob_arr

    def memory_drift_long_sleep_get_raw_results(self, template_type, part_to_analyze, pop_vec_threshold=2,
                                                pre_file_name=None, post_file_name=None, only_stable_cells=False):

        first = 0
        pre_prob_list = []
        post_prob_list = []
        event_times_list = []
        for l_s in self.long_sleep:
            duration = l_s.get_duration_sec()
            pre_prob, post_prob, ev_t, _ = l_s.decode_activity_using_pre_post(template_type=template_type,
                                                                              pre_file_name=pre_file_name,
                                                                              post_file_name=post_file_name,
                                                                              part_to_analyze=part_to_analyze,
                                                                              only_stable_cells=only_stable_cells)
            pre_prob_list.extend(pre_prob)
            post_prob_list.extend(post_prob)
            event_times_list.extend(ev_t + first)
            first += duration

        length_per_event = [x.shape[0] for x in pre_prob_list]
        # delete events that are too short (< 2 population vectors)
        short_nrem_events_to_delete = np.where(np.array(length_per_event) < pop_vec_threshold)[0]
        for index in sorted(short_nrem_events_to_delete, reverse=True):
            del pre_prob_list[index]
            del post_prob_list[index]
            del event_times_list[index]

        return pre_prob_list, post_prob_list, event_times_list

    # decoding analysis (Ising or pHMM)
    # ------------------------------------------------------------------------------------------------------------------

    def memory_drift(self, template_type, measure, pre_file_name=None, post_file_name=None,
                               n_moving_average_pop_vec=60, rem_pop_vec_threshold=10, plotting=False,
                               only_stable_cells=False):
        """
        detailed memory drift analysis: opposing effects of NREM/REM etc.

        :param template_type: which template to use ("phmm" or "ising")
        :type template_type: str
        :param measure: which measure to use to compare PRE and POST similarity (model_evidence, log_like_ratio,
                        normalized_ratio
        :type measure: str
        :param pre_file_name: name of PRE template, if None --> default is used
        :type pre_file_name: str
        :param post_file_name: name of POST template, if None --> default is used
        :type post_file_name: str_decoding_similarity_temporal
        :param n_moving_average_pop_vec: how much smoothing to apply across population vectors
        :type n_moving_average_pop_ - np.mean(dec, axis=0)[0]vec: int
        :param rem_pop_vec_threshold: minimum length of REM epochs (shorter ones are discarded)
        :type rem_pop_vec_threshold: int
        :param plotting: whether to plot
        :type plotting: bool
        :param only_stable_cells: whether to only use stable cells
        :type only_stable_cells: bool
        :return: ds_rem_sum, ds_nrem_sum, ratio_per_merged_nrem_event, ratio_per_merged_rem_event
        :rtype:
        """
        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_rem_event, event_times_rem, event_lengths_rem, event_duration_rem_in_s, _, _ = \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold, measure=measure,
                                                     only_stable_cells=only_stable_cells)

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_nrem_event, event_times_nrem, event_lengths_nrem, event_duration_nrem_in_s, _ ,_ = \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="nrem",
                                                     pop_vec_threshold=2, measure=measure,
                                                     only_stable_cells=only_stable_cells)

        # combine rem and nrem data to order it in the right way (chronologically)
        # --------------------------------------------------------------------------------------_decoding_similarity_temporal------------------------

        all_events_ratio = ratio_per_rem_event + ratio_per_nrem_event
        all_events_length = event_lengths_rem + event_lengths_nrem
        all_events_length_s = np.hstack((event_duration_rem_in_s, event_duration_nrem_in_s))
        labels_events = np.zeros(len(ratio_per_rem_event)+len(ratio_per_nrem_event))
        labels_events[:len(ratio_per_rem_event)] = 1
        all_times = np.hstack((event_times_rem[:, 0], event_times_nrem[:,0]))
        all_end_times = np.hstack((event_times_rem[:, 1], event_times_nrem[:, 2]))

        # sort events according to time
        sorted_events_ratio_list = [x for _, x in sorted(zip(all_times, all_events_ratio))]
        sorted_labels_events = [x for _, x in sorted(zip(all_times, labels_events))]
        sorted_lengths = [x for _, x in sorted(zip(all_times, all_events_length))]
        sorted_lengths_s = [x for _, x in sorted(zip(all_times, all_events_length_s))]
        sorted_pop_vec_ratio = np.hstack(sorted_events_ratio_list)
        sorted_times = [x for x in sorted(all_times)]
        sorted_end_times = [x for _, x in sorted(zip(all_times, all_end_times))]

        # compute labels per population vector --> rem events = 1, nrem events = 0
        # --------------------------------------------------------------------------------------------------------------
        labels_per_pop_vec = []
        for len_event, label in zip(sorted_lengths, sorted_labels_events):
            if label:
                labels_per_pop_vec.extend(np.ones(len_event))
            elif label == 0:
                labels_per_pop_vec.extend(np.zeros(len_event))

        labels_per_pop_vec = np.array(labels_per_pop_vec)

        # concatenate events that have the same label
        # --------------------------------------------------------------------------------------------------------------
        sorted_labels_events = np.array(sorted_labels_events)
        label_change = np.diff(sorted_labels_events)
        merged_events_length_s = []
        merged_events_labels = []
        merged_events_times = []
        first = 0
        while True:
            dat = label_change[first:]
            if first > label_change.shape[0]:
                break
            if sorted_labels_events[first] == 0:
                # nrem event
                # check if this is the last event
                if np.where(dat==1)[0].shape[0] == 0:
                    trans = label_change.shape[0] - first + 1
                else:
                    trans = np.where(dat == 1)[0][0]+1
                merged_events_length_s.append(np.sum(sorted_lengths_s[first:first+trans]))
                merged_events_labels.append(np.unique(sorted_labels_events[first:first + trans]))
                merged_events_times.append([sorted_times[first], sorted_end_times[first + trans - 1]])
            elif sorted_labels_events[first] == 1:
                # rem event
                # check if this is the last event
                if np.where(dat==-1)[0].shape[0] == 0:
                    trans = label_change.shape[0] - first + 1
                else:
                    trans = np.where(dat == -1)[0][0]+1
                merged_events_length_s.append(np.sum(sorted_lengths_s[first:first+trans]))
                merged_events_labels.append(np.unique(sorted_labels_events[first:first + trans]))
                merged_events_times.append([sorted_times[first], sorted_end_times[first+trans-1]])
            first += trans

        merged_events_labels = np.squeeze(np.array(merged_events_labels))
        merged_events_length_s = np.array(merged_events_length_s)
        merged_events_times = np.vstack(merged_events_times)

        # merged_rem_event_times = merged_events_times[merged_events_labels == 1]
        # merged_nrem_event_times = merged_events_times[merged_events_labels == 0]
        merged_events_length_s = merged_events_times[:,1]-merged_events_times[:,0]


        # concatenate population vectors that have the same label: TODO: there might be a better way to combine this
        # step and the previous step
        # --------------------------------------------------------------------------------------------------------------
        new_event_time_stamps = np.abs(np.diff(labels_per_pop_vec))

        # need to add one element at the beginning
        new_event_time_stamps = np.insert(new_event_time_stamps, 0, 1)
        start = np.nonzero(new_event_time_stamps==1)[0]
        end = start[1:]
        end = np.append(end, new_event_time_stamps.shape[0])

        # only select data (pre_post ratio) from population vectors that belong to selected rem/nrem events
        # --> results per merged event
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_merged_rem_event = []
        ratio_per_merged_nrem_event = []
        ratio_rem_nrem_events = []
        rem_nrem_events_label = []
        ratio_rem_nrem_pop_vec = []
        rem_nrem_pop_vec_label = []

        for start_event, end_event in zip(start, end):
            # rem event
            if labels_per_pop_vec[start_event + 1] == 1:
                ratio_per_merged_rem_event.append(sorted_pop_vec_ratio[start_event:end_event])
            # nrem event
            else:
                ratio_per_merged_nrem_event.append(sorted_pop_vec_ratio[start_event:end_event])

            ratio_rem_nrem_events.append(sorted_pop_vec_ratio[start_event:end_event])
            rem_nrem_events_label.extend([labels_per_pop_vec[start_event + 1]])

            ratio_rem_nrem_pop_vec.extend(sorted_pop_vec_ratio[start_event:end_event])
            rem_nrem_pop_vec_label.extend(labels_per_pop_vec[start_event:end_event])


        # smooth across population vectors --> that means also across REM/NREM epochs
        # --------------------------------------------------------------------------------------------------------------
        len_new_events = [x.shape[0] for x in ratio_rem_nrem_events]
        ratio_per_pop_vec_new = np.hstack(ratio_rem_nrem_events)
        ratio_per_pop_vec_new_smooth = moving_average(a=np.array(ratio_per_pop_vec_new), n=n_moving_average_pop_vec)

        ratio_rem_nrem_events_smooth = []
        first = 0
        for event_id in range(len(len_new_events)):
            # check if data is still available (due to smoothing --> data gets shorter)
            # if not --> leave loop
            if first >= ratio_per_pop_vec_new_smooth.shape[0]:
                break
            end_event = min(first + len_new_events[event_id], ratio_per_pop_vec_new_smooth.shape[0])
            ratio_rem_nrem_events_smooth.append(ratio_per_pop_vec_new_smooth[first:end_event])
            first += len_new_events[event_id]

        # need to delete indices of events that happend after the last data point after smoothing
        rem_nrem_events_label = rem_nrem_events_label[:len(ratio_rem_nrem_events_smooth)]

        # get rem data of merged events after smoothing
        # --------------------------------------------------------------------------------------------------------------
        rem_events_indices = np.squeeze(np.argwhere(np.array(rem_nrem_events_label) == 1))
        ds_rem = []
        ds_rem_smoothed_within = []
        merged_events_rem_length = []
        ratio_rem_events_smooth = []
        for rem_index in rem_events_indices:
            ds_rem.append(ratio_rem_nrem_events_smooth[rem_index][-1]-ratio_rem_nrem_events_smooth[rem_index][0])
            smooth_event = moving_average(a=ratio_rem_nrem_events[rem_index], n=10)
            ds_rem_smoothed_within.append(smooth_event[-1]-smooth_event[0])
            merged_events_rem_length.append(ratio_rem_nrem_events_smooth[rem_index].shape[0])
            ratio_rem_events_smooth.append(ratio_rem_nrem_events_smooth[rem_index])

        # get nrem data of merged events after smoothing
        # --------------------------------------------------------------------------------------------------------------
        nrem_events_indices = np.squeeze(np.argwhere(np.array(rem_nrem_events_label) == 0))
        ds_nrem = []
        ds_nrem_smoothed_within = []
        merged_events_nrem_length = []
        ratio_nrem_events_smooth = []
        for nrem_index in nrem_events_indices:
            ds_nrem.append(ratio_rem_nrem_events_smooth[nrem_index][-1]-ratio_rem_nrem_events_smooth[nrem_index][0])
            merged_events_nrem_length.append(ratio_rem_nrem_events_smooth[nrem_index].shape[0])
            ratio_nrem_events_smooth.append(ratio_rem_nrem_events_smooth[nrem_index])

        # compute delta score sum & cum sum
        # --------------------------------------------------------------------------------------------------------------
        ds_nrem_sum_smoothed_within = np.sum(np.array(ds_nrem_smoothed_within))
        ds_rem_sum_smoothed_within = np.sum(np.array(ds_rem_smoothed_within))
        ds_nrem_sum = np.sum(np.array(ds_nrem))
        ds_rem_sum = np.sum(np.array(ds_rem))
        ds_nrem_cum = np.cumsum(np.array(ds_nrem))
        ds_rem_cum = np.cumsum(np.array(ds_rem))

        # compute cross correlation of delta scores NREM/REM
        # --------------------------------------------------------------------------------------------------------------
        min_len = min(len(ds_rem), len(ds_nrem))
        ds_rem_arr = np.array(ds_rem)[:min_len]
        ds_nrem_arr = np.array(ds_nrem)[:min_len]
        corr_list_pos = []
        corr_list_neg = []
        shift_array_cross_corr = [0, 1, 2, 3, 4, 5, 6]
        for shift in shift_array_cross_corr:
            corr_list_pos.append(
                np.round(pearsonr(ds_rem_arr[shift:], ds_nrem_arr[:ds_nrem_arr.shape[0] - shift])[0], 2))
            corr_list_neg.append(
                np.round(pearsonr(ds_nrem_arr[shift:], ds_rem_arr[:ds_rem_arr.shape[0] - shift])[0], 2))
        corr_list = np.hstack((np.flip(np.array(corr_list_neg)), np.array(corr_list_pos)))

        # get mean score per event
        # --------------------------------------------------------------------------------------------------------------
        mean_nrem = []
        for i, event in enumerate(ratio_per_merged_nrem_event):
            mean_nrem.append(np.mean(event))
        mean_rem = []
        for i, event in enumerate(ratio_per_merged_rem_event):
            mean_rem.append(np.mean(event))

        # get data smoothed only across REM events and only across NREM events
        # --------------------------------------------------------------------------------------------------------------
        ratio_merged_nrem_events_arr = np.hstack(ratio_per_merged_nrem_event)
        ratio_merged_nrem_events_arr_smooth = moving_average(a=ratio_merged_nrem_events_arr, n=n_moving_average_pop_vec)
        ratio_merged_rem_events_arr = np.hstack(ratio_per_merged_rem_event)
        ratio_merged_rem_events_arr_smooth = moving_average(a=ratio_merged_rem_events_arr, n=n_moving_average_pop_vec)

        # compute oscillations of REM/NREM periods
        # --------------------------------------------------------------------------------------------------------------
        # moving window
        window_size = 100
        step_size = 100

        window_start = np.arange(0, ratio_merged_rem_events_arr_smooth.shape[0] - window_size, step_size)
        min_max_rem = []
        std_rem = []
        for w_s in window_start:
            min_max_rem.append(max(ratio_merged_rem_events_arr_smooth[w_s:w_s + step_size])
                               - min(ratio_merged_rem_events_arr_smooth[w_s:w_s + step_size]))
            std_rem.append(np.std(ratio_merged_rem_events_arr_smooth[w_s:w_s + step_size]))
        window_start = np.arange(0, ratio_merged_nrem_events_arr_smooth.shape[0] - window_size, step_size)
        min_max_nrem = []
        std_nrem = []
        for w_s in window_start:
            min_max_nrem.append(max(ratio_merged_nrem_events_arr_smooth[w_s:w_s + step_size])
                                - min(ratio_merged_nrem_events_arr_smooth[w_s:w_s + step_size]))
            std_nrem.append(np.std(ratio_merged_nrem_events_arr_smooth[w_s:w_s + step_size]))

        if plotting:

            # plotting
            fig = plt.figure()
            ax = fig.add_subplot()
            start = 0
            for event, label in zip(ratio_rem_nrem_events_smooth, rem_nrem_events_label):
                event_length = event.shape[0]
                if label:
                    ax.plot(np.arange(start, start + event_length), event, c="r", label="REM")
                else:
                    ax.plot(np.arange(start, start + event_length), event, c="b", label="NREM")
                start += event_length
            handles, labels = ax.get_legend_handles_labels()
            by_label = OrderedDict(zip(labels, handles))
            ax.legend(by_label.values(), by_label.keys())
            plt.grid()
            plt.title("SMOOTHING: n="+str(n_moving_average_pop_vec))
            plt.xlabel("POPULATION VECTOR ID")
            if measure == "normalized_ratio":
                plt.ylim(-1, 1)
                plt.ylabel("PRE_POST RATIO")
            elif measure == "log_like_ratio":
                plt.ylabel("LOG-LIKELIHOOD RATIO")
                y_min, y_max = ax.get_ylim()
                plt.ylim(y_min, -1*y_min)

            elif measure == "model_evidence":
                plt.ylabel("MODEL EVIDENCE")
                y_min, y_max = ax.get_ylim()
                plt.ylim(y_min, -1*y_min)

            plt.show()

            plt.plot(ratio_merged_rem_events_arr_smooth, c="r", label="REM")
            plt.plot(ratio_merged_nrem_events_arr_smooth, c="b", label="NREM", alpha=0.8)
            plt.legend()
            plt.xlabel("POP.VEC.ID")
            plt.ylabel("PRE_POST_SCORE")
            plt.show()

            plt.hist(min_max_rem, color="r", density=True, label="REM")
            plt.hist(min_max_nrem, color="b", density=True, alpha=0.6, label="NREM")
            plt.legend()
            plt.xlabel("MAX-MIN IN SLIDING WINDOW OF "+str(window_size)+" POP.VEC.")
            plt.ylabel("DENSITY")
            plt.show()

            plt.hist(std_rem, color="r", density=True, label="REM")
            plt.hist(std_nrem, color="b", density=True, alpha=0.6, label="NREM")
            plt.legend()
            plt.xlabel("STD IN SLIDING WINDOW OF "+str(window_size)+" POP.VEC.")
            plt.ylabel("DENSITY")
            plt.show()

            def make_square_axes(ax):
                """Make an axes square in screen units.

                Should be called after plotting.
                """
                ax.set_aspect(1 / ax.get_data_ratio())

            plt.scatter(ds_rem_arr, ds_nrem_arr)
            plt.title("NEIGHBOURING PERIODS, R="+str(np.round(pearsonr(ds_rem_arr, ds_nrem_arr)[0], 2)))
            plt.xlabel("DELTA SCORE REM")
            plt.ylabel("DELTA SCORE NREM")
            make_square_axes(plt.gca())
            plt.show()

            x_axis_cross_corr = -1 * np.flip(np.array(shift_array_cross_corr)+1)
            x_axis_cross_corr = np.hstack((x_axis_cross_corr, np.array(shift_array_cross_corr)+1))
            plt.plot(x_axis_cross_corr, corr_list, marker=".")
            plt.xlabel("REM OFFSET (+X MEANS REM BEHIND NREM BY X)")
            plt.ylabel("CORRELATION DELTA SCORE REM VS. NREM")
            plt.title("CROSS CORRELATION OF DELTA SCORES REM VS. NREM")
            plt.show()
            #
            # plt.plot(ds_rem, color="gray", linewidth=0.8)
            # plt.scatter(range(len(ds_rem)), ds_rem, c=merged_events_length_s[merged_events_labels == 1], cmap=cm.Reds)
            # a = plt.colorbar()
            # a.set_label("DURATION / s")
            # plt.hlines(0, 0, len(ds_rem), color="w", linewidth=0.5, zorder=-1000)
            # plt.ylabel("DELTA SCORE")
            # plt.xlabel("EVENT ID")
            # plt.title("REM: PER EVENT DELTA SCORE & DURATION")
            # plt.show()
            #
            # plt.plot(ds_nrem, color="gray", linewidth=0.8)
            # plt.scatter(range(len(ds_nrem)), ds_nrem, c=merged_events_length_s[merged_events_labels == 0], cmap=cm.Reds)
            # a = plt.colorbar()
            # a.set_label("DURATION / s")
            # plt.hlines(0, 0, len(ds_nrem), color="w", linewidth=0.5, zorder=-1000)
            # plt.ylabel("DELTA SCORE")
            # plt.xlabel("EVENT ID")
            # plt.title("NREM: PER EVENT DELTA SCORE & DURATION")
            # plt.show()

            # plt.plot(merged_events_length_s[merged_events_labels == 1], color="gray", linewidth=0.8)
            # plt.scatter(range(len(merged_events_length_s[merged_events_labels == 1])),
            #             merged_events_length_s[merged_events_labels == 1], c=ds_rem, cmap=cm.coolwarm)
            # a = plt.colorbar()
            # a.set_label("DELTA SCORE")
            # plt.ylabel("DURATION EVENT")
            # plt.xlabel("EVENT ID")
            # plt.title("REM: PER EVENT DELTA SCORE & DURATION")
            # plt.show()
            #
            # plt.plot(merged_events_length_s[merged_events_labels == 0], color="gray", linewidth=0.8)
            # plt.scatter(range(len(merged_events_length_s[merged_events_labels == 0])),
            #             merged_events_length_s[merged_events_labels == 0], c=ds_nrem, cmap=cm.coolwarm)
            # a = plt.colorbar()
            # a.set_label("DELTA SCORE")
            # plt.ylabel("DURATION EVENT")
            # plt.xlabel("EVENT ID")
            # plt.title("NREM: PER EVENT DELTA SCORE & DURATION")
            # plt.show()

            # check which phase came first
            if rem_nrem_events_label[0] == 1:
                # first event was a rem event
                rem_x_axis = np.arange(0,2*len(ds_rem), 2)
                nrem_x_axis = np.arange(1, 2 * len(ds_nrem) + 1, 2)
            elif rem_nrem_events_label[0] == 0:
                # first event was a nrem event
                nrem_x_axis = np.arange(0,2*len(ds_nrem), 2)
                rem_x_axis = np.arange(1, 2 * len(ds_rem) + 1, 2)

            plt.plot(rem_x_axis, ds_rem, marker="." ,label="REM", color="r")
            plt.plot(nrem_x_axis, ds_nrem, marker=".", label="NREM", color="b")
            plt.legend()
            plt.hlines(0, 0, rem_x_axis.shape[0]+nrem_x_axis.shape[0], color="gray")
            plt.ylabel("DELTA SCORE")
            plt.xlabel("EVENT ID")
            plt.title("PER EVENT DELTA SCORE")
            plt.show()

            # plt.figure(figsize=(11, 6))
            # plt.subplot(1, 2, 1)
            # plt.plot(rem_x_axis, merged_events_rem_length, marker="." ,label="REM", color="r")
            # plt.plot(nrem_x_axis, merged_events_nrem_length, marker=".", label="NREM", color="b")
            # plt.legend()
            # plt.ylabel("#POPULATION VECTORS")
            # plt.xlabel("EVENT ID")
            # plt.title("#POPULATION VECTORS PER EVENT")
            # plt.subplot(1, 2, 2)
            # plt.plot(rem_x_axis, merged_events_length_s[merged_events_labels == 1], marker="." ,label="REM", color="r")
            # plt.plot(nrem_x_axis, merged_events_length_s[merged_events_labels == 0], marker=".", label="NREM", color="b")
            # plt.ylabel("DURATION / s")
            # plt.xlabel("EVENT ID")
            # plt.title("DURATION OF EACH EVENT")
            # plt.legend()
            # plt.show()


            plt.hist(merged_events_length_s[merged_events_labels == 1], bins=10, label="REM", density=True, color="r")
            plt.hist(merged_events_length_s[merged_events_labels == 0], bins=10, label="NREM", density=True, color="b")
            plt.title("EVENT DURATION (REM.POP.VEC.THRS.="+str(rem_pop_vec_threshold)+")")
            plt.xlabel("DURATION OF EVENT / s")
            plt.ylabel("DENSITY")
            plt.legend()
            plt.show()


            plt.figure(figsize=(11, 6))
            plt.subplot(1, 2, 1)
            plt.scatter(merged_events_length_s[rem_events_indices], ds_rem, color="r")
            plt.hlines(0, 0, plt.gca().get_xlim()[1], color="gray", linewidth=0.5)
            plt.ylabel("DELTA SCORE")
            plt.xlabel("DURATION / s")
            plt.title("REM PER EVENT (POP.THR.REM.="+str(rem_pop_vec_threshold)+")")
            make_square_axes(plt.gca())
            plt.subplot(1, 2, 2)
            plt.scatter(merged_events_length_s[nrem_events_indices], ds_nrem, color="b")
            plt.xlabel("DURATION / s")
            plt.hlines(0, 0, plt.gca().get_xlim()[1], color="gray", linewidth=0.5)
            plt.title("NREM PER EVENT")
            make_square_axes(plt.gca())
            plt.show()

            plt.scatter(merged_events_length_s[rem_events_indices], ds_rem, color="r", label="REM")
            plt.scatter(merged_events_length_s[nrem_events_indices], ds_nrem, color="b", label="NREM")
            plt.hlines(0, 0, plt.gca().get_xlim()[1], color="gray", linewidth=0.5)
            plt.ylabel("DELTA SCORE")
            plt.xlabel("DURATION / s")
            plt.title("DURATION - DELTA SCORE (POP.THR.REM.=" + str(rem_pop_vec_threshold) + ")")
            make_square_axes(plt.gca())
            plt.legend()
            plt.show()


            plt.figure(figsize=(11, 6))
            plt.subplot(1, 2, 1)
            plt.scatter(merged_events_rem_length, ds_rem, color="r")
            plt.hlines(0, 0, plt.gca().get_xlim()[1], color="gray", linewidth=0.5)
            plt.ylabel("DELTA SCORE")
            plt.xlabel("#POP.VEC.")
            plt.title("REM PER EVENT (POP.THR.REM.="+str(rem_pop_vec_threshold)+")")
            make_square_axes(plt.gca())
            plt.subplot(1, 2, 2)
            plt.scatter(merged_events_nrem_length, ds_nrem, color="b")
            plt.xlabel("#POP.VEC.")
            plt.hlines(0, 0, plt.gca().get_xlim()[1], color="gray", linewidth=0.5)
            plt.title("NREM PER EVENT")
            make_square_axes(plt.gca())
            plt.show()

            plt.title("SMOOTHING n=" + str(n_moving_average_pop_vec) + ", POP.THR.REM=" + str(rem_pop_vec_threshold))
            plt.plot(mean_nrem, marker=".", color="b", label="NREM")
            plt.plot(mean_rem, marker=".", color="r", label="REM")
            plt.xlabel("EVENT ID")
            plt.ylabel("MEAN SCORE PER EVENT")
            plt.legend()
            plt.show()
            common_nr = min(len(mean_nrem), len(mean_rem))
            diff_in_mean = np.abs(np.array(mean_nrem[:common_nr]) - np.array(mean_rem[:common_nr]))
            plt.title("DIFF REM NREM, SMOOTHING n=" + str(n_moving_average_pop_vec) + ", POP.THR.REM=" + str(rem_pop_vec_threshold))
            plt.plot(diff_in_mean, marker=".", color="w")
            plt.xlabel("EVENT ID")
            plt.ylabel("ABS. DIFF. BETWEEN MEAN SCORE PER EVENT")
            plt.show()

            plt.scatter(1, ds_nrem_sum, color="b", label="NREM", zorder=1000)
            # plt.scatter(1, ds_nrem_cum_smoothed_within, color="lightskyblue", label="NREM - WITHIN", zorder=1000)
            plt.scatter(1, ds_rem_sum, color="r", label="REM", zorder=1000)
            # plt.scatter(1, ds_rem_cum_smoothed_within, color="lightcoral", label="REM - WITHIN", zorder=1000)
            plt.legend()
            plt.ylabel("CUMULATIVE DELTA SCORE")
            plt.ylim(-(max(abs(ds_nrem_sum_smoothed_within), abs(ds_rem_sum_smoothed_within), abs(ds_rem_sum), abs(ds_nrem_sum))+1),
                     (max(abs(ds_nrem_sum_smoothed_within), abs(ds_rem_sum_smoothed_within), abs(ds_rem_sum), abs(ds_nrem_sum))+1))
            plt.title("SMOOTHING: n="+str(n_moving_average_pop_vec))
            plt.grid()
            plt.show()

            plt.title("SMOOTHING n=" + str(n_moving_average_pop_vec) + ", POP.THR.REM=" + str(rem_pop_vec_threshold))
            plt.plot(ds_nrem_cum, color="b", label="NREM")
            plt.plot(ds_rem_cum, color="r", label="REM")
            plt.grid()
            plt.xlabel("EVENT ID")
            plt.legend()
            plt.ylabel("CUM SUM")
            plt.show()

            y_rem,_,_ =plt.hist(ds_rem, color="r", label="REM", density=True)
            y_nrem,_,_ = plt.hist(ds_nrem, color="b", alpha=0.5, label="NREM", density=True)
            plt.vlines(np.mean(ds_nrem),0,y_nrem.max(), color="lightskyblue", label="MEAN NREM")
            plt.vlines(np.mean(ds_rem), 0, y_rem.max(), color="lightcoral", label="MEAN REM")
            plt.xlabel("DELTA SCORE")
            plt.ylabel("DENSITY")
            plt.title("SMOOTHING: n=" + str(n_moving_average_pop_vec))
            plt.legend()
            plt.show()

            # event shape

            plt.figure(figsize=(12, 6))
            plt.subplot(1, 2, 1)

            x_len = len(ratio_rem_events_smooth)
            y_len = len(max(ratio_rem_events_smooth, key=lambda x: len(x)))

            b = np.empty((x_len, y_len))
            b[:] = np.nan
            for i, j in enumerate(ratio_rem_events_smooth):
                b[i][0:len(j)] = j

            # subtract means to center
            b = b - np.nanmean(b, axis=1, keepdims=True)
            # scale values to lie between -1 and 1
            b = b / np.nanmax(np.abs(b), axis=1, keepdims= True)

            mean_shape = np.nanmean(b, axis=0)
            for shape in b:
                plt.plot(shape, c="gray")
            plt.plot(mean_shape, c="r")
            plt.ylabel("PRE_POST SCORE - SINGLE EVENTS + MEAN")
            plt.title("REM")
            plt.xlabel("POP.VEC.ID. FROM EVENT ONSET")

            plt.subplot(1, 2, 2)
            x_len = len(ratio_nrem_events_smooth)
            y_len = len(max(ratio_nrem_events_smooth, key=lambda x: len(x)))

            b = np.empty((x_len, y_len))
            b[:] = np.nan
            for i, j in enumerate(ratio_nrem_events_smooth):
                b[i][0:len(j)] = j

            # subtract means to center
            b = b - np.nanmean(b, axis=1, keepdims=True)
            # scale values to lie between -1 and 1
            b = b / np.nanmax(np.abs(b), axis=1, keepdims= True)

            mean_shape = np.nanmean(b, axis=0)
            for shape in b:
                plt.plot(shape, c="gray")
            plt.plot(mean_shape, c="b")
            plt.title("NREM")
            plt.xlabel("POP.VEC.ID. FROM EVENT ONSET")
            plt.show()

        else:
            return ds_rem_sum, ds_nrem_sum, ratio_per_merged_nrem_event, ratio_per_merged_rem_event

    def memory_drift_plot_epochs_separate(self, template_type, part_to_analyze, pre_file_name=None, post_file_name=None,
                                n_moving_average_swr=15, n_moving_average_pop_vec=60, only_stable_cells=False):
        """
        plot results of memory drift analysis for epochs separately (or in same plot, but not concatenated)

        :param template_type: which template to use ("phmm", "ising")
        :type template_type: str
        :param part_to_analyze: which sleep epoch to analyze ("rem", "nrem", "nrem_rem")
        :type part_to_analyze: str
        :param pre_file_name: name of PRE template, if None --> use default
        :type pre_file_name: str
        :param post_file_name: name of POST template, if None --> use default
        :type post_file_name: str
        :param n_moving_average_swr: how much smoothing to apply across SWR
        :type n_moving_average_swr: int
        :param n_moving_average_pop_vec: how much smoothing to apply across population vectors
        :type n_moving_average_pop_vec: int
        :param only_stable_cells: whether to only use stable cells
        :type only_stable_cells: bool
        """
        if part_to_analyze == "nrem_rem":
            first = 0
            # plot nrem data first
            # ----------------------------------------------------------------------------------------------------------
            pre_prob_list = []
            post_prob_list = []
            event_times_list = []
            for l_s in self.long_sleep:
                duration = l_s.get_duration_sec()
                pre_prob, post_prob, ev_t, _ = l_s.decode_activity_using_pre_post(template_type=template_type,
                                                                                  pre_file_name=pre_file_name, post_file_name=post_file_name, part_to_analyze="nrem",
                                                                                  only_stable_cells=only_stable_cells)
                pre_prob_list.extend(pre_prob)
                post_prob_list.extend(post_prob)
                event_times_list.extend(ev_t+first)
                first += duration

            pre_prob_arr = np.vstack(pre_prob_list)
            post_prob_arr = np.vstack(post_prob_list)
            event_times = np.vstack(event_times_list)

            # z-scoring of probabilites
            pre_prob_arr_z = zscore(pre_prob_arr, axis=0)
            post_prob_arr_z = zscore(post_prob_arr, axis=0)

            # assign array chunks to events again (either SWR or rem phases)
            event_lengths = [x.shape[0] for x in pre_prob_list]
            pre_prob_z = []
            post_prob_z = []
            first = 0
            for swr_id in range(len(event_lengths)):
                pre_prob_z.append(pre_prob_arr_z[first:first + event_lengths[swr_id], :])
                post_prob_z.append(post_prob_arr_z[first:first + event_lengths[swr_id], :])
                first += event_lengths[swr_id]

            event_pre_post_ratio, event_pre_post_ratio_z, event_pre_prob, event_post_prob, event_len_seq, \
            pop_vec_pre_post_ratio, pre_seq_list, pre_seq_list_z, post_seq_list, pre_seq_list_prob, \
            post_seq_list_prob, pop_vec_post_prob, pop_vec_pre_prob = self.compute_values_from_probabilities(
                pre_prob_list=pre_prob_list, post_prob_list=post_prob_list, post_prob_z_list=post_prob_z,
                pre_prob_z_list=pre_prob_z)

            # smoothen
            # ------------------------------------------------------------------------------------------------------
            event_pre_post_ratio_smooth = moving_average(a=np.array(event_pre_post_ratio), n=n_moving_average_swr)
            event_pre_post_ratio_smooth_z = moving_average(a=np.array(event_pre_post_ratio_z), n=n_moving_average_swr)
            pop_vec_pre_post_ratio = np.array(pop_vec_pre_post_ratio)
            # compute moving average to smooth signal
            pop_vec_pre_post_ratio_smooth = moving_average(a=pop_vec_pre_post_ratio, n=n_moving_average_pop_vec)
            event_len_seq_smooth = moving_average(a=np.array(event_len_seq), n=n_moving_average_swr)

            event_times = event_times[:event_pre_post_ratio_smooth.shape[0], :]
            # # plot per nrem phase
            fig = plt.figure()
            ax = fig.add_subplot()
            # for nrem_id in range(swr_to_nrem.shape[0]):
            #     ax.plot(event_times[swr_to_nrem[nrem_id,:]==1,1],
            #             event_pre_post_ratio_smooth[swr_to_nrem[nrem_id,:]==1],c="blue", label="NREM")

            # plot per nrem phase
            start = 0
            for rem_length, rem_time in zip(event_lengths, event_times):
                if start + rem_length > pop_vec_pre_post_ratio_smooth.shape[0]:
                    continue
                ax.plot(np.linspace(rem_time[0], rem_time[1], rem_length),
                        pop_vec_pre_post_ratio_smooth[start:start + rem_length], c="blue", label="NREM")
                start += rem_length


            # plot rem data
            # ----------------------------------------------------------------------------------------------------------
            first = 0

            pre_prob_list = []
            post_prob_list = []
            event_times_list = []
            for l_s in self.long_sleep:
                duration = l_s.get_duration_sec()
                pre_prob, post_prob, ev_t, _ = l_s.decode_activity_using_pre_post(template_type=template_type,
                                                                                  pre_file_name=pre_file_name, post_file_name=post_file_name, part_to_analyze="rem",
                                                                                  only_stable_cells=only_stable_cells)
                pre_prob_list.extend(pre_prob)
                post_prob_list.extend(post_prob)
                event_times_list.extend(ev_t+first)
                first += duration

            pre_prob_arr = np.vstack(pre_prob_list)
            post_prob_arr = np.vstack(post_prob_list)
            event_times = np.vstack(event_times_list)

            # assign array chunks to events again (either SWR or rem phases)
            event_lengths = [x.shape[0] for x in pre_prob_list]
            pre_prob_z = []
            post_prob_z = []
            first = 0
            for swr_id in range(len(event_lengths)):
                pre_prob_z.append(pre_prob_arr_z[first:first + event_lengths[swr_id], :])
                post_prob_z.append(post_prob_arr_z[first:first + event_lengths[swr_id], :])
                first += event_lengths[swr_id]

            event_pre_post_ratio, event_pre_post_ratio_z, event_pre_prob, event_post_prob, event_len_seq, \
            pop_vec_pre_post_ratio, pre_seq_list, pre_seq_list_z, post_seq_list, pre_seq_list_prob, \
            post_seq_list_prob, pop_vec_post_prob, pop_vec_pre_prob = self.compute_values_from_probabilities(
                pre_prob_list=pre_prob_list, post_prob_list=post_prob_list, post_prob_z_list=post_prob_z,
            pre_prob_z_list=pre_prob_z)

            # smoothen
            # ------------------------------------------------------------------------------------------------------
            event_pre_post_ratio_smooth = moving_average(a=np.array(event_pre_post_ratio), n=n_moving_average_swr)
            event_pre_post_ratio_smooth_z = moving_average(a=np.array(event_pre_post_ratio_z), n=n_moving_average_swr)
            pop_vec_pre_post_ratio = np.array(pop_vec_pre_post_ratio)
            # compute moving average to smooth signal
            pop_vec_pre_post_ratio_smooth = moving_average(a=pop_vec_pre_post_ratio, n=n_moving_average_pop_vec)
            event_len_seq_smooth = moving_average(a=np.array(event_len_seq), n=n_moving_average_swr)

            # plot per nrem phase
            start = 0
            for rem_length, rem_time in zip(event_lengths, event_times):
                if start + rem_length > pop_vec_pre_post_ratio_smooth.shape[0]:
                    continue
                ax.plot(np.linspace(rem_time[0], rem_time[1], rem_length),
                        pop_vec_pre_post_ratio_smooth[start:start+rem_length],c="red", label="REM", alpha=0.5)
                start += rem_length

            handles, labels = ax.get_legend_handles_labels()
            by_label = OrderedDict(zip(labels, handles))
            ax.legend(by_label.values(), by_label.keys())
            plt.xlabel("TIME / s")
            plt.ylabel("PRE_POST SIMILARITY")
            plt.ylim(-1,1)
            plt.grid()
            plt.show()

        else:
            first = 0
            pre_prob_list = []
            post_prob_list = []
            event_times_list = []
            for l_s in self.long_sleep:
                duration = l_s.get_duration_sec()
                pre_prob, post_prob, ev_t, _ = l_s.decode_activity_using_pre_post(template_type=template_type,
                                                                                  pre_file_name=pre_file_name, post_file_name=post_file_name, part_to_analyze=part_to_analyze)
                pre_prob_list.extend(pre_prob)
                post_prob_list.extend(post_prob)
                event_times_list.extend(ev_t+first)
                first += duration

            pre_prob_arr = np.vstack(pre_prob_list)
            post_prob_arr = np.vstack(post_prob_list)

            # z-scoring of probabilites
            pre_prob_arr_z = zscore(pre_prob_arr, axis=0)
            post_prob_arr_z = zscore(post_prob_arr, axis=0)

            # assign array chunks to events again (either SWR or rem phases)
            event_lengths = [x.shape[0] for x in pre_prob_list]
            pre_prob_z = []
            post_prob_z = []
            first = 0
            for swr_id in range(len(event_lengths)):
                pre_prob_z.append(pre_prob_arr_z[first:first + event_lengths[swr_id], :])
                post_prob_z.append(post_prob_arr_z[first:first + event_lengths[swr_id], :])
                first += event_lengths[swr_id]

            nr_modes_pre = pre_prob_list[0].shape[1]
            nr_modes_post = post_prob_list[0].shape[1]

            # per event results
            event_pre_post_ratio = []
            event_pre_post_ratio_z = []
            event_pre_prob = []
            event_post_prob = []
            event_len_seq = []

            # per population vector results
            pop_vec_pre_post_ratio = []
            pre_seq_list = []
            pre_seq_list_z = []
            post_seq_list = []
            pre_seq_list_prob = []
            post_seq_list_prob = []
            pop_vec_post_prob = []
            pop_vec_pre_prob = []

            # go trough all events
            for pre_array, post_array, pre_array_z, post_array_z in zip(pre_prob_list, post_prob_list, pre_prob_z,
                                                                        post_prob_z):
                # make sure that there is any data for the current SWR
                if pre_array.shape[0] > 0:
                    pre_sequence = np.argmax(pre_array, axis=1)
                    pre_sequence_z = np.argmax(pre_array_z, axis=1)
                    pre_sequence_prob = np.max(pre_array, axis=1)
                    post_sequence = np.argmax(post_array, axis=1)
                    post_sequence_prob = np.max(post_array, axis=1)
                    pre_seq_list_z.extend(pre_sequence_z)
                    pre_seq_list.extend(pre_sequence)
                    post_seq_list.extend(post_sequence)
                    pre_seq_list_prob.extend(pre_sequence_prob)
                    post_seq_list_prob.extend(post_sequence_prob)

                    # check how likely observed sequence is considering transitions from model (awake behavior)
                    mode_before = pre_sequence[:-1]
                    mode_after = pre_sequence[1:]
                    event_len_seq.append(pre_sequence.shape[0])

                    # per SWR computations
                    # ----------------------------------------------------------------------------------------------
                    # arrays: [nr_pop_vecs_per_SWR, nr_time_spatial_time_bins]
                    # get maximum value per population vector and take average across the SWR
                    if pre_array.shape[0] > 0:
                        # save pre and post probabilities
                        event_pre_prob.append(np.mean(np.max(pre_array, axis=1)))
                        event_post_prob.append(np.mean(np.max(post_array, axis=1)))
                        # compute ratio by picking "winner" mode by first comparing z scored probabilities
                        # then the probability of the most over expressed mode (highest z-score) is used
                        pre_sequence_z = np.argmax(pre_array_z, axis=1)
                        prob_pre_z = np.mean(pre_array[:, pre_sequence_z])
                        post_sequence_z = np.argmax(post_array_z, axis=1)
                        prob_post_z = np.mean(post_array[:, post_sequence_z])
                        event_pre_post_ratio_z.append((prob_post_z - prob_pre_z) / (prob_post_z + prob_pre_z))

                        # compute ratio using probabilites
                        prob_pre = np.mean(np.max(pre_array, axis=1))
                        prob_post = np.mean(np.max(post_array, axis=1))
                        event_pre_post_ratio.append((prob_post - prob_pre) / (prob_post + prob_pre))
                    else:
                        event_pre_prob.append(np.nan)
                        event_post_prob.append(np.nan)
                        event_pre_post_ratio.append(np.nan)

                    # per population vector computations
                    # ----------------------------------------------------------------------------------------------
                    # compute per population vector similarity score
                    prob_post = np.max(post_array, axis=1)
                    prob_pre = np.max(pre_array, axis=1)
                    pop_vec_pre_post_ratio.extend((prob_post - prob_pre) / (prob_post + prob_pre))

                    if pre_array.shape[0] > 0:
                        pop_vec_pre_prob.extend(np.max(pre_array, axis=1))
                        pop_vec_post_prob.extend(np.max(post_array, axis=1))
                    else:
                        pop_vec_pre_prob.extend([np.nan])
                        pop_vec_post_prob.extend([np.nan])

            pop_vec_pre_prob = np.array(pop_vec_pre_prob)
            pop_vec_post_prob = np.array(pop_vec_post_prob)
            pop_vec_pre_post_ratio = np.array(pop_vec_pre_post_ratio)
            pre_seq_list = np.array(pre_seq_list)

            r_to_plot = range(0, 200)
            plt.figure(figsize=(10, 15))
            plt.subplot(3, 1, 1)
            plt.plot(pop_vec_pre_prob[r_to_plot], label="MAX. PROB. PRE")
            plt.plot(pop_vec_post_prob[r_to_plot], label="MAX. PROB. POST")
            # plt.plot(pre_SWR_prob_arr[r_to_plot, 10], c="r", label="PROB. MODE 60")
            plt.legend()
            plt.ylabel("PROB")
            plt.grid()
            plt.yscale("log")
            plt.subplot(3, 1, 2)
            plt.scatter(r_to_plot, pop_vec_pre_post_ratio[r_to_plot], c="magenta")
            plt.ylabel("PRE_POST RATIO")
            plt.grid()
            plt.subplot(3, 1, 3)
            plt.scatter(r_to_plot, pre_seq_list[r_to_plot], c="y")
            plt.ylabel("PRE MODE ID")
            plt.grid()
            plt.xlabel("POP. VEC. ID")
            plt.show()

            # smoothen
            # ------------------------------------------------------------------------------------------------------
            event_pre_post_ratio_smooth = moving_average(a=np.array(event_pre_post_ratio), n=n_moving_average_swr)
            event_pre_post_ratio_smooth_z = moving_average(a=np.array(event_pre_post_ratio_z), n=n_moving_average_swr)
            pop_vec_pre_post_ratio = np.array(pop_vec_pre_post_ratio)
            # compute moving average to smooth signal
            pop_vec_pre_post_ratio_smooth = moving_average(a=pop_vec_pre_post_ratio, n=n_moving_average_pop_vec)
            event_len_seq_smooth = moving_average(a=np.array(event_len_seq), n=n_moving_average_swr)

            # compute per mode info
            # ------------------------------------------------------------------------------------------------------
            pre_seq = np.array(pre_seq_list)
            post_seq = np.array(post_seq_list)
            mode_score_mean_pre = np.zeros(pre_prob_list[0].shape[1])
            mode_score_std_pre = np.zeros(pre_prob_list[0].shape[1])
            mode_score_mean_post = np.zeros(post_prob_list[0].shape[1])
            mode_score_std_post = np.zeros(post_prob_list[0].shape[1])

            # go through all pre modes and check the average score
            for i in range(pre_prob_list[0].shape[1]):
                ind_sel = np.where(pre_seq == i)[0]
                if ind_sel.size == 0 or ind_sel.size == 1:
                    mode_score_mean_pre[i] = np.nan
                    mode_score_std_pre[i] = np.nan
                else:
                    # delete all indices that are too large (becaue of moving average)
                    ind_sel = ind_sel[ind_sel < pop_vec_pre_post_ratio.shape[0]]
                    mode_score_mean_pre[i] = np.mean(pop_vec_pre_post_ratio[ind_sel])
                    mode_score_std_pre[i] = np.std(pop_vec_pre_post_ratio[ind_sel])

            # go through all post modes and check the average score
            for i in range(post_prob_list[0].shape[1]):
                ind_sel = np.where(post_seq == i)[0]
                if ind_sel.size == 0 or ind_sel.size == 1:
                    mode_score_mean_post[i] = np.nan
                    mode_score_std_post[i] = np.nan
                else:
                    # delete all indices that are too large (becaue of moving average)
                    ind_sel = ind_sel[ind_sel < pop_vec_pre_post_ratio.shape[0]]
                    mode_score_mean_post[i] = np.mean(pop_vec_pre_post_ratio[ind_sel])
                    mode_score_std_post[i] = np.std(pop_vec_pre_post_ratio[ind_sel])

            low_score_modes = np.argsort(mode_score_mean_pre)
            # need to skip nans
            nr_nans = np.count_nonzero(np.isnan(mode_score_mean_pre))
            high_score_modes = np.flip(low_score_modes)[nr_nans:]

            # check if modes get more often/less often reactivated over time
            pre_seq_list = np.array(pre_seq_list)
            nr_pop_vec = 20
            nr_windows = int(pre_seq_list.shape[0] / nr_pop_vec)
            occurence_modes_pre = np.zeros((nr_modes_pre, nr_windows))
            for i in range(nr_windows):
                seq = pre_seq_list[i * nr_pop_vec:(i + 1) * nr_pop_vec]
                mode, counts = np.unique(seq, return_counts=True)
                occurence_modes_pre[mode, i] = counts

            # check if modes get more often/less often reactivated over time
            post_seq_list = np.array(post_seq_list)
            nr_pop_vec = 20
            nr_windows = int(post_seq_list.shape[0] / nr_pop_vec)
            occurence_modes_post = np.zeros((nr_modes_pre, nr_windows))
            for i in range(nr_windows):
                seq = post_seq_list[i * nr_pop_vec:(i + 1) * nr_pop_vec]
                mode, counts = np.unique(seq, return_counts=True)
                occurence_modes_post[mode, i] = counts

            # plot similarity scores for SWR & pop vec
            # ------------------------------------------------------------------------------------------------------
            plt.plot(event_pre_post_ratio_smooth, c="r", label="n_mov_avg = " + str(n_moving_average_swr))
            plt.title("PRE-POST RATIO FOR EACH EVENT: PHMM")
            if part_to_analyze == "nrem":
                plt.xlabel("SWR ID")
            elif part_to_analyze == "rem":
                plt.xlabel("REM PHASE ID")
            plt.ylabel("PRE-POST SIMILARITY")
            plt.ylim(-1, 1)
            plt.grid()
            plt.legend()
            plt.show()

            plt.plot(event_pre_post_ratio_smooth_z, c="r", label="n_mov_avg = " + str(n_moving_average_swr))
            plt.title("PRE-POST RATIO FOR EACH EVENT: PHMM\n Z-SCORED TO SELECT WINNER")
            if part_to_analyze == "nrem":
                plt.xlabel("SWR ID")
            elif part_to_analyze == "rem":
                plt.xlabel("REM PHASE ID")
            plt.ylabel("PRE-POST SIMILARITY")
            plt.ylim(-1, 1)
            plt.grid()
            plt.legend()
            plt.show()

            plt.plot(pop_vec_pre_post_ratio_smooth, label="n_mov_avg = " + str(n_moving_average_pop_vec))
            plt.title("PRE-POST RATIO FOR EACH POP. VECTOR: PHMM")
            plt.xlabel("POP.VEC. ID")
            plt.ylabel("PRE-POST SIMILARITY")
            plt.ylim(-1, 1)
            plt.grid()
            plt.legend()
            plt.show()

            plt.plot(event_len_seq_smooth, label="n_mov_avg = " + str(n_moving_average_swr))
            plt.title("EVENT LENGTH")
            if part_to_analyze == "nrem":
                plt.xlabel("SWR ID")
            elif part_to_analyze == "rem":
                plt.xlabel("REM PHASE ID")
            plt.ylabel("#POP.VEC. PER SWR")
            plt.grid()
            plt.legend()
            plt.show()

            plt.imshow(occurence_modes_pre, interpolation='nearest', aspect='auto')
            plt.ylabel("MODE ID")
            plt.xlabel("WINDOW ID")
            a = plt.colorbar()
            a.set_label("#WINS/" + str(nr_pop_vec) + " POP. VEC. WINDOW")
            plt.title("PRE: OCCURENCE (#WINS) OF MODES IN WINDOWS OF FIXED LENGTH")
            plt.show()

            plt.imshow(occurence_modes_post, interpolation='nearest', aspect='auto')
            plt.ylabel("MODE ID")
            plt.xlabel("WINDOW ID")
            a = plt.colorbar()
            a.set_label("#WINS/" + str(nr_pop_vec) + " POP. VEC. WINDOW")
            plt.title("POST: OCCURENCE (#WINS) OF MODES IN WINDOWS OF FIXED LENGTH")
            plt.show()

            plt.errorbar(range(pre_prob[0].shape[1]), mode_score_mean_pre, yerr=mode_score_std_pre,
                         linestyle="")
            plt.scatter(range(pre_prob[0].shape[1]), mode_score_mean_pre)
            plt.title("PRE-POST SCORE PER MODE: PRE")
            plt.xlabel("MODE ID")
            plt.ylabel("PRE-POST SCORE: MEAN AND STD")
            plt.show()

            plt.errorbar(range(post_prob[0].shape[1]), mode_score_mean_post, yerr=mode_score_std_post,
                         linestyle="")
            plt.scatter(range(post_prob[0].shape[1]), mode_score_mean_post)
            plt.title("PRE-POST SCORE PER MODE: POST")
            plt.xlabel("MODE ID")
            plt.ylabel("PRE-POST SCORE: MEAN AND STD")
            plt.show()

    # population vector analysis
    # ------------------------------------------------------------------------------------------------------------------

    def firing_rate_changes(self):

        # get stable, decreasing, increasing cells

        with open(
                self.params.pre_proc_dir + "cell_classification/" + self.params.session_name + "_k_means.pickle",
                "rb") as f:
            class_dic = pickle.load(f)

        stable_ids = class_dic["stable_cell_ids"]
        inc_ids = class_dic["increase_cell_ids"]
        dec_ids = class_dic["decrease_cell_ids"]

        # get REM and NREM rasters & times

        rem_rasters = []
        nrem_rasters = []
        event_times_rem = []
        event_end_times_rem = []
        event_times_nrem = []
        event_end_times_nrem = []

        first = 0
        for l_s in self.long_sleep:
            duration = l_s.get_duration_sec()
            # all_event_rasters = all_event_rasters + l_s.get_event_spike_rasters(part_to_analyze=part_to_analyze)[0]
            ls_rem_raster, _, start_times_rem, end_times_rem = l_s.get_event_spike_rasters(part_to_analyze="rem",
                                                                                return_event_times=True)
            ls_nrem_raster, _, start_times_nrem, end_times_nrem = l_s.get_event_spike_rasters(part_to_analyze="nrem",
                                                            return_event_times=True)
            rem_rasters.extend(ls_rem_raster)
            nrem_rasters.extend(ls_nrem_raster)

            event_times_rem.extend(first+start_times_rem)
            event_times_nrem.extend(first + start_times_nrem)
            event_end_times_rem.extend(first + end_times_rem)
            event_end_times_nrem.extend(first + end_times_nrem)
            first += duration

        # TODO: filter REM/NREM that are too short

        event_times_nrem = np.vstack(event_times_nrem)
        event_times_rem = np.vstack(event_times_rem)

        # combine rem and nrem data to order it in the right way (chronologically)
        # --------------------------------------------------------------------------------------------------------------

        all_events_raster = rem_rasters + nrem_rasters
        labels_events = np.zeros(len(all_events_raster))
        labels_events[:len(rem_rasters)] = 1
        all_times = np.vstack((event_times_rem, event_times_nrem))
        all_end_times = np.hstack((event_end_times_rem, event_end_times_nrem))


        # sort events according to time
        sorted_events_raster = [x for _, x in sorted(zip(all_times, all_events_raster))]
        sorted_labels_events = [x for _, x in sorted(zip(all_times, labels_events))]
        sorted_times = [x for x in sorted(all_times)]
        sorted_end_times = [x for _, x in sorted(zip(all_times, all_end_times))]


        # merge neighbouring epochs if they have the same label (rem events == 1, nrem events == 0)
        merged_event_rasters = []
        merged_event_labels = []
        tmp_merged_event_labels = []
        tmp_merged_event_time = np.zeros(2)
        prev_end_time = None
        tmp_merged_event_raster = np.empty((sorted_events_raster[0].shape[0],0))
        prev = sorted_labels_events[0]
        tmp_merged_event_time[0] = sorted_times[0]
        merged_events_times = []
        for label, raster, start_time, end_time in zip(sorted_labels_events, sorted_events_raster,
                                                       sorted_times, sorted_end_times):
            if label == prev:
                # need to merge the two
                tmp_merged_event_raster = np.hstack((tmp_merged_event_raster, raster))
                tmp_merged_event_labels.append(label)
            else:
                # add last merged event
                merged_event_rasters.append(tmp_merged_event_raster)
                merged_event_labels.append(tmp_merged_event_labels)
                # get end time from prev. event (is the last event belonging to the merged event)
                tmp_merged_event_time[1] = prev_end_time
                merged_events_times.append(tmp_merged_event_time)
                # need to start with new merged event
                tmp_merged_event_raster = np.empty((sorted_events_raster[0].shape[0],0))
                tmp_merged_event_labels = []
                tmp_merged_event_time = np.zeros(2)
                # need start time since it is a new merged event
                tmp_merged_event_time[0] = start_time
                tmp_merged_event_raster = np.hstack((tmp_merged_event_raster, raster))
                tmp_merged_event_labels.append(label)
                # remember current end time, in case this is the last event belonging to the merged event
            prev_end_time = end_time
            prev = label

        len_new_events = [x.shape[1] for x in merged_event_rasters]
        merged_event_labels = [x[0] for x in merged_event_labels]
        one_raster_all_events = np.hstack(merged_event_rasters)
        one_raster_all_events_stable = one_raster_all_events[stable_ids, :]
        one_raster_all_events_inc = one_raster_all_events[inc_ids, :]
        one_raster_all_events_dec = one_raster_all_events[dec_ids, :]

        one_raster_all_events_dec_mean = np.mean(one_raster_all_events_dec, axis=0)
        one_raster_all_events_dec_mean_smooth = moving_average(one_raster_all_events_dec_mean, n=500)

        one_raster_all_events_inc_mean = np.mean(one_raster_all_events_inc, axis=0)
        one_raster_all_events_inc_mean_smooth = moving_average(one_raster_all_events_inc_mean, n=500)

        one_raster_all_events_stable_mean = np.mean(one_raster_all_events_stable, axis=0)
        one_raster_all_events_stable_mean_smooth = moving_average(one_raster_all_events_stable_mean, n=500)

        dec_smooth_rem = []
        dec_smooth_nrem = []
        dec_smooth = []
        dec_smooth_delta = []
        inc_smooth_rem = []
        inc_smooth_nrem = []
        inc_smooth = []
        inc_smooth_delta = []
        stable_smooth_rem = []
        stable_smooth_nrem = []
        stable_smooth = []
        stable_smooth_delta = []

        start = 0

        for event_len, label in zip(len_new_events, merged_event_labels):

            end = min(start + event_len, one_raster_all_events_dec_mean_smooth.shape[0]-1)
            # check if start still lies within rasterstart

            if (start >= one_raster_all_events_dec_mean_smooth.shape[0]):
                break

            # check which label (rem == 1, nrem == 0)
            if label == 1:
                dec_smooth_rem.append(one_raster_all_events_dec_mean_smooth[start:end])
                inc_smooth_rem.append(one_raster_all_events_inc_mean_smooth[start:end])
                stable_smooth_rem.append(one_raster_all_events_stable_mean_smooth[start:end])
            else:
                dec_smooth_nrem.append(one_raster_all_events_dec_mean_smooth[start:end])
                inc_smooth_nrem.append(one_raster_all_events_inc_mean_smooth[start:end])
                stable_smooth_nrem.append(one_raster_all_events_stable_mean_smooth[start:end])

            dec_smooth.append(one_raster_all_events_dec_mean_smooth[start:end])
            dec_smooth_delta.append(one_raster_all_events_dec_mean_smooth[end]-one_raster_all_events_dec_mean_smooth[start])
            inc_smooth.append(one_raster_all_events_inc_mean_smooth[start:end])
            inc_smooth_delta.append(one_raster_all_events_inc_mean_smooth[end]-one_raster_all_events_inc_mean_smooth[start])
            stable_smooth.append(one_raster_all_events_stable_mean_smooth[start:end])
            stable_smooth_delta.append(one_raster_all_events_stable_mean_smooth[end]-one_raster_all_events_stable_mean_smooth[start])

            start = end

        first = 0

        # might need to delete last label
        merged_event_labels = merged_event_labels[:len(dec_smooth)]

        # plot results
        # --------------------------------------------------------------------------------------------------------------

        # correlation of delta average firing rate and epoch length
        # compute duration of event
        merged_times = np.vstack(merged_events_times)
        merged_dur = merged_times[:, 1] - merged_times[:, 0]

        plt.scatter(merged_dur, np.array(dec_smooth_delta), label="DEC. CELLS, R = "+str(
            np.round(pearsonr(merged_dur, np.array(dec_smooth_delta))[0],2)), marker="_", color="lightcoral")
        plt.scatter(merged_dur, np.array(inc_smooth_delta), label="INC. CELLS, R = "+str(
            np.round(pearsonr(merged_dur, np.array(inc_smooth_delta))[0],2)), marker="+", color="cornflowerblue")
        plt.scatter(merged_dur, np.array(stable_smooth_delta), label="STABLE. CELLS, R = "+str(
            np.round(pearsonr(merged_dur, np.array(stable_smooth_delta))[0],2)), s=2.5, color="white")
        plt.xlabel("DURATION EPOCH (s)")
        plt.ylabel("DELTA MEAN FIRING")
        plt.legend()
        plt.title("DURATION EPOCH VS. DELTA MEAN FIRING")
        plt.show()

        thresh_dur = 3000

        plt.scatter(merged_dur[merged_dur < thresh_dur], np.array(dec_smooth_delta)[merged_dur < thresh_dur], label="DEC. CELLS, R = "+str(
        np.round(pearsonr(merged_dur[merged_dur < thresh_dur], np.array(dec_smooth_delta)[merged_dur < thresh_dur])[0],2)), marker="_", color="lightcoral")
        plt.scatter(merged_dur[merged_dur < thresh_dur], np.array(inc_smooth_delta)[merged_dur < thresh_dur], label="INC. CELLS, R = "+str(
            np.round(pearsonr(merged_dur[merged_dur < thresh_dur], np.array(inc_smooth_delta)[merged_dur < thresh_dur])[0],2)), marker="+", color="cornflowerblue")
        plt.scatter(merged_dur[merged_dur < thresh_dur], np.array(stable_smooth_delta)[merged_dur < thresh_dur], label="STABLE. CELLS, R = "+str(
            np.round(pearsonr(merged_dur[merged_dur < thresh_dur], np.array(stable_smooth_delta)[merged_dur < thresh_dur])[0],2)), s=2.5, color="white")
        plt.xlabel("DURATION EPOCH (s)")
        plt.ylabel("DELTA MEAN FIRING")
        plt.legend()
        plt.title("DURATION EPOCH VS. DELTA MEAN FIRING\n DURATION < "+str(thresh_dur)+"s")
        plt.show()

        # REM vs. NREM
        merged_event_labels_arr = np.array(merged_event_labels)
        rem_dec_smooth = np.array(dec_smooth_delta)[merged_event_labels_arr == 1]
        nrem_dec_smooth = np.array(dec_smooth_delta)[merged_event_labels_arr == 0]
        plt.hist(rem_dec_smooth, color="r", label="REM", orientation='horizontal', density=True)
        plt.hist(nrem_dec_smooth, color="b", label="NREM", alpha=0.5, orientation='horizontal', density=True)
        plt.title("DELTA MEAN FIRING: DECREASING CELLS")
        plt.legend()
        plt.xlabel("DENSITY")
        plt.ylabel("DELTA MEAN FIRING PER EPOCH")
        plt.ylim(-max(abs(np.array(dec_smooth_delta))), max(abs(np.array(dec_smooth_delta))))
        plt.show()

        rem_inc_smooth = np.array(inc_smooth_delta)[merged_event_labels_arr == 1]
        nrem_inc_smooth = np.array(inc_smooth_delta)[merged_event_labels_arr == 0]
        plt.hist(rem_inc_smooth, color="r", label="REM", orientation='horizontal', density=True)
        plt.hist(nrem_inc_smooth, color="b", label="NREM", alpha=0.5, orientation='horizontal', density=True)
        plt.title("DELTA MEAN FIRING: INCREASING CELLS")
        plt.legend()
        plt.xlabel("DENSITY")
        plt.ylabel("DELTA MEAN FIRING PER EPOCH")
        plt.ylim(-max(abs(np.array(inc_smooth_delta))), max(abs(np.array(inc_smooth_delta))))
        plt.show()

        rem_stable_smooth = np.array(stable_smooth_delta)[merged_event_labels_arr == 1]
        nrem_stable_smooth = np.array(stable_smooth_delta)[merged_event_labels_arr == 0]
        plt.hist(rem_stable_smooth, color="r", label="REM", orientation='horizontal', density=True)
        plt.hist(nrem_stable_smooth, color="b", label="NREM", alpha=0.5, orientation='horizontal', density=True)
        plt.title("DELTA MEAN FIRING: STABLE CELLS")
        plt.legend()
        plt.xlabel("DENSITY")
        plt.ylabel("DELTA MEAN FIRING PER EPOCH")
        plt.ylim(-max(abs(np.array(stable_smooth_delta))), max(abs(np.array(stable_smooth_delta))))
        plt.show()


        max_len = min(rem_dec_smooth.shape[0], nrem_dec_smooth.shape[0])
        plt.scatter(rem_dec_smooth[:max_len], nrem_dec_smooth[:max_len])
        plt.title("NEIGBOURING PERIODS: DECREASING CELLS\nR="+str(np.round(pearsonr(
            rem_dec_smooth[:max_len], nrem_dec_smooth[:max_len])[0],2)))
        plt.xlabel("DELTA MEAN FIRING REM")
        plt.ylabel("DELTA MEAN FIRING NREM")
        plt.show()

        plt.scatter(rem_inc_smooth[:max_len], nrem_inc_smooth[:max_len])
        plt.title("NEIGBOURING PERIODS: INCREASING CELLS\nR="+str(np.round(pearsonr(
            rem_inc_smooth[:max_len], nrem_inc_smooth[:max_len])[0],2)))
        plt.xlabel("DELTA MEAN FIRING REM")
        plt.ylabel("DELTA MEAN FIRING NREM")
        plt.show()

        plt.scatter(rem_stable_smooth[:max_len], nrem_stable_smooth[:max_len])
        plt.title("NEIGBOURING PERIODS: STABLE CELLS\nR="+str(np.round(pearsonr(
            rem_stable_smooth[:max_len], nrem_stable_smooth[:max_len])[0],2)))
        plt.xlabel("DELTA MEAN FIRING REM")
        plt.ylabel("DELTA MEAN FIRING NREM")
        plt.show()



        fig = plt.figure()
        ax = fig.add_subplot()
        first = 0
        for label, res in zip(merged_event_labels, dec_smooth):
            # rem
            if label == 1:
                ax.plot(first+np.arange(res.shape[0]), res, c="r", label="REM")
            else:
                ax.plot(first + np.arange(res.shape[0]), res, c="b", label="NREM")
            first += res.shape[0]
        handles, labels = ax.get_legend_handles_labels()
        by_label = OrderedDict(zip(labels, handles))
        ax.legend(by_label.values(), by_label.keys())
        plt.xlabel("POP. VEC ID")
        plt.ylabel("MEAN FIRING RATE")
        plt.title("DECREASING CELLS")
        plt.show()

        fig = plt.figure()
        ax = fig.add_subplot()
        first = 0
        for label, res in zip(merged_event_labels, inc_smooth):
            # rem
            if label == 1:
                ax.plot(first+np.arange(res.shape[0]), res, c="r", label="REM")
            else:
                ax.plot(first + np.arange(res.shape[0]), res, c="b", label="NREM")
            first += res.shape[0]
        handles, labels = ax.get_legend_handles_labels()
        by_label = OrderedDict(zip(labels, handles))
        ax.legend(by_label.values(), by_label.keys())
        plt.xlabel("POP. VEC ID")
        plt.ylabel("MEAN FIRING RATE")
        plt.title("INCREASING CELLS")
        plt.show()

        fig = plt.figure()
        ax = fig.add_subplot()
        first = 0
        for label, res in zip(merged_event_labels, stable_smooth):
            # rem
            if label == 1:
                ax.plot(first+np.arange(res.shape[0]), res, c="r", label="REM")
            else:
                ax.plot(first + np.arange(res.shape[0]), res, c="b", label="NREM")
            first += res.shape[0]
        handles, labels = ax.get_legend_handles_labels()
        by_label = OrderedDict(zip(labels, handles))
        ax.legend(by_label.values(), by_label.keys())
        plt.xlabel("POP. VEC ID")
        plt.ylabel("MEAN FIRING RATE")
        plt.title("STABLE CELLS")
        plt.show()

        fig = plt.figure()
        ax = fig.add_subplot()
        # plot results
        first = 0
        for label, res in zip(merged_event_labels, dec_smooth_delta):
            # rem
            if label == 1:
                ax.scatter(first, res, c="r", label="REM")
            else:
                ax.scatter(first, res, c="b", label="NREM")
            first += 1

        handles, labels = ax.get_legend_handles_labels()
        by_label = OrderedDict(zip(labels, handles))
        ax.legend(by_label.values(), by_label.keys())
        plt.title("DECREASING CELLS")
        plt.ylabel("DELTA IN MEAN FIRING RATE")
        plt.xlabel("EPOCH ID")
        plt.show()

        fig = plt.figure()
        ax = fig.add_subplot()
        first = 0
        for label, res in zip(merged_event_labels, inc_smooth_delta):
            # rem
            if label == 1:
                ax.scatter(first, res, c="r", label="REM")
            else:
                ax.scatter(first, res, c="b", label="NREM")
            first += 1

        handles, labels = ax.get_legend_handles_labels()
        by_label = OrderedDict(zip(labels, handles))
        ax.legend(by_label.values(), by_label.keys())
        plt.title("INCREASING CELLS")
        plt.ylabel("DELTA IN MEAN FIRING RATE")
        plt.xlabel("EPOCH ID")
        plt.show()
        #
        fig = plt.figure()
        ax = fig.add_subplot()
        first = 0
        for label, res in zip(merged_event_labels, stable_smooth_delta):
            # rem
            if label == 1:
                ax.scatter(first, res, c="r", label="REM")
            else:
                ax.scatter(first, res, c="b", label="NREM")
            first += 1

        handles, labels = ax.get_legend_handles_labels()
        by_label = OrderedDict(zip(labels, handles))
        ax.legend(by_label.values(), by_label.keys())
        plt.title("STABLE CELLS")
        plt.ylabel("DELTA IN MEAN FIRING RATE")
        plt.xlabel("EPOCH ID")
        plt.show()

        exit()

        for event in merged_event_rasters:
            stable = event[stable_ids,:]
            inc = event[inc_ids,:]
            dec = event[dec_ids,:]

            stable_mean = np.mean(stable, axis=0)
            inc_mean = np.mean(inc, axis=0)
            dec_mean = np.mean(dec, axis=0)

            plt.scatter(first+np.arange(event.shape[1]), inc_mean)
            plt.show()
            first += event.shape[1]

        plt.show()










        exit()
        # compute labels per population vector --> rem events = 1, nrem events = 0
        # --------------------------------------------------------------------------------------------------------------
        labels_per_pop_vec = []
        for len_event, label in zip(sorted_lengths, sorted_labels_events):
            if label:
                labels_per_pop_vec.extend(np.ones(len_event))
            elif label == 0:
                labels_per_pop_vec.extend(np.zeros(len_event))

        labels_per_pop_vec = np.array(labels_per_pop_vec)

        # concatenate events that have the same label
        # --------------------------------------------------------------------------------------------------------------
        sorted_labels_events = np.array(sorted_labels_events)
        label_change = np.diff(sorted_labels_events)
        merged_events_labels = []
        merged_events_times = []
        first = 0
        while True:
            dat = label_change[first:]
            if first > label_change.shape[0]:
                break
            if sorted_labels_events[first] == 0:
                # nrem event
                # check if this is the last event
                if np.where(dat == 1)[0].shape[0] == 0:
                    trans = label_change.shape[0] - first + 1
                else:
                    trans = np.where(dat == 1)[0][0] + 1
                merged_events_labels.append(np.unique(sorted_labels_events[first:first + trans]))
                merged_events_times.append([sorted_times[first], sorted_end_times[first + trans - 1]])
            elif sorted_labels_events[first] == 1:
                # rem event
                # check if this is the last event
                if np.where(dat == -1)[0].shape[0] == 0:
                    trans = label_change.shape[0] - first + 1
                else:
                    trans = np.where(dat == -1)[0][0] + 1
                merged_events_labels.append(np.unique(sorted_labels_events[first:first + trans]))
                merged_events_times.append([sorted_times[first], sorted_end_times[first + trans - 1]])
            first += trans

        merged_events_labels = np.squeeze(np.array(merged_events_labels))

        # concatenate population vectors that have the same label: TODO: there might be a better way to combine this
        # step and the previous step
        # --------------------------------------------------------------------------------------------------------------
        new_event_time_stamps = np.abs(np.diff(labels_per_pop_vec))

        # need to add one element at the beginning
        new_event_time_stamps = np.insert(new_event_time_stamps, 0, 1)
        start = np.nonzero(new_event_time_stamps == 1)[0]
        end = start[1:]
        end = np.append(end, new_event_time_stamps.shape[0])

        # only select data (pre_post ratio) from population vectors that belong to selected rem/nrem events
        # --> results per merged event
        # --------------------------------------------------------------------------------------------------------------
        pre_prob_per_merged_rem_event = []
        pre_prob_per_merged_nrem_event = []
        pre_prob_rem_nrem_events = []
        rem_nrem_events_label = []
        pre_prob_rem_nrem_pop_vec = []
        rem_nrem_pop_vec_label = []

        for start_event, end_event in zip(start, end):
            # rem event
            if labels_per_pop_vec[start_event + 1] == 1:
                pre_prob_per_merged_rem_event.append(sorted_pre_prob[start_event:end_event])
            # nrem event
            else:
                pre_prob_per_merged_nrem_event.append(sorted_pre_prob[start_event:end_event])

            pre_prob_rem_nrem_events.append(sorted_pre_prob[start_event:end_event])
            rem_nrem_events_label.extend([labels_per_pop_vec[start_event + 1]])

            pre_prob_rem_nrem_pop_vec.extend(sorted_pre_prob[start_event:end_event])
            rem_nrem_pop_vec_label.extend(labels_per_pop_vec[start_event:end_event])

        merged_events_times = np.vstack(merged_events_times)

        merged_rem_event_times = merged_events_times[merged_events_labels == 1]
        merged_nrem_event_times = merged_events_times[merged_events_labels == 0]

        # take samples from each epoch --> otherwise MDS won't work (too much data)
        pre_prob_per_merged_rem_event_samples = []
        for epoch in pre_prob_per_merged_rem_event:
            sample_ind = np.random.randint(0, epoch.shape[0], size=samples_per_epoch)
            pre_prob_per_merged_rem_event_samples.append(epoch[sample_ind, :])

        pre_prob_per_merged_nrem_event_samples = []
        for epoch in pre_prob_per_merged_nrem_event:
            sample_ind = np.random.randint(0, epoch.shape[0], size=samples_per_epoch)
            pre_prob_per_merged_nrem_event_samples.append(epoch[sample_ind, :])

        nr_rem_epochs = len(pre_prob_per_merged_rem_event_samples)
        nr_nrem_epochs = len(pre_prob_per_merged_nrem_event_samples)

    def predict_time_progression_const_spikes(self, part_to_analyze="all"):

        if part_to_analyze == "all":
            raster = []
            times = []
            first = 0
            for l_s in self.long_sleep:
                duration = l_s.get_duration_sec()
                r, t = l_s.get_spike_binned_raster(return_estimated_times=True)
                raster.append(r)
                times.append(t+first)
                first += duration

            raster = np.hstack(raster)
            times = np.hstack(times)

            # load only stable cells
            with open(self.params.pre_proc_dir + "cell_classification/" + self.params.session_name + "_k_means.pickle",
                      "rb") as f:
                class_dic = pickle.load(f)

            stable_ids = class_dic["stable_cell_ids"]
            inc_ids = class_dic["increase_cell_ids"]
            raster_stable = raster[stable_ids, :]
            raster_wo_stable = np.delete(raster, stable_ids, axis=0)

            new_ml = MlMethodsOnePopulation()
            all_r2 = []
            stable_r2 = []
            wo_stable = []

            r2 = new_ml.ridge_time_bin_progress(x=raster, y=times, new_time_bin_size=" 50 SPIKE", alpha=100,
                                                alpha_fitting=False, plotting=True)

            for i in range(15):
                r2 = new_ml.ridge_time_bin_progress(x=raster, y=times, new_time_bin_size="CONST #SPIKES", alpha=100,
                                                    alpha_fitting=False, plotting=False)
                all_r2.append(r2)

            for i in range(15):
                r2 = new_ml.ridge_time_bin_progress(x=raster_wo_stable, y=times, new_time_bin_size="CONST #SPIKES", alpha=100,
                                                    alpha_fitting=False, plotting=False)
                wo_stable.append(r2)

            for i in range(15):
                r2 = new_ml.ridge_time_bin_progress(x=raster_stable, y=times, new_time_bin_size="CONST #SPIKES", alpha=100,
                                                    alpha_fitting=False, plotting=False)
                stable_r2.append(r2)

            c = "white"

            stable_r2 = np.array(stable_r2)
            wo_stable = np.array(wo_stable)
            all_r2 = np.array(all_r2)

            res = np.vstack((all_r2, wo_stable, stable_r2)).T

            bplot = plt.boxplot(res, positions=[1,2,3], patch_artist=True,
                                labels=["ALL", "W/O STABLE", "ONLY STABLE"],
                                boxprops=dict(color=c),
                                capprops=dict(color=c),
                                whiskerprops=dict(color=c),
                                flierprops=dict(color=c, markeredgecolor=c),
                                medianprops=dict(color=c),
                                )
            colors = ["yellow", 'blue', 'red']
            for patch, color in zip(bplot['boxes'], colors):
                patch.set_facecolor(color)
            plt.title("R2 VALUES OF RIDGE REGRESSION")
            plt.ylabel("R2 (15 SPLITS)")
            plt.grid(color="grey", axis="y")
            plt.show()



        else:
            # check how many cells
            nr_cells = self.long_sleep[0].get_raster().shape[0]
            all_event_rasters = []
            all_raster_lengths = []
            start_times = []
            end_times = []

            # need to offset each sleep file by duration of previous sleep files
            first = 0
            for l_s in self.long_sleep:
                duration = l_s.get_duration_sec()
                # all_event_rasters = all_event_rasters + l_s.get_event_spike_rasters(part_to_analyze=part_to_analyze)[0]
                new_event_raster, start_time, end_time = l_s.get_event_time_bin_rasters(part_to_analyze=part_to_analyze,
                                                                                        time_bin_size=0.01)
                all_event_rasters = all_event_rasters + new_event_raster
                start_times.append(start_time+first)
                end_times.append(end_time+first)
                first += duration

            start_times = np.hstack(start_times)
            end_times = np.hstack(end_times)

            new_time_stamps = []

            for event, start, end in zip(all_event_rasters, start_times, end_times):
                new_time_stamps.extend(np.linspace(start, end, event.shape[1]))

            new_time_stamps = np.expand_dims(np.array(new_time_stamps), 0)
            all_event_rasters = np.hstack(all_event_rasters)

            scaler = int(time_bin_size / 0.01)

            down_sampled = down_sample_array_sum(x=all_event_rasters, chunk_size=scaler)
            times_down_sampled = down_sample_array_mean(x=new_time_stamps, chunk_size=scaler)
            times_down_sampled = np.squeeze(times_down_sampled)

            new_ml = MlMethodsOnePopulation()
            new_ml.ridge_time_bin_progress(x=down_sampled, y=times_down_sampled,
                                                           new_time_bin_size=0.5, alpha_fitting=True)

    def predict_time_progression_time_bin(self, part_to_analyze="all", time_bin_size=0.1):

        if part_to_analyze == "all":
            raster = []
            first = 0
            for l_s in self.long_sleep:
                duration = l_s.get_duration_sec()
                r = l_s.get_raster()
                raster.append(r)
                first += duration

            raster = np.hstack(raster)

            scaler = int(time_bin_size / self.params.time_bin_size)

            raster = down_sample_array_sum(x=raster, chunk_size=scaler)

            print(raster.shape)

            #
            times = np.arange(0, raster.shape[1]) * time_bin_size

            # load only stable cells
            with open(self.params.pre_proc_dir + "cell_classification/" + self.params.session_name + "_k_means.pickle",
                      "rb") as f:
                class_dic = pickle.load(f)

            stable_ids = class_dic["stable_cell_ids"]
            inc_ids = class_dic["increase_cell_ids"]
            raster_stable = raster[stable_ids, :]
            raster_wo_stable = np.delete(raster, stable_ids, axis=0)

            new_ml = MlMethodsOnePopulation()
            all_r2 = []
            stable_r2 = []
            wo_stable = []
            for i in range(15):
                r2 = new_ml.ridge_time_bin_progress(x=raster, y=times, new_time_bin_size=time_bin_size, alpha=100,
                                                    alpha_fitting=False, plotting=True)
                all_r2.append(r2)

            for i in range(15):
                r2 = new_ml.ridge_time_bin_progress(x=raster_wo_stable, y=times, new_time_bin_size="CONST #SPIKES",
                                                    alpha=100,
                                                    alpha_fitting=False, plotting=False)
                wo_stable.append(r2)

            for i in range(15):
                r2 = new_ml.ridge_time_bin_progress(x=raster_stable, y=times, new_time_bin_size="CONST #SPIKES",
                                                    alpha=100,
                                                    alpha_fitting=False, plotting=False)
                stable_r2.append(r2)

            c = "white"

            stable_r2 = np.array(stable_r2)
            wo_stable = np.array(wo_stable)
            all_r2 = np.array(all_r2)

            res = np.vstack((all_r2, wo_stable, stable_r2)).T

            bplot = plt.boxplot(res, positions=[1, 2, 3], patch_artist=True,
                                labels=["ALL", "W/O STABLE", "ONLY STABLE"],
                                boxprops=dict(color=c),
                                capprops=dict(color=c),
                                whiskerprops=dict(color=c),
                                flierprops=dict(color=c, markeredgecolor=c),
                                medianprops=dict(color=c),
                                )
            colors = ["yellow", 'blue', 'red']
            for patch, color in zip(bplot['boxes'], colors):
                patch.set_facecolor(color)
            plt.title("R2 VALUES OF RIDGE REGRESSION")
            plt.ylabel("R2 (15 SPLITS)")
            plt.grid(color="grey", axis="y")
            plt.show()


        else:
            # check how many cells
            nr_cells = self.long_sleep[0].get_raster().shape[0]
            all_event_rasters = []
            all_raster_lengths = []
            start_times = []
            end_times = []

            # need to offset each sleep file by duration of previous sleep files
            first = 0
            for l_s in self.long_sleep:
                duration = l_s.get_duration_sec()
                # all_event_rasters = all_event_rasters + l_s.get_event_spike_rasters(part_to_analyze=part_to_analyze)[0]
                new_event_raster, start_time, end_time = l_s.get_event_time_bin_rasters(part_to_analyze=part_to_analyze,
                                                                                        time_bin_size=0.01)
                all_event_rasters = all_event_rasters + new_event_raster
                start_times.append(start_time + first)
                end_times.append(end_time + first)
                first += duration

            start_times = np.hstack(start_times)
            end_times = np.hstack(end_times)

            new_time_stamps = []

            for event, start, end in zip(all_event_rasters, start_times, end_times):
                new_time_stamps.extend(np.linspace(start, end, event.shape[1]))

            new_time_stamps = np.expand_dims(np.array(new_time_stamps), 0)
            all_event_rasters = np.hstack(all_event_rasters)

            scaler = int(time_bin_size / 0.01)

            down_sampled = down_sample_array_sum(x=all_event_rasters, chunk_size=scaler)
            times_down_sampled = down_sample_array_mean(x=new_time_stamps, chunk_size=scaler)
            times_down_sampled = np.squeeze(times_down_sampled)

            new_ml = MlMethodsOnePopulation()
            new_ml.ridge_time_bin_progress(x=down_sampled, y=times_down_sampled,
                                           new_time_bin_size=0.5, alpha_fitting=True)

    def predict_time_progression_time_bin_correlations(self, plot_file_name="test_1", exclude_diagonal=False,
                                                       part_to_analyze="all",
                                                       bins_per_corr_matrix=20, only_stable_cells=False):

        if part_to_analyze == "all":

            # load only stable cells
            if only_stable_cells:
                with open(self.params.pre_proc_dir + "cell_classification/" + self.params.session_name + "_k_means.pickle",
                          "rb") as f:
                    class_dic = pickle.load(f)

                stable_ids = class_dic["stable_cell_ids"]
                print("ONLY STABLE CELLS!!!")

                nr_cells = stable_ids.shape[0]

                if exclude_diagonal is True:
                    corr_mat = np.zeros((int(nr_cells * (nr_cells - 1) / 2), 0))
                else:
                    corr_mat = np.zeros((nr_cells ** 2, 0))

                for l_s in self.long_sleep:
                    c_m = l_s.get_correlation_matrices(bins_per_corr_matrix=bins_per_corr_matrix,
                                                       exclude_diagonal=exclude_diagonal, cell_selection=stable_ids)
                    corr_mat = np.hstack((corr_mat, c_m))

            else:

                nr_cells = self.long_sleep[0].get_nr_cells()

                if exclude_diagonal is True:
                    corr_mat = np.zeros((int(nr_cells * (nr_cells - 1) / 2), 0))
                else:
                    corr_mat = np.zeros((nr_cells ** 2, 0))

                for l_s in self.long_sleep:
                    c_m = l_s.get_correlation_matrices(bins_per_corr_matrix=bins_per_corr_matrix,
                                                       exclude_diagonal=exclude_diagonal)
                    corr_mat = np.hstack((corr_mat, c_m))

            times = np.arange(0, corr_mat.shape[1]) * self.params.time_bin_size * bins_per_corr_matrix

            # only diagonal
            # diagonals = []
            # for mat in corr_mat.T:
            #     print(mat.shape)
            #     diagonals.append(np.diag(np.reshape(mat, (128, 128))))
            # diagonals = np.array(diagonals).T

            # only upper triangle
            triangles = []
            for mat in corr_mat.T:
                a = upper_tri_without_diag(np.reshape(mat, (128, 128)))
                triangles.append(np.hstack((np.random.uniform(0,1,16256-a.shape[0]), a)))
            triangles = np.array(triangles).T

            # whole matrix without diagonal
            # wo_diagonal = []
            # for mat in corr_mat.T:
            #     A = np.reshape(mat, (128, 128))
            #     wo_diagonal.append(A[~np.eye(A.shape[0],dtype=bool)].reshape(A.shape[0],-1).flatten())
            # wo_diagonal = np.array(wo_diagonal).T

            new_ml = MlMethodsOnePopulation()
            print("\nSTARTING RIDGE ... \n")
            r2 = new_ml.ridge_time_bin_progress(x=triangles, y=times, new_time_bin_size="CONST #SPIKES",
                                                    alpha_fitting=True, plotting=True)

            plt.savefig(self.params.pre_proc_dir + plot_file_name)


            exit()
            c = "white"

            stable_r2 = np.array(stable_r2)
            wo_stable = np.array(wo_stable)
            all_r2 = np.array(all_r2)

            res = np.vstack((all_r2, wo_stable, stable_r2)).T

            bplot = plt.boxplot(res, positions=[1, 2, 3], patch_artist=True,
                                labels=["ALL", "W/O STABLE", "ONLY STABLE"],
                                boxprops=dict(color=c),
                                capprops=dict(color=c),
                                whiskerprops=dict(color=c),
                                flierprops=dict(color=c, markeredgecolor=c),
                                medianprops=dict(color=c),
                                )
            colors = ["yellow", 'blue', 'red']
            for patch, color in zip(bplot['boxes'], colors):
                patch.set_facecolor(color)
            plt.title("R2 VALUES OF RIDGE REGRESSION")
            plt.ylabel("R2 (15 SPLITS)")
            plt.grid(color="grey", axis="y")
            plt.show()


        else:
            # check how many cells
            nr_cells = self.long_sleep[0].get_raster().shape[0]
            all_event_rasters = []
            all_raster_lengths = []
            start_times = []
            end_times = []

            # need to offset each sleep file by duration of previous sleep files
            first = 0
            for l_s in self.long_sleep:
                duration = l_s.get_duration_sec()
                # all_event_rasters = all_event_rasters + l_s.get_event_spike_rasters(part_to_analyze=part_to_analyze)[0]
                new_event_raster, start_time, end_time = l_s.get_event_time_bin_rasters(part_to_analyze=part_to_analyze,
                                                                                        time_bin_size=0.01)
                all_event_rasters = all_event_rasters + new_event_raster
                start_times.append(start_time + first)
                end_times.append(end_time + first)
                first += duration

            start_times = np.hstack(start_times)
            end_times = np.hstack(end_times)

            new_time_stamps = []

            for event, start, end in zip(all_event_rasters, start_times, end_times):
                new_time_stamps.extend(np.linspace(start, end, event.shape[1]))

            new_time_stamps = np.expand_dims(np.array(new_time_stamps), 0)
            all_event_rasters = np.hstack(all_event_rasters)

            scaler = int(time_bin_size / 0.01)

            down_sampled = down_sample_array_sum(x=all_event_rasters, chunk_size=scaler)
            times_down_sampled = down_sample_array_mean(x=new_time_stamps, chunk_size=scaler)
            times_down_sampled = np.squeeze(times_down_sampled)

            new_ml = MlMethodsOnePopulation()
            new_ml.ridge_time_bin_progress(x=down_sampled, y=times_down_sampled,
                                           new_time_bin_size=0.5, alpha_fitting=True)

    def memory_drift_cosine_similarity_pop_vec_spatial_bins(self, new_time_bin_size=None):
        """
        computes pre-post similarity using population vectors of sleep and spatial bins of pre or post exploration.
        Only considers non-HSE periods

        @param new_time_bin_size: whether to use different time bin size for the population vectors than defined in
        parameters in seconds
        @type new_time_bin_size: float
        """
        print(" - COMPUTING MEMORY DRIFT USING COSINE SIM. AND SPATIAL BINS")

        rate_maps_orig = np.array(self.exploration_fam.get_rate_maps())
        rate_maps_fam = np.reshape(rate_maps_orig, (rate_maps_orig.shape[0], rate_maps_orig.shape[1]*rate_maps_orig.shape[2]))

        rate_maps_orig = np.array(self.exploration_novel.get_rate_maps())
        rate_maps_novel = np.reshape(rate_maps_orig, (rate_maps_orig.shape[0], rate_maps_orig.shape[1]*rate_maps_orig.shape[2]))

        raster_sleep = self.sleep_fam.get_raster()

        # plt.imshow(raster_sleep, interpolation='nearest', aspect='auto', cmap="jet")
        # a = plt.colorbar()
        # plt.show()

        ind_hse = np.array(find_hse(x=raster_sleep)).flatten()

        # remove high synchrony events
        raster_sleep = np.delete(raster_sleep, ind_hse, axis=1)
        # raster_sleep = raster_sleep[:,:10]

        if new_time_bin_size is not None:
            # down/up sample data
            time_bin_scaler = int(new_time_bin_size / self.params.time_bin_size)

            new_raster = np.zeros((raster_sleep.shape[0], int(raster_sleep.shape[1] / time_bin_scaler)))

            # down sample spikes by combining multiple bins
            for i in range(new_raster.shape[1]):
                new_raster[:, i] = np.sum(raster_sleep[:, (i * time_bin_scaler): ((1 + i) * time_bin_scaler)], axis=1)

            raster_sleep = new_raster

        sim_fam = np.zeros((raster_sleep.shape[1], rate_maps_fam.shape[1]))
        sim_novel = np.zeros((raster_sleep.shape[1], rate_maps_novel.shape[1]))
        # compute correlation between mean prob. vectors for each mode and all spatially binned population vectors
        for mode_id, pop_vec in enumerate(raster_sleep.T):
            for spat_bin_id, spat_bin_fam in enumerate(rate_maps_fam.T):
                sim_fam[mode_id, spat_bin_id] = np.nan_to_num((1-distance.cosine(pop_vec, spat_bin_fam)))
            for spat_bin_id, spat_bin_nov in enumerate(rate_maps_novel.T):
                sim_novel[mode_id, spat_bin_id] = np.nan_to_num((1-distance.cosine(pop_vec, spat_bin_nov)))

        sim_fam_max= np.max(sim_fam, axis=1)
        sim_novel_max = np.max(sim_novel, axis=1)

        drift_meas = (sim_novel_max - sim_fam_max)/(sim_novel_max + sim_fam_max)
        plt.plot(drift_meas)
        plt.title("SIMILARITY BEFORE - AFTER")
        plt.ylabel("SIMILARITY BEFORE / AFTER")
        plt.xlabel("TIME BINS (" + str(new_time_bin_size)+"s)")
        plt.show()

    def memory_drift_cosine_similarity_pop_vec_temporal_bins(self, new_time_bin_size=0.1):

        print(" - COMPUTING MEMORY DRIFT USING COSINE SIM. AND TEMPORAL BINNING")

        raster_fam = self.exploration_fam.get_raster()
        raster_novel = self.exploration_novel.get_raster()

        raster_sleep = self.sleep_fam.get_raster()

        # ind_hse = np.array(find_hse(x=raster_sleep)).flatten()
        #
        # # remove high synchrony events
        # raster_sleep = np.delete(raster_sleep, ind_hse, axis=1)
        # raster_sleep = raster_sleep[:,:10]


        # # down/up sample data
        # time_bin_scaler = int(new_time_bin_size / self.params.time_bin_size)
        #
        # new_raster = np.zeros((raster_sleep.shape[0], int(raster_sleep.shape[1] / time_bin_scaler)))
        #
        # # down sample spikes by combining multiple bins
        # for i in range(new_raster.shape[1]):
        #     new_raster[:, i] = np.sum(raster_sleep[:, (i * time_bin_scaler): ((1 + i) * time_bin_scaler)], axis=1)
        #
        # raster_sleep = new_raster

        sim_fam = np.zeros((raster_sleep.shape[1], raster_fam.shape[1]))
        sim_novel = np.zeros((raster_sleep.shape[1], raster_novel.shape[1]))
        # compute correlation between mean prob. vectors for each mode and all spatially binned population vectors
        for mode_id, pop_vec in enumerate(raster_sleep.T):
            for spat_bin_id, spat_bin_fam in enumerate(raster_fam.T):
                sim_fam[mode_id, spat_bin_id] = np.nan_to_num((1-distance.cosine(pop_vec, spat_bin_fam)))
            for spat_bin_id, spat_bin_nov in enumerate(raster_novel.T):
                sim_novel[mode_id, spat_bin_id] = np.nan_to_num((1-distance.cosine(pop_vec, spat_bin_nov)))

        # plt.imshow(sim_fam,  interpolation='nearest', aspect='auto')
        # plt.colorbar()
        # plt.show()
        # plt.imshow(sim_novel,  interpolation='nearest', aspect='auto')
        # plt.colorbar()
        # plt.show()

        sim_fam_max= np.max(sim_fam, axis=1)
        sim_novel_max = np.max(sim_novel, axis=1)

        drift_meas = (sim_novel_max - sim_fam_max)/(sim_novel_max + sim_fam_max)
        plt.plot(drift_meas)
        plt.show()

    def memory_drift_cosine_similarity_pop_vec_poisson_hmm_modes(self, file_phmm_before, file_phmm_after):
        # --------------------------------------------------------------------------------------------------------------
        # analyzes memory drift using lambda vectors (poisson HMM) established during awake behavior. Compares sleep
        # activity to all lambda vectors of behavior before vs. to all lambda vectors of behavior after
        #
        # parameters:   - file_phmm_before, string: file name containing phmm model for behavior before
        #               - file_phmm_after, string: file name containing phmm model for behavior after
        #
        # returns:      -
        # --------------------------------------------------------------------------------------------------------------

        print(" - COMPUTING MEMORY DRIFT - PHMM LAMBDA PER MODE")

        _, _, _, _, _, _, phmm_means_before = self.exploration_fam.fit_poisson_hmm(file_name=file_phmm_before)
        _, _, _, _, _, _, phmm_means_after = self.exploration_novel.fit_poisson_hmm(file_name=file_phmm_after)

        # plt.imshow(phmm_means_after, interpolation='nearest', aspect='auto')
        # plt.show()
        #
        # plt.imshow(phmm_means_before, interpolation='nearest', aspect='auto')
        # plt.show()

        raster_sleep = self.sleep_fam.get_spike_binned_raster()

        # raster_sleep = raster_sleep[:, :1000]

        # ind_hse = np.array(find_hse(x=raster_sleep)).flatten()
        #
        # # remove high synchrony events
        # raster_sleep = np.delete(raster_sleep, ind_hse, axis=1)
        # raster_sleep = raster_sleep[:,:10]


        # # down/up sample data
        # time_bin_scaler = int(new_time_bin_size / self.params.time_bin_size)
        #
        # new_raster = np.zeros((raster_sleep.shape[0], int(raster_sleep.shape[1] / time_bin_scaler)))
        #
        # # down sample spikes by combining multiple bins
        # for i in range(new_raster.shape[1]):
        #     new_raster[:, i] = np.sum(raster_sleep[:, (i * time_bin_scaler): ((1 + i) * time_bin_scaler)], axis=1)
        #
        # raster_sleep = new_raster

        sim_fam = np.zeros((raster_sleep.shape[1], phmm_means_before.shape[0]))
        sim_novel = np.zeros((raster_sleep.shape[1], phmm_means_after.shape[0]))
        # compute correlation between mean prob. vectors for each mode and all spatially binned population vectors
        for mode_id, pop_vec in enumerate(raster_sleep.T):
            for spat_bin_id, spat_bin_fam in enumerate(phmm_means_before):
                sim_fam[mode_id, spat_bin_id] = np.nan_to_num((1-distance.cosine(pop_vec, spat_bin_fam)))
                # sim_fam[mode_id, spat_bin_id], _ = pearsonr(pop_vec, spat_bin_fam)
            for spat_bin_id, spat_bin_nov in enumerate(phmm_means_after):
                sim_novel[mode_id, spat_bin_id] = np.nan_to_num((1-distance.cosine(pop_vec, spat_bin_nov)))
                # sim_novel[mode_id, spat_bin_id], _ = pearsonr(pop_vec, spat_bin_nov)

        # plt.imshow(sim_fam,  interpolation='nearest', aspect='auto')
        # plt.colorbar()
        # plt.show()
        # plt.imshow(sim_novel,  interpolation='nearest', aspect='auto')
        # plt.colorbar()
        # plt.show()

        sim_fam_max = np.max(sim_fam, axis=1)
        sim_novel_max = np.max(sim_novel, axis=1)

        drift_meas = (sim_novel_max - sim_fam_max)/(sim_novel_max + sim_fam_max)
        plt.plot(drift_meas)
        plt.show()

    def predict_time_bin_across_sleeps(self, normalize_firing_rates=False):
        # --------------------------------------------------------------------------------------------------------------
        # analysis of drift using population vectors
        #
        # parameters:   - normalize_firing_rates, bool: yes if true
        # --------------------------------------------------------------------------------------------------------------

        X_f = self.sleep_fam.get_raster()
        X_n = self.sleep_novel.get_raster()

        # find high synchrony events
        ind_hse = find_hse(x=X_f)

        # remove high synchrony events
        x_f_wo_hse = np.delete(X_f, ind_hse, axis=1)

        # find high synchrony events
        ind_hse = find_hse(x=X_n)

        # remove high synchrony events
        x_n_wo_hse = np.delete(X_n, ind_hse, axis=1)

        from sklearn.linear_model import Ridge

        y_f = np.arange(x_f_wo_hse.shape[1]) * self.params.time_bin_size
        y_n = np.arange(x_n_wo_hse.shape[1]) * self.params.time_bin_size

        clf = Ridge(alpha=100)
        clf.fit(x_f_wo_hse.T, y_f)

        true_values = y_n.T/60
        pred_values = clf.predict(x_n_wo_hse.T).T/60
        r2 = clf.score(x_n_wo_hse.T, y_n)

        plt.scatter(true_values, pred_values, label="MODEL")
        plt.gca().set_aspect('equal', adjustable='box')
        plt.xlabel("TIME / min - TRUE VALUES")
        plt.ylabel("TIME / min - PREDICTED VALUES")
        plt.title("RIDGE REGRESSION, PREDICTING TIME BINS: r2="+str(round(r2, 2)))
        plt.show()

    # correlation analysis
    # ------------------------------------------------------------------------------------------------------------------

    def memory_drift_correlation_structure(self, plot_file_name="test", bins_per_corr_matrix=50, only_stable_cells=False,
                                           n_smoothing=100):
        # --------------------------------------------------------------------------------------------------------------
        # analyzes memory drift using correlation structure. Computes correlation matrix of awake behavior before and
        # correlation matrix of behavior after -> compares correlation matrix computed from sliding window during sleep
        # with before/after correlation matrix using Pearson correlation value
        #
        # parameters:   - correlation_window_size, int: size of sliding window (nr. time bins) to compute correlations
        #                 during sleep
        #
        # returns:      -
        # --------------------------------------------------------------------------------------------------------------

        # check if cheeseboard data or exploration data is used
        if hasattr(self, "learning_cheeseboard"):
            # get rasters from exploration before/after
            raster_pre = self.learning_cheeseboard[0].get_raster(trials_to_use="all")
            raster_post = self.learning_cheeseboard[1].get_raster(trials_to_use="all")

            if only_stable_cells:
                # load only stable cells
                with open(
                        self.params.pre_proc_dir + "cell_classification/" + self.params.session_name + "_k_means.pickle",
                        "rb") as f:
                    class_dic = pickle.load(f)

                stable_ids = class_dic["stable_cell_ids"]

                nr_cells = stable_ids.shape[0]

                corr_sleep = np.zeros((int(nr_cells * (nr_cells - 1) / 2), 0))

                for l_s in self.long_sleep:
                    c_m = l_s.get_correlation_matrices(bins_per_corr_matrix=bins_per_corr_matrix,
                                                       cell_selection=stable_ids, exclude_diagonal=True)
                    corr_sleep = np.hstack((corr_sleep, c_m))

                # compute correlations of exploration before/after
                corr_pre = np.nan_to_num(np.corrcoef(raster_pre[stable_ids, :]))
                corr_post = np.nan_to_num(np.corrcoef(raster_post[stable_ids, :]))


            else:

                # compute correlations of exploration before/after
                corr_pre = np.nan_to_num(np.corrcoef(raster_pre))
                corr_post = np.nan_to_num(np.corrcoef(raster_post))

                nr_cells = raster_pre.shape[0]

                # corr_sleep = np.zeros((nr_cells ** 2, 0))
                # only off-diagonal elements
                corr_sleep = np.zeros((int(nr_cells * (nr_cells - 1) / 2), 0))

                for l_s in self.long_sleep:
                    c_m = l_s.get_correlation_matrices(bins_per_corr_matrix=bins_per_corr_matrix, exclude_diagonal=True)
                    corr_sleep = np.hstack((corr_sleep, c_m))

        elif hasattr(self, "exploration_fam"):
            # get rasters from exploration before/after
            raster_pre = self.exploration_fam.get_raster()
            raster_post = self.exploration_novel.get_raster()

            # compute correlations of exploration before/after
            corr_pre = np.nan_to_num(np.corrcoef(raster_pre))
            corr_post = np.nan_to_num(np.corrcoef(raster_post))

            raise Exception("NEED TO IMPLEMENT CORRELATION WINDOW SIZE")

            correlation_window_size = 2

            # compute correlations for sliding window during sleep
            corr_sleep = self.sleep_fam.dynamic_co_firing(co_firing_window_size=correlation_window_size)

        corr_post = upper_tri_without_diag(corr_post)
        corr_pre = upper_tri_without_diag(corr_pre)

        sim_pearson = []

        # for each sliding window compute similarity with behavior before/after
        for corr in corr_sleep.T:
            sim_post = abs(pearsonr(corr, corr_post)[0])
            sim_pre = abs(pearsonr(corr, corr_pre)[0])
            sim_pearson.append((sim_post-sim_pre)/(sim_post + sim_pre))

        fig = plt.figure()
        ax = fig.add_subplot()
        ax.plot(sim_pearson, color="red", label="PEARSON")
        plt.title("CORRELATION STRUCTURE SIMILARITY: BEFORE - AFTER\n #BINS PER WINDOW: "+str(bins_per_corr_matrix))
        plt.xlabel("WINDOW ID")
        plt.ylabel("SIMILARITY BEFORE - AFTER / PEARSON")
        plt.ylim(-1, 1)
        plt.show()

        sim_pearson = np.array(sim_pearson)

        s = sim_pearson.copy()
        control = []
        # control --> do 50 shuffles
        for i in range(50):
            np.random.shuffle(s)
            s_smooth = moving_average(a=s, n=n_smoothing)
            control.append(s_smooth)

        control = np.array(control)
        con_mean = np.mean(control, axis=0)
        con_std = np.std(control, axis=0)
        fig = plt.figure()
        ax = fig.add_subplot()
        # smoothing
        sim_pearson_s = moving_average(a=np.array(sim_pearson), n=n_smoothing)
        ax.plot(con_mean, color="grey", label="CONTROL (MEAN +- STD), 50 SHUFFLES")
        ax.plot(con_mean+con_std, color="grey",linestyle="dashed")
        ax.plot(con_mean - con_std, color="grey",linestyle="dashed")

        ax.plot(sim_pearson_s, color="red", label="DATA")
        plt.title("CORRELATION STRUCTURE SIMILARITY: PRE - POST\n #BINS PER WINDOW: "+str(bins_per_corr_matrix))
        plt.xlabel("WINDOW ID")
        plt.ylabel("PRE_POST SIMILARITY")
        plt.ylim(-0.33, 0.33)
        # plt.ylim(min(sim_pearson_s), -1*min(sim_pearson_s))
        plt.legend()
        plt.show()
        plt.savefig(self.params.pre_proc_dir+plot_file_name)

    # additional analysis
    # ------------------------------------------------------------------------------------------------------------------

    def memory_drift_rem_reset_parameter_influence(self, template_type, n_moving_average_pop_vec):
        fig = plt.figure()
        ax = fig.add_subplot()
        for i, rem_pop_vec_threshold in enumerate([20, 50, 100, 150, 200, 300, 400, 500, 600]):
            ds_rem_sum, ds_nrem_sum, ratio_per_merged_nrem_event, ratio_per_merged_rem_event =\
                self.memory_drift_rem_reset(template_type=template_type,
                                            n_moving_average_pop_vec=n_moving_average_pop_vec, rem_pop_vec_threshold=\
                    rem_pop_vec_threshold)
            ax.scatter(rem_pop_vec_threshold, ds_nrem_sum, color="b", label="NREM", zorder=1000)
            ax.scatter(rem_pop_vec_threshold, ds_rem_sum, color="r", label="REM", zorder=1000)

        handles, labels = ax.get_legend_handles_labels()
        by_label = OrderedDict(zip(labels, handles))
        ax.legend(by_label.values(), by_label.keys())
        plt.grid()
        plt.ylabel("CUMULATIVE DELTA SCORE")
        plt.xlabel("REM THRESHOLD ( POP.VEC. < THRS. ARE DISCARDED)")
        plt.title("SMOOTHING, n="+str(n_moving_average_pop_vec))
        plt.show()

    def memory_drift_spatial_content_progression_swr(self, pHMM_file_pre, pHMM_file_post,
                                   plot_for_control=False, n_moving_average_swr=10, n_moving_average_pop_vec=40):

        print("SPATIAL CONTENT PROGRESSION \n")

        # get pre-post similarity
        pre_SWR_prob, post_SWR_prob, event_times = self.sleep_fam.content_based_memory_drift_phmm(
                        pHMM_file_pre=pHMM_file_pre,
                        pHMM_file_post=pHMM_file_post,
                        plot_for_control=plot_for_control, return_results=True)

        # get spatial information for each mode from PRE
        sparsity_pre, skaggs_pre = self.exploration_fam.phmm_mode_spatial_information_from_model(file_name=pHMM_file_pre,
                                                                                          plot_for_control=plot_for_control,
                                                                                         spatial_resolution=10)

        # get spatial information for each mode from POST
        sparsity_post, skaggs_post = self.exploration_novel.phmm_mode_spatial_information_from_model(file_name=pHMM_file_post,
                                                                                          plot_for_control=plot_for_control,
                                                                                         spatial_resolution=10)

        plt.hist(skaggs_post, label="POST MODES")
        plt.hist(skaggs_pre, color="r", alpha=0.5, label="PRE MODES")
        plt.xlabel("SKAGGS INFO")
        plt.ylabel("COUNTS")
        plt.legend()
        plt.show()

        # get model from PRE and transition matrix
        model_pre = self.exploration_fam.load_poisson_hmm(file_name=pHMM_file_pre)
        transmat_pre = model_pre.transmat_

        nr_modes_pre = pre_SWR_prob[0].shape[1]
        nr_modes_post = post_SWR_prob[0].shape[1]

        # per SWR results
        swr_pre_sparsity = []
        swr_pre_skaggs = []
        swr_post_sparsity = []
        swr_post_skaggs = []
        swr_pre_post_ratio = []
        swr_pre_prob = []
        swr_post_prob = []
        swr_seq_similarity = []
        swr_len_seq = []

        # per population vector results
        pop_vec_pre_post_ratio = []
        pre_seq_list = []
        post_seq_list = []
        pre_seq_list_prob = []
        post_seq_list_prob = []
        pop_vec_post_prob = []
        pop_vec_pre_prob = []

        # go trough all SWR
        for pre_array, post_array in zip(pre_SWR_prob, post_SWR_prob):

            # make sure that there is any data for the current SWR

            if pre_array.shape[0] > 0:
                pre_sequence = np.argmax(pre_array, axis=1)
                pre_sequence_prob = np.max(pre_array, axis=1)
                post_sequence = np.argmax(post_array, axis=1)
                post_sequence_prob = np.max(post_array, axis=1)
                pre_seq_list.extend(pre_sequence)
                post_seq_list.extend(post_sequence)
                pre_seq_list_prob.extend(pre_sequence_prob)
                post_seq_list_prob.extend(post_sequence_prob)

                # check how likely observed sequence is considering transitions from model (awake behavior)
                mode_before = pre_sequence[:-1]
                mode_after = pre_sequence[1:]
                transition_prob = 0
                # go trough each transition of the sequence
                for bef, aft in zip(mode_before, mode_after):
                    transition_prob += np.log(transmat_pre[bef, aft])

                swr_seq_similarity.append(np.exp(transition_prob))
                swr_len_seq.append(pre_sequence.shape[0])

                # per SWR computations
                # ----------------------------------------------------------------------------------------------------------
                # compute spatial information per SWR
                swr_pre_sparsity.append(np.mean(sparsity_pre[pre_sequence]))
                swr_pre_skaggs.append(np.mean(skaggs_pre[pre_sequence]))
                swr_post_sparsity.append(np.mean(sparsity_post[post_sequence]))
                swr_post_skaggs.append(np.mean(skaggs_post[post_sequence]))
                # arrays: [nr_pop_vecs_per_SWR, nr_time_spatial_time_bins]
                # get maximum value per population vector and take average across the SWR
                if pre_array.shape[0] > 0:
                    # save pre and post probabilities
                    swr_pre_prob.append(np.mean(np.max(pre_array, axis=1)))
                    swr_post_prob.append(np.mean(np.max(post_array, axis=1)))
                    # compute ratio
                    prob_pre = np.mean(np.max(pre_array, axis=1))
                    prob_post = np.mean(np.max(post_array, axis=1))
                    swr_pre_post_ratio.append((prob_post - prob_pre) / (prob_post + prob_pre))
                else:
                    swr_pre_prob.append(np.nan)
                    swr_post_prob.append(np.nan)
                    swr_pre_post_ratio.append(np.nan)

                # per population vector computations
                # ----------------------------------------------------------------------------------------------------------
                # compute per population vector similarity score
                prob_post = np.max(post_array, axis=1)
                prob_pre = np.max(pre_array, axis=1)
                pop_vec_pre_post_ratio.extend((prob_post - prob_pre) / (prob_post + prob_pre))
                pop_vec_pre_prob.extend(np.max(pre_array, axis=1))
                if pre_array.shape[0] > 0:
                    pop_vec_pre_prob.extend(np.max(pre_array, axis=1))
                    pop_vec_post_prob.extend(np.max(post_array, axis=1))
                else:
                    pop_vec_pre_prob.extend([np.nan])
                    pop_vec_post_prob.extend([np.nan])

        # smoothen per SWR info
        # --------------------------------------------------------------------------------------------------------------
        swr_pre_sparsity = moving_average(a=np.array(swr_pre_sparsity), n=n_moving_average_swr)
        swr_pre_skaggs = moving_average(a=np.array(swr_pre_skaggs), n=n_moving_average_swr)
        swr_post_skaggs = moving_average(a=np.array(swr_post_skaggs), n=n_moving_average_swr)
        swr_post_sparsity = moving_average(a=np.array(swr_post_sparsity), n=n_moving_average_swr)
        swr_pre_post_ratio = moving_average(a=np.array(swr_pre_post_ratio), n=n_moving_average_swr)

        # plot per SWR info
        # --------------------------------------------------------------------------------------------------------------
        plt.plot(swr_pre_post_ratio)
        plt.title("PRE-POST RATIO FOR EACH SWR: PHMM")
        plt.xlabel("SWR ID")
        plt.ylabel("PRE-POST SIMILARITY")
        plt.ylim(-1, 1)
        plt.grid()
        plt.show()

        plt.subplot(2,1,1)
        plt.plot(swr_pre_skaggs)
        plt.title("SPATIAL INFORMATION PER SWR: PRE, SKAGGS")
        plt.xlabel("SWR ID")
        plt.ylabel("AVG. SKAGGS INFO")
        plt.subplot(2,1,2)
        plt.plot(swr_pre_sparsity)
        plt.title("SPATIAL INFORMATION PER SWR: PRE, SPARSITY")
        plt.xlabel("SWR ID")
        plt.ylabel("AVG. SPARSITY")
        plt.show()

        plt.scatter(swr_pre_post_ratio, swr_pre_skaggs)
        plt.title("PER SWR CORRELATION: SCORE vs. SKAGGS\n"+str(pearsonr(swr_pre_post_ratio, swr_pre_skaggs)))
        plt.xlabel("SIMILARITY SCORE")
        plt.ylabel("SKAGGS INFORMATION")
        plt.show()
        plt.scatter(swr_pre_post_ratio, swr_pre_sparsity)
        plt.title("PER SWR CORRELATION: SCORE vs. SPARSITY\n"+str(pearsonr(swr_pre_post_ratio, swr_pre_sparsity)))
        plt.xlabel("SIMILARITY SCORE")
        plt.ylabel("SKAGGS SPARSITY")
        plt.show()

        plt.plot(swr_pre_skaggs, c="r", label="PRE")
        plt.plot(swr_post_skaggs, label="POST")
        plt.title("SPATIAL INFORMATION PER SWR: PRE, SKAGGS")
        plt.xlabel("SWR ID")
        plt.ylabel("AVG. SKAGGS INFO")
        plt.legend()
        plt.show()

        plt.plot(swr_pre_sparsity, c="r", label="PRE")
        plt.plot(swr_post_sparsity, label="POST")
        plt.title("SPATIAL INFORMATION PER SWR: PRE, SPARSITY")
        plt.xlabel("SWR ID")
        plt.ylabel("AVG. SPARSITY")
        plt.legend()
        plt.show()

        # sequence probability
        swr_seq_similarity = moving_average(a=np.array(swr_seq_similarity), n=n_moving_average_pop_vec)
        plt.plot(swr_seq_similarity)
        plt.title("PROBABILITY SWR PHMM MODE SEQUENCES \n USING AWAKE TRANSITION PROB. PRE")
        plt.ylabel("JOINT PROBABILITY")
        plt.xlabel("SWR ID")
        plt.show()

        # plot per population vector info
        # --------------------------------------------------------------------------------------------------------------
        pop_vec_pre_post_ratio = np.array(pop_vec_pre_post_ratio)
        # compute moving average to smooth signal
        pop_vec_pre_post_ratio_smooth = moving_average(a=pop_vec_pre_post_ratio, n=n_moving_average_pop_vec)
        plt.plot(pop_vec_pre_post_ratio_smooth)
        plt.title("PRE-POST RATIO FOR EACH POP. VECTOR: PHMM")
        plt.xlabel("POP.VEC. ID")
        plt.ylabel("PRE-POST SIMILARITY")
        plt.ylim(-1, 1)
        plt.grid()
        plt.show()

        # compute per mode info
        # --------------------------------------------------------------------------------------------------------------
        pre_seq = np.array(pre_seq_list)
        post_seq = np.array(post_seq_list)
        mode_score_mean_pre = np.zeros(pre_SWR_prob[0].shape[1])
        mode_score_std_pre = np.zeros(pre_SWR_prob[0].shape[1])
        mode_score_mean_post = np.zeros(post_SWR_prob[0].shape[1])
        mode_score_std_post = np.zeros(post_SWR_prob[0].shape[1])

        # go through all pre modes and check the average score
        for i in range(pre_SWR_prob[0].shape[1]):
            ind_sel = np.where(pre_seq == i)[0]
            if ind_sel.size == 0 or ind_sel.size == 1:
                mode_score_mean_pre[i] = np.nan
                mode_score_std_pre[i] = np.nan
            else:
                # delete all indices that are too large (becaue of moving average)
                ind_sel = ind_sel[ind_sel < pop_vec_pre_post_ratio.shape[0]]
                mode_score_mean_pre[i] = np.mean(pop_vec_pre_post_ratio[ind_sel])
                mode_score_std_pre[i] = np.std(pop_vec_pre_post_ratio[ind_sel])

        # go through all post modes and check the average score
        for i in range(post_SWR_prob[0].shape[1]):
            ind_sel = np.where(post_seq == i)[0]
            if ind_sel.size == 0 or ind_sel.size == 1:
                mode_score_mean_post[i] = np.nan
                mode_score_std_post[i] = np.nan
            else:
                # delete all indices that are too large (becaue of moving average)
                ind_sel = ind_sel[ind_sel < pop_vec_pre_post_ratio.shape[0]]
                mode_score_mean_post[i] = np.mean(pop_vec_pre_post_ratio[ind_sel])
                mode_score_std_post[i] = np.std(pop_vec_pre_post_ratio[ind_sel])

        low_score_modes = np.argsort(mode_score_mean_pre)
        # need to skip nans
        nr_nans = np.count_nonzero(np.isnan(mode_score_mean_pre))
        high_score_modes = np.flip(low_score_modes)[nr_nans:]

        plt.errorbar(range(pre_SWR_prob[0].shape[1]), mode_score_mean_pre, yerr=mode_score_std_pre, linestyle="")
        plt.scatter(range(pre_SWR_prob[0].shape[1]), mode_score_mean_pre)
        plt.title("PRE-POST SCORE PER MODE: PRE")
        plt.xlabel("MODE ID")
        plt.ylabel("PRE-POST SCORE: MEAN AND STD")
        plt.show()

        plt.errorbar(range(post_SWR_prob[0].shape[1]), mode_score_mean_post, yerr=mode_score_std_post, linestyle="")
        plt.scatter(range(post_SWR_prob[0].shape[1]), mode_score_mean_post)
        plt.title("PRE-POST SCORE PER MODE: POST")
        plt.xlabel("MODE ID")
        plt.ylabel("PRE-POST SCORE: MEAN AND STD")
        plt.show()

        plt.scatter(mode_score_mean_pre, sparsity_pre)
        plt.title("PER MODE: CORRELATION SPARSITY - PRE_POST SCORE\n"+
                  str(pearsonr(np.nan_to_num(mode_score_mean_pre), np.nan_to_num(sparsity_pre))))
        plt.xlabel("PRE_POST SCORE")
        plt.ylabel("MEAN SPARSITY")
        plt.show()
        plt.scatter(mode_score_mean_pre, skaggs_pre)
        plt.title("PER MODE: CORRELATION SKAGGS - PRE_POST SCORE\n"+
                  str(pearsonr(np.nan_to_num(mode_score_mean_pre), np.nan_to_num(skaggs_pre))))
        plt.xlabel("PRE_POST SCORE")
        plt.ylabel("MEAN SKAGGS INFO")
        plt.show()

        # check if modes get more often/less often reactivated over time
        pre_seq_list = np.array(pre_seq_list)
        nr_pop_vec = 20
        nr_windows = int(pre_seq_list.shape[0]/nr_pop_vec)
        occurence_modes = np.zeros((nr_modes_pre, nr_windows))
        for i in range(nr_windows):
            seq = pre_seq_list[i*nr_pop_vec:(i+1)*nr_pop_vec]
            mode, counts = np.unique(seq, return_counts=True)
            occurence_modes[mode,i] = counts

        plt.imshow(occurence_modes, interpolation='nearest', aspect='auto')
        plt.ylabel("MODE ID")
        plt.xlabel("WINDOW ID")
        a = plt.colorbar()
        a.set_label("#WINS/"+str(nr_pop_vec)+" POP. VEC. WINDOW")
        plt.title("OCCURENCE (#WINS) OF MODES IN WINDOWS OF FIXED LENGTH")
        plt.show()

    def memory_drift_spatial_content_progression_window(self, pHMM_file_pre, pHMM_file_post,
                                   plot_for_control=False, n_moving_average=10, sliding_window=True):

        print("SPATIAL CONTENT PROGRESSION USING WINDOWS \n")

        # get pre-post similarity
        pre_SWR_prob, post_SWR_prob, event_times = self.sleep_fam.content_based_memory_drift_phmm(
                        pHMM_file_pre=pHMM_file_pre,
                        pHMM_file_post=pHMM_file_post,
                        plot_for_control=plot_for_control, return_results=True)

        # get spatial information for each mode from PRE
        sparsity_pre, skaggs_pre = self.exploration_fam.phmm_mode_spatial_information_from_model(file_name=pHMM_file_pre,
                                                                                          plot_for_control=plot_for_control,
                                                                                         spatial_resolution=10)

        # get spatial information for each mode from POST
        sparsity_post, skaggs_post = self.exploration_novel.phmm_mode_spatial_information_from_model(file_name=pHMM_file_post,
                                                                                          plot_for_control=plot_for_control,
                                                                                         spatial_resolution=10)

        # get model from PRE and transition matrix
        model_pre = self.exploration_fam.load_poisson_hmm(file_name=pHMM_file_pre)
        transmat_pre = model_pre.transmat_


        pre_SWR_prob_arr = np.vstack(pre_SWR_prob)
        post_SWR_prob_arr = np.vstack(post_SWR_prob)

        win_pre_sparsity = []
        win_pre_skaggs = []
        win_post_sparsity = []
        win_post_skaggs = []
        win_pre_post_ratio = []
        win_seq_similarity = []


        # use sliding window to go over SWR pop. vec
        n_slid_win = 20
        overlap = 5

        if sliding_window:

            for i in np.arange(0,pre_SWR_prob_arr.shape[0] - n_slid_win +1, overlap):
                pre_array = pre_SWR_prob_arr[i:(i + n_slid_win)]
                pre_sequence = np.argmax(pre_array, axis=1)
                post_array = post_SWR_prob_arr[i:(i + n_slid_win)]
                post_sequence = np.argmax(post_array, axis=1)
                win_pre_sparsity.append(np.mean(sparsity_pre[pre_sequence]))
                win_pre_skaggs.append(np.mean(skaggs_pre[pre_sequence]))
                win_post_sparsity.append(np.mean(sparsity_post[post_sequence]))
                win_post_skaggs.append(np.mean(skaggs_post[post_sequence]))
                prob_pre = np.mean(np.max(pre_array, axis=1))
                prob_post = np.mean(np.max(post_array, axis=1))
                win_pre_post_ratio.append((prob_post - prob_pre) / (prob_post + prob_pre))

                # check how likely observed sequence is considering transitions from model (awake behavior)
                mode_before = pre_sequence[:-1]
                mode_after = pre_sequence[1:]
                transition_prob = 0
                # go trough each transition of the sequence
                for bef, aft in zip(mode_before, mode_after):
                    transition_prob += np.log(transmat_pre[bef, aft])

                win_seq_similarity.append(np.exp(transition_prob))

        else:

            for i in range(round(pre_SWR_prob_arr.shape[0]/n_slid_win)):
                pre_array = pre_SWR_prob_arr[i*n_slid_win:(i+1)*n_slid_win]
                pre_sequence = np.argmax(pre_array, axis=1)
                post_array = post_SWR_prob_arr[i*n_slid_win:(i+1)*n_slid_win]
                post_sequence = np.argmax(post_array, axis=1)
                win_pre_sparsity.append(np.mean(sparsity_pre[pre_sequence]))
                win_pre_skaggs.append(np.mean(skaggs_pre[pre_sequence]))
                win_post_sparsity.append(np.mean(sparsity_post[post_sequence]))
                win_post_skaggs.append(np.mean(skaggs_post[post_sequence]))
                prob_pre = np.mean(np.max(pre_array, axis=1))
                prob_post = np.mean(np.max(post_array, axis=1))
                win_pre_post_ratio.append((prob_post - prob_pre) / (prob_post + prob_pre))

                # check how likely observed sequence is considering transitions from model (awake behavior)
                mode_before = pre_sequence[:-1]
                mode_after = pre_sequence[1:]
                transition_prob = 0
                # go trough each transition of the sequence
                for bef, aft in zip(mode_before, mode_after):
                    transition_prob += np.log(transmat_pre[bef, aft])

                win_seq_similarity.append(np.exp(transition_prob))

        win_pre_skaggs = moving_average(a=np.array(win_pre_skaggs), n=n_moving_average)
        win_pre_sparsity = moving_average(a=np.array(win_pre_sparsity), n=n_moving_average)
        win_post_skaggs = moving_average(a=np.array(win_post_skaggs), n=n_moving_average)
        win_post_sparsity = moving_average(a=np.array(win_post_sparsity), n=n_moving_average)
        win_pre_post_ratio = moving_average(a=np.array(win_pre_post_ratio), n=n_moving_average)
        # win_seq_similarity = moving_average(a=np.array(win_seq_similarity), n=30)

        # plot per SWR info
        # --------------------------------------------------------------------------------------------------------------
        plt.plot(win_pre_post_ratio)
        plt.title("PRE-POST RATIO FOR EACH WINDOW: PHMM")
        plt.xlabel("WINDOW ID")
        plt.ylabel("PRE-POST SIMILARITY")
        plt.ylim(-1, 1)
        plt.grid()
        plt.show()

        plt.subplot(2,1,1)
        plt.plot(win_pre_skaggs)
        plt.title("SPATIAL INFORMATION PER WINDOW: PRE, SKAGGS")
        plt.xlabel("WINDOW ID")
        plt.ylabel("AVG. SKAGGS INFO")
        plt.subplot(2,1,2)
        plt.plot(win_pre_sparsity)
        plt.title("SPATIAL INFORMATION PER WINDOW: PRE, SPARSITY")
        plt.xlabel("WINDOW ID")
        plt.ylabel("AVG. SPARSITY")
        plt.show()

        plt.scatter(win_pre_post_ratio, win_pre_skaggs)
        plt.title("PER WINDOW CORRELATION: SCORE vs. SKAGGS\n"+str(pearsonr(win_pre_post_ratio, win_pre_skaggs)))
        plt.xlabel("SIMILARITY SCORE")
        plt.ylabel("SKAGGS INFORMATION")
        plt.show()
        plt.scatter(win_pre_post_ratio, win_pre_sparsity)
        plt.title("PER WINDOW CORRELATION: SCORE vs. SPARSITY\n"+str(pearsonr(win_pre_post_ratio, win_pre_sparsity)))
        plt.xlabel("SIMILARITY SCORE")
        plt.ylabel("SKAGGS SPARSITY")
        plt.show()

        plt.plot(win_pre_skaggs, c="r", label="PRE")
        plt.plot(win_post_skaggs, label="POST")
        plt.title("SPATIAL INFORMATION PER WINDOW: PRE, SKAGGS")
        plt.xlabel("WINDOW ID")
        plt.ylabel("AVG. SKAGGS INFO")
        plt.legend()
        plt.show()

        plt.plot(win_pre_sparsity, c="r", label="PRE")
        plt.plot(win_post_sparsity, label="POST")
        plt.title("SPATIAL INFORMATION PER WINDOW: PRE, SPARSITY")
        plt.xlabel("WINDOW ID")
        plt.ylabel("AVG. SPARSITY")
        plt.legend()
        plt.show()

        plt.plot(win_seq_similarity)
        plt.title("SEQUENCE PROBABILITY: PHMM")
        plt.xlabel("WINDOW ID")
        plt.ylabel("PROBABILITY")
        plt.grid()
        plt.show()

    def memory_drift_phmm_mode_analysis(self, template_type, pre_file_name=None, post_file_name=None,
                                        rem_pop_vec_threshold=10, only_stable_cells=False, perc_inc_thresh=20,
                                        nr_modes=25):

        print("ANALYZING MODES OF MEMORY DRIFT \n")

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_nrem_event, event_times_nrem, event_lengths_nrem, event_duration_nrem_in_s, pre_prob_nrem, \
        post_prob_nrem= self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="nrem",
                                                     pop_vec_threshold=2, only_stable_cells=only_stable_cells)


        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_rem_event, event_times_rem, event_lengths_rem, event_duration_rem_in_s, pre_prob_rem, post_prob_rem= \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold,
                                                     only_stable_cells=only_stable_cells)



        _, _, _, _, pre_prob_rem_stable, post_prob_rem_stable= \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold,
                                                     only_stable_cells=True)


        _, _, _, _, pre_prob_nrem_stable, post_prob_nrem_stable= \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="nrem",
                                                     pop_vec_threshold=rem_pop_vec_threshold,
                                                     only_stable_cells=True)


        mode_ids= np.arange(nr_modes)
        # check reactivated modes
        modes_pre_rem = np.argmax(pre_prob_rem, axis=1)
        modes_rem, counts = np.unique(modes_pre_rem, return_counts=True)
        counts_rem = np.zeros(nr_modes)
        counts_rem[modes_rem] = counts

        modes_pre_rem_stable = np.argmax(pre_prob_rem_stable, axis=1)
        modes_rem_stable, counts = np.unique(modes_pre_rem_stable, return_counts=True)
        counts_rem_stable = np.zeros(nr_modes)
        counts_rem_stable[modes_rem_stable] = counts

        modes_pre_nrem = np.argmax(pre_prob_nrem, axis=1)
        modes_nrem, counts = np.unique(modes_pre_nrem, return_counts=True)
        counts_nrem = np.zeros(nr_modes)
        counts_nrem[modes_nrem] = counts

        modes_pre_nrem_stable = np.argmax(pre_prob_nrem_stable, axis=1)
        modes_nrem_stable, counts = np.unique(modes_pre_nrem_stable, return_counts=True)
        counts_nrem_stable = np.zeros(nr_modes)
        counts_nrem_stable[modes_nrem_stable] = counts



        lines = []
        for i in range(len(modes_rem)):
            pair = [(modes_rem[i], 0), (modes_rem[i], counts_rem[i])]
            lines.append(pair)

        linecoll = matcoll.LineCollection(lines)
        fig, ax = plt.subplots()
        ax.add_collection(linecoll)
        plt.scatter(mode_ids, counts_rem, label="ALL CELLS")
        plt.scatter(mode_ids, counts_rem_stable, label="ONLY STABLE")
        plt.xlabel("MODE ID")
        plt.ylabel("COUNTS")
        plt.title("REM")
        plt.legend()
        plt.show()


        lines = []
        for i in range(len(modes_nrem)):
            pair = [(modes_nrem[i], 0), (modes_nrem[i], counts_nrem[i])]
            lines.append(pair)

        linecoll = matcoll.LineCollection(lines)
        fig, ax = plt.subplots()
        ax.add_collection(linecoll)
        plt.scatter(mode_ids, counts_nrem, label="ALL CELLS")
        plt.scatter(mode_ids, counts_nrem_stable, label="ONLY STABLE")
        plt.xlabel("MODE ID")
        plt.ylabel("COUNTS")
        plt.title("NREM")
        plt.legend()
        plt.show()


        perc_change_nrem = counts_nrem_stable/(counts_nrem/100)-100
        modes_increase_stable_nrem = perc_change_nrem > perc_inc_thresh


        perc_change_rem = counts_rem_stable/(counts_rem/100)-100
        modes_increase_stable_rem = perc_change_rem > perc_inc_thresh

        fig, ax = plt.subplots()
        self.learning_cheeseboard[0].plot_tracking(ax=ax)
        for mode_id in mode_ids[modes_increase_stable_rem]:
            self.learning_cheeseboard[0].plot_phmm_modes(nr_modes=nr_modes, mode_id=mode_id, ax=ax)

        # handles, labels = ax.get_legend_handles_labels()
        # by_label = OrderedDict(zip(labels, handles))
        # ax.legend(by_label.values(), by_label.keys())
        plt.title("MODES WITH INCREASED REACTIVATION\n USING ONLY STABLE CELLS (REM)")
        plt.show()

        fig, ax = plt.subplots()
        self.learning_cheeseboard[0].plot_tracking(ax=ax)
        for mode_id in mode_ids[~modes_increase_stable_rem]:
            self.learning_cheeseboard[0].plot_phmm_modes(nr_modes=nr_modes, mode_id=mode_id, ax=ax)

        # handles, labels = ax.get_legend_handles_labels()
        # by_label = OrderedDict(zip(labels, handles))
        # ax.legend(by_label.values(), by_label.keys())
        plt.title("MODES WITH STABLE/DECREASED REACTIVATION\n USING ONLY STABLE CELLS (REM)")
        plt.show()

        # nrem
        # --------------------------------------------------------------------------------------------------------------

        fig, ax = plt.subplots()
        self.learning_cheeseboard[0].plot_tracking(ax=ax)
        for mode_id in mode_ids[modes_increase_stable_nrem]:
            self.learning_cheeseboard[0].plot_phmm_modes(nr_modes=nr_modes, mode_id=mode_id, ax=ax)

        # handles, labels = ax.get_legend_handles_labels()
        # by_label = OrderedDict(zip(labels, handles))
        # ax.legend(by_label.values(), by_label.keys())
        plt.title("MODES WITH INCREASED REACTIVATION\n USING ONLY STABLE CELLS (NREM)")
        plt.show()

        fig, ax = plt.subplots()
        self.learning_cheeseboard[0].plot_tracking(ax=ax)
        for mode_id in mode_ids[~modes_increase_stable_nrem]:
            self.learning_cheeseboard[0].plot_phmm_modes(nr_modes=nr_modes, mode_id=mode_id, ax=ax)

        # handles, labels = ax.get_legend_handles_labels()
        # by_label = OrderedDict(zip(labels, handles))
        # ax.legend(by_label.values(), by_label.keys())
        plt.title("MODES WITH STABLE/DECREASED REACTIVATION\n USING ONLY STABLE CELLS (NREM)")
        plt.show()


        exit()
        # get spatial information for each mode from PRE
        sparsity_pre, skaggs_pre = self.exploration_fam.phmm_mode_spatial_information_from_model(file_name=pHMM_file_pre,
                                                                                          plot_for_control=plot_for_control,
                                                                                         spatial_resolution=10)

        # get spatial information for each mode from POST
        sparsity_post, skaggs_post = self.exploration_novel.phmm_mode_spatial_information_from_model(file_name=pHMM_file_post,
                                                                                          plot_for_control=plot_for_control,
                                                                                         spatial_resolution=10)


        plt.hist(skaggs_pre, color="b")
        plt.hist(skaggs_post, color="r", alpha=0.5)
        plt.show()

        plt.hist(sparsity_pre, color="b")
        plt.hist(sparsity_post, color="r", alpha=0.5)
        plt.show()

        swr_lengths = [x.shape[0] for x in pre_SWR_prob]

        # z-score probabilites for each mode before proceeding
        pre_SWR_prob_arr = np.vstack(pre_SWR_prob)
        pre_SWR_prob_arr_z = zscore(pre_SWR_prob_arr, axis=0)

        pre_SWR_prob_z = []
        first = 0
        for swr_id in range(len(swr_lengths)):
            pre_SWR_prob_z.append(pre_SWR_prob_arr_z[first:first + swr_lengths[swr_id], :])
            first += swr_lengths[swr_id]

        post_SWR_prob_arr = np.vstack(post_SWR_prob)
        post_SWR_prob_arr_z = zscore(post_SWR_prob_arr, axis=0)

        post_SWR_prob_z = []
        first = 0
        for swr_id in range(len(swr_lengths)):
            post_SWR_prob_z.append(post_SWR_prob_arr_z[first:first + swr_lengths[swr_id], :])
            first += swr_lengths[swr_id]

    # decoding content
    # ------------------------------------------------------------------------------------------------------------------

    def memory_drift_rem_nrem_temporal_dynamics(self, template_type, pre_file_name=None, post_file_name=None,
                                                rem_pop_vec_threshold=100):

        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_rem_event, event_times_rem, event_lengths_rem, event_duration_rem_in_s, pre_prob_rem, post_prob_rem = \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold)
        ratio_rem = np.hstack(ratio_per_rem_event)

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_nrem_event, event_times_nrem, event_lengths_nrem, event_duration_nrem_in_s, pre_prob_nrem, \
        post_prob_nrem = self.memory_drift_long_sleep_get_results(template_type=template_type,
                                                                  pre_file_name=pre_file_name,
                                                                  post_file_name=post_file_name, part_to_analyze="nrem",
                                                                  pop_vec_threshold=2)
        ratio_nrem = np.hstack(ratio_per_nrem_event)

        auto_corr_rem = []
        shift_array = np.arange(-100, 100)
        for mode in range(pre_prob_rem.shape[1]):
            ac, _ = cross_correlate(pre_prob_rem[:, mode], pre_prob_rem[:, mode], shift_array=shift_array)
            auto_corr_rem.append(ac)
        auto_corr_rem = np.vstack(auto_corr_rem)
        mean_auto_corr_rem = np.mean(auto_corr_rem, axis=0)

        auto_corr_nrem = []
        for mode in range(pre_prob_rem.shape[1]):
            ac, _ = cross_correlate(pre_prob_nrem[:, mode], pre_prob_nrem[:, mode], shift_array=shift_array)
            auto_corr_nrem.append(ac)
        auto_corr_nrem = np.vstack(auto_corr_nrem)
        mean_auto_corr_nrem = np.mean(auto_corr_nrem, axis=0)

        # auto_corr_raw = np.correlate(ratio_all, ratio_all, mode="full")
        auto_corr_raw_rem, shift_array = cross_correlate(ratio_rem, ratio_rem, shift_array=np.arange(-100, 100))
        auto_corr_raw_nrem, _ = cross_correlate(ratio_nrem, ratio_nrem, shift_array=np.arange(-100, 100))

        auto_corr_raw_nrem[int(auto_corr_raw_nrem.shape[0] / 2)] = np.nan
        auto_corr_raw_rem[int(auto_corr_raw_rem.shape[0] / 2)] = np.nan
        plt.plot(shift_array, auto_corr_raw_nrem, c="b", label="NREM")
        plt.plot(shift_array, auto_corr_raw_rem, c="r", label="REM")
        plt.title("AUTO CORRELATION OF RAW SCORES")
        plt.xlabel("SHIFT / #POP.VEC.")
        plt.ylabel("PEARSON CORRELATION")
        plt.legend()
        plt.show()

        mean_auto_corr_nrem[int(mean_auto_corr_nrem.shape[0] / 2)] = np.nan
        mean_auto_corr_rem[int(mean_auto_corr_rem.shape[0] / 2)] = np.nan

        plt.plot(shift_array, mean_auto_corr_nrem, c="b", label="NREM")
        plt.plot(shift_array, mean_auto_corr_rem, c="r", label="REM")
        plt.title("MEAN(AUTO CORRELATION OF PRE PROB. PER MODE)")
        plt.xlabel("SHIFT / #POP.VEC.")
        plt.ylabel("PEARSON CORRELATION")
        plt.legend()
        plt.show()

        max_pre_prob_rem = np.max(pre_prob_rem, axis=1)
        auto_corr_max_pre_prob_rem, shift_array = cross_correlate(max_pre_prob_rem, max_pre_prob_rem,
                                                                  shift_array=np.arange(-100, 100))
        max_pre_prob_nrem = np.max(pre_prob_nrem, axis=1)
        auto_corr_max_pre_prob_nrem, shift_array = cross_correlate(max_pre_prob_nrem, max_pre_prob_nrem,
                                                                   shift_array=np.arange(-100, 100))

        auto_corr_max_pre_prob_nrem[int(auto_corr_max_pre_prob_nrem.shape[0] / 2)] = np.nan
        auto_corr_max_pre_prob_rem[int(auto_corr_max_pre_prob_rem.shape[0] / 2)] = np.nan
        plt.plot(shift_array, auto_corr_max_pre_prob_nrem, c="b", label="NREM")
        plt.plot(shift_array, auto_corr_max_pre_prob_rem, c="r", label="REM")
        plt.title("AUTO CORRELATION OF MAX. PRE PROB.")
        plt.xlabel("SHIFT / #POP.VEC.")
        plt.ylabel("PEARSON CORRELATION")
        plt.legend()
        plt.show()

    def memory_drift_analyze_nrem(self, template_type, pre_file_name=None, post_file_name=None,
                                                      rem_pop_vec_threshold=100, log_transform=False):
        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_nrem_event, event_times_nrem, event_lengths_nrem, event_duration_nrem_in_s, pre_prob_nrem, \
        post_prob_nrem = self.memory_drift_long_sleep_get_results(template_type=template_type,
                                                                  pre_file_name=pre_file_name,
                                                                  post_file_name=post_file_name, part_to_analyze="nrem",
                                                                  pop_vec_threshold=2)

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        _, _, _, _, pre_prob_rem, \
        post_prob_rem = self.memory_drift_long_sleep_get_results(template_type=template_type,
                                                                  pre_file_name=pre_file_name,
                                                                  post_file_name=post_file_name, part_to_analyze="rem",
                                                                  pop_vec_threshold=2)

        # get most likely mode
        ml_mode_nrem = np.argmax(pre_prob_nrem, axis=1)
        ml_mode_rem = np.argmax(pre_prob_rem, axis=1)

        with open("nrem_spikes","rb") as fp:
            nrem_spikes = pickle.load(fp)

        nrem_spikes = np.hstack(nrem_spikes)

        with open("rem_spikes","rb") as fp:
            rem_spikes = pickle.load(fp)

        rem_spikes = np.hstack(rem_spikes)

        mode_to_plot_1 = 5
        mode_to_plot_2 = 2
        # select only one mode
        nrem_spikes_sel_1 = nrem_spikes[:, ml_mode_nrem==mode_to_plot_1]
        rem_spikes_sel_1 = rem_spikes[:, ml_mode_rem == mode_to_plot_1]
        nrem_spikes_sel_2= nrem_spikes[:, ml_mode_nrem==mode_to_plot_2]
        rem_spikes_sel_2 = rem_spikes[:, ml_mode_rem == mode_to_plot_2]


        comb = np.hstack((rem_spikes_sel_1, rem_spikes_sel_2,
                          nrem_spikes_sel_1, nrem_spikes_sel_2))

        # active and non active set

        comb[comb>0] = 1

        print(comb.shape)
        DD = pairwise_distances(comb.T, metric="jaccard")


        seed = np.random.RandomState(seed=3)
        mds = MDS(n_components=2, max_iter=3000, eps=1e-9, random_state=seed,
                           dissimilarity="precomputed", n_jobs = 1)
        pos = mds.fit(DD).embedding_
        nmds = MDS(n_components=2, metric=False, max_iter=3000, eps=1e-12,
                            dissimilarity="precomputed", random_state=seed, n_jobs=1,
                                                                                       n_init = 1)
        npos = nmds.fit_transform(DD, init=pos)

        # npos = TSNE(n_components=2, perplexity=5).fit_transform(comb.T)

        sep_1 = rem_spikes_sel_1.shape[1]
        sep_2 = sep_1 + rem_spikes_sel_2.shape[1]
        sep_3 = sep_2 + nrem_spikes_sel_1.shape[1]

        rem_res_1 = npos[:sep_1,:]
        rem_res_2 = npos[sep_1:sep_2, :]
        nrem_res_1 = npos[sep_2:sep_3, :]
        nrem_res_2 = npos[sep_3:, :]

        plt.scatter(rem_res_1[:,0], rem_res_1[:,1], color="lightcoral", marker="*")
        plt.scatter(rem_res_2[:, 0], rem_res_2[:, 1], edgecolors="red", facecolors="none")
        plt.scatter(nrem_res_1[:,0], nrem_res_1[:,1], color="cornflowerblue", marker="*", alpha=0.5)
        plt.scatter(nrem_res_2[:, 0], nrem_res_2[:, 1], edgecolors="blue", facecolors="none")
        plt.show()

        # compute transition matrix
        # trans_mat = transition_matrix(ml_mode)
        print("HERE")

    def memory_drift_rem_nrem_and_awake_visualization(self, template_type, pre_file_name=None, post_file_name=None,
                                                      rem_pop_vec_threshold=100, log_transform=False):
        # get awake results first
        # --------------------------------------------------------------------------------------------------------------
        pre_prob_awake = self.learning_cheeseboard[0].decode_awake_activity()
        pre_prob_awake = np.vstack(pre_prob_awake)

        # get rem data
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_rem_event, event_times_rem, event_lengths_rem, event_duration_rem_in_s, pre_prob_rem, post_prob_rem = \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold)

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_nrem_event, event_times_nrem, event_lengths_nrem, event_duration_nrem_in_s, pre_prob_nrem, \
        post_prob_nrem = self.memory_drift_long_sleep_get_results(template_type=template_type,
                                                                  pre_file_name=pre_file_name,
                                                                  post_file_name=post_file_name, part_to_analyze="nrem",
                                                                  pop_vec_threshold=2)
        if log_transform:
            pre_prob_rem_log = np.log(pre_prob_rem)
            pre_prob_nrem_log = np.log(pre_prob_nrem)
            pre_prob_awake_log = np.log(pre_prob_awake)

        else:
            pre_prob_rem_log = pre_prob_rem
            pre_prob_nrem_log = pre_prob_nrem
            pre_prob_awake_log = pre_prob_awake

        pre_prob_rem_log_mean = np.mean(pre_prob_rem_log, axis=0)
        pre_prob_nrem_log_mean = np.mean(pre_prob_nrem_log, axis=0)
        pre_prob_awake_log_mean = np.mean(pre_prob_awake_log, axis=0)
        pre_prob_rem_mean = np.mean(pre_prob_rem, axis=0)
        pre_prob_nrem_mean = np.mean(pre_prob_nrem, axis=0)
        pre_prob_awake_mean = np.mean(pre_prob_awake, axis=0)

        pre_prob_rem_log_samples = pre_prob_rem_log[0::50, :]
        pre_prob_nrem_log_samples = pre_prob_nrem_log[0::50, :]
        pre_prob_awake_log_samples = pre_prob_awake_log[0::20, :]

        sep_1 = pre_prob_rem_log_samples.shape[0]
        sep_2 = pre_prob_rem_log_samples.shape[0] + pre_prob_nrem_log_samples.shape[0]
        comb = np.vstack((pre_prob_rem_log_samples, pre_prob_nrem_log_samples, pre_prob_awake_log_samples)).T
        sep_3 = comb.shape[1]

        result = multi_dim_scaling(act_mat=comb, param_dic=self.params)
        # result = perform_isomap(act_mat=comb, param_dic=self.params)
        # result = perform_PCA(act_mat=comb, param_dic=self.params)[0]
        # result = perform_TSNE(act_mat=comb, param_dic=self.params)


        # split again into REM/NREM/AWAKE
        result_rem = result[:sep_1,:]
        result_nrem = result[sep_1:sep_2,:]
        result_awake = result[sep_2:,:]

        fig, axs = plt.subplots(2, 2)
        axs[0, 0].scatter(result_rem[:,0], result_rem[:,1], color="r", s=1)
        axs[0, 0].set_title("REM")
        axs[0, 1].scatter(result_nrem[:,0], result_nrem[:,1], color="b", s=1)
        axs[0, 1].set_title("NREM")
        axs[1, 0].scatter(result_awake[:,0], result_awake[:,1], color="y", s=1)
        axs[1, 0].set_title("AWAKE")
        axs[1, 1].scatter(result_rem[:,0], result_rem[:,1], color="r", s=1)
        axs[1, 1].scatter(result_nrem[:, 0], result_nrem[:, 1], color="b", s=1)
        axs[1, 1].scatter(result_awake[:, 0], result_awake[:, 1], color="y", s=1)

        axs[1, 1].set_title("ALL")
        plt.show()
        exit()



        if self.params.dr_method_p2 == 3:
            # create figure instance
            fig = plt.figure()
            ax = fig.add_subplot(111, projection='3d')
            plot_3D_scatter(ax=ax, mds=result, params=self.params, data_sep=np.array([sep_1, sep_2]), data_sep2=sep_2)
        else:
            fig = plt.figure()
            ax = fig.add_subplot(111)
            plot_2D_scatter(ax=ax, mds=result, params=self.params, data_sep=np.array([sep_1, sep_2, sep_3]),
                            labels=["REM",
                                    "NREM",
                                    "AWAKE"])
        handles, labels = ax.get_legend_handles_labels()
        by_label = OrderedDict(zip(labels, handles))
        ax.legend(by_label.values(), by_label.keys())
        plt.show()

        exit()
        comb_min = np.min(np.hstack((pre_prob_nrem_log_mean, pre_prob_rem_log_mean, pre_prob_awake_log_mean)))
        comb_max = np.max(np.hstack((pre_prob_nrem_log_mean, pre_prob_rem_log_mean, pre_prob_awake_log_mean)))
        plt.subplot(1, 3, 1)
        plt.imshow(np.expand_dims(pre_prob_nrem_log_mean, 1), vmin=comb_min, vmax=comb_max, interpolation='nearest',
                   aspect='auto')
        plt.ylabel("MODE ID")
        plt.title("NREM")
        plt.gca().set_xticklabels([])
        plt.subplot(1, 3, 2)
        plt.imshow(np.expand_dims(pre_prob_rem_log_mean, 1), vmin=comb_min, vmax=comb_max, interpolation='nearest',
                   aspect='auto')
        plt.title("REM")
        plt.gca().set_xticklabels([])
        plt.subplot(1, 3, 3)
        plt.imshow(np.expand_dims(pre_prob_awake_log_mean, 1), vmin=comb_min, vmax=comb_max, interpolation='nearest',
                   aspect='auto')
        a = plt.colorbar()
        a.set_label("MEAN LOG-PROB.")
        plt.title("AWAKE")
        plt.gca().set_xticklabels([])
        plt.show()

        comb_min = np.min(np.hstack((pre_prob_nrem_mean, pre_prob_rem_mean, pre_prob_awake_mean)))
        comb_max = np.max(np.hstack((pre_prob_nrem_mean, pre_prob_rem_mean, pre_prob_awake_mean)))
        plt.subplot(1, 3, 1)
        plt.imshow(np.expand_dims(pre_prob_nrem_mean, 1), vmin=comb_min, vmax=comb_max, interpolation='nearest',
                   aspect='auto')
        plt.ylabel("MODE ID")
        plt.title("NREM")
        plt.gca().set_xticklabels([])
        plt.subplot(1, 3, 2)
        plt.imshow(np.expand_dims(pre_prob_rem_mean, 1), vmin=comb_min, vmax=comb_max, interpolation='nearest',
                   aspect='auto')
        plt.title("REM")
        plt.gca().set_xticklabels([])
        plt.subplot(1, 3, 3)
        plt.imshow(np.expand_dims(pre_prob_awake_mean, 1), vmin=comb_min, vmax=comb_max, interpolation='nearest',
                   aspect='auto')
        a = plt.colorbar()
        a.set_label("MEAN PROB.")
        plt.title("AWAKE")
        plt.gca().set_xticklabels([])
        plt.show()

        plt.imshow(pre_prob_rem_log.T, interpolation='nearest', aspect='auto')
        a = plt.colorbar()
        a.set_label("LOG(PROB)")
        plt.xlabel("POP.VEC.ID")
        plt.ylabel("MODE ID")
        plt.title("REM")
        plt.show()

        plt.imshow(pre_prob_nrem_log.T, interpolation='nearest', aspect='auto')
        plt.xlabel("POP.VEC.ID")
        plt.ylabel("MODE ID")
        a = plt.colorbar()
        a.set_label("LOG(PROB)")
        plt.title("NREM")
        plt.show()

        plt.imshow(pre_prob_awake_log.T, interpolation='nearest', aspect='auto')
        plt.xlabel("POP.VEC.ID")
        plt.ylabel("MODE ID")
        a = plt.colorbar()
        a.set_label("LOG(PROB)")
        plt.title("AWAKE")
        plt.show()

    def memory_drift_rem_nrem_and_awake_fluctuations(self, template_type, pre_file_name=None, post_file_name=None,
                                                     rem_pop_vec_threshold=100):
        # get awake results first
        # --------------------------------------------------------------------------------------------------------------
        pre_prob_awake = self.learning_cheeseboard[0].decode_awake_activity()
        pre_prob_awake = np.vstack(pre_prob_awake)
        pre_prob_awake_log = np.log(pre_prob_awake).T

        # get rem data
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_rem_event, event_times_rem, event_lengths_rem, event_duration_rem_in_s, pre_prob_rem, post_prob_rem = \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold)

        pre_prob_rem = np.vstack(pre_prob_rem).T
        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_nrem_event, event_times_nrem, event_lengths_nrem, event_duration_nrem_in_s, pre_prob_nrem, \
        post_prob_nrem = self.memory_drift_long_sleep_get_results(template_type=template_type,
                                                                  pre_file_name=pre_file_name,
                                                                  post_file_name=post_file_name, part_to_analyze="nrem",
                                                                  pop_vec_threshold=2)

        pre_prob_nrem = np.vstack(pre_prob_nrem).T

        pre_prob_rem_log = np.log(pre_prob_rem)
        pre_prob_nrem_log = np.log(pre_prob_nrem)
        pre_prob_awake_log = np.log(pre_prob_awake)

        ab_angle, rel_angle = angle_between_col_vectors(pre_prob_nrem_log)
        plt.plot(rel_angle)
        plt.show()

    def memory_drift_rem_nrem_decoding_similarity(self, template_type, pre_file_name=None, post_file_name=None,
                                                  rem_pop_vec_threshold=100, only_stable_cells=False):

        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_rem_event, event_times_rem, event_lengths_rem, event_duration_rem_in_s, pre_prob_rem, post_prob_rem= \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold,
                                                     only_stable_cells=only_stable_cells)
        ratio_rem = np.hstack(ratio_per_rem_event)

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_nrem_event, event_times_nrem, event_lengths_nrem, event_duration_nrem_in_s, pre_prob_nrem, \
        post_prob_nrem= self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="nrem",
                                                     pop_vec_threshold=2, only_stable_cells=only_stable_cells)
        ratio_nrem = np.hstack(ratio_per_nrem_event)


        pre_prob_rem_log = np.log(pre_prob_rem)
        pre_prob_nrem_log = np.log(pre_prob_nrem)

        pre_prob_rem_log_mean = np.mean(pre_prob_rem_log, axis=0)
        pre_prob_nrem_log_mean = np.mean(pre_prob_nrem_log, axis=0)
        pre_prob_rem_mean = np.mean(pre_prob_rem, axis=0)
        pre_prob_nrem_mean = np.mean(pre_prob_nrem, axis=0)

         # compute similiarty within vs. across
        # --------------------------------------------------------------------------------------------------------------
        dat_rem = pre_prob_rem_log[0::50]
        dat_nrem = pre_prob_nrem_log[0::50]

        # across
        across = []
        sign_rem = []
        sign_nrem = []
        for i_temp, template in enumerate(dat_rem):
            for i_comp, comp in enumerate(dat_nrem):
                across.append(pearsonr(template, comp)[1])
                if pearsonr(template, comp)[1] < 0.01:
                    sign_rem.append(i_temp)
                    sign_nrem.append(i_comp)


        sign_rem, nr_sign_rem = np.unique(np.array(sign_rem), return_counts=True)
        sign_nrem, nr_sign_nrem = np.unique(np.array(sign_nrem), return_counts=True)

        perc_sign_rem = nr_sign_rem/(dat_nrem.shape[0]*0.01)
        perc_sign_nrem = nr_sign_nrem / (dat_rem.shape[0] * 0.01)

        plt.hist(perc_sign_rem, color="r", density=True, bins=20, label="REM")
        plt.hist(perc_sign_nrem, color="b", alpha=0.8, density=True, bins=20, label="NREM")
        plt.ylabel("DENSITY")
        plt.xlabel("% OF SIGN. CORRELATIONS FROM ALL\n POSSIBLE CORRELATIONS ACROSS")
        plt.legend()
        plt.show()

        y,x,_=plt.hist(across, bins=200, density=True)
        plt.vlines(0.05, 0, y.max(), color="y", label="0.05")
        plt.vlines(0.01, 0, y.max(), color="g", label="0.01")
        plt.xlabel("P-VALUE OF CORRELATION")
        plt.ylabel("DENSITY")
        plt.title("CORR. PROB. VECTORS ACROSS REM/NREM")
        plt.legend()
        plt.show()

        # within rem
        within_rem = []
        for temp_ind, template in enumerate(dat_rem):
            if temp_ind+1 < dat_rem.shape[1]:
                for comp in dat_rem[temp_ind+1:]:
                    within_rem.append(pearsonr(template, comp)[0])

        # within nrem
        within_nrem = []
        for temp_ind, template in enumerate(dat_nrem):
            if temp_ind+1 < dat_nrem.shape[1]:
                for comp in dat_nrem[temp_ind+1:]:
                    within_nrem.append(pearsonr(template, comp)[0])


        plt.hist(within_rem, density=True, color="r", label="REM")
        plt.hist(within_nrem, density=True, color="b", label="NREM", alpha=0.8)
        plt.hist(across, density=True, color="w", label="ACROSS", alpha=0.5)
        plt.xlabel("PEARSONR(LOG.PROB.VEC.)")
        plt.ylabel("DENSITY")
        plt.legend()
        # plt.savefig("plots/01_memory_drift/pearson_within_across.png")
        plt.show()

    def memory_drift_rem_nrem_decoding_visualization(self, template_type, pre_file_name=None, post_file_name=None,
                                                  rem_pop_vec_threshold=100, only_stable_cells=False):

        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_rem_event, event_times_rem, event_lengths_rem, event_duration_rem_in_s, pre_prob_rem, post_prob_rem= \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold,
                                                     only_stable_cells=only_stable_cells)


        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_nrem_event, event_times_nrem, event_lengths_nrem, event_duration_nrem_in_s, pre_prob_nrem, \
        post_prob_nrem= self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="nrem",
                                                     pop_vec_threshold=2, only_stable_cells=only_stable_cells)

        pre_prob_rem_log = np.log(pre_prob_rem)
        pre_prob_nrem_log = np.log(pre_prob_nrem)

        pre_prob_rem_log_mean = np.mean(pre_prob_rem_log, axis=0)
        pre_prob_nrem_log_mean = np.mean(pre_prob_nrem_log, axis=0)
        pre_prob_rem_mean = np.mean(pre_prob_rem, axis=0)
        pre_prob_nrem_mean = np.mean(pre_prob_nrem, axis=0)

        pre_prob_rem = pre_prob_rem_log[0::50,:]
        pre_prob_nrem = pre_prob_nrem_log[0::50,:]

        comb_mean = np.vstack((pre_prob_rem, pre_prob_nrem)).T
        sep = np.array([pre_prob_rem.shape[0], comb_mean.shape[1]])

        result = multi_dim_scaling(act_mat=comb_mean, param_dic=self.params)

        if self.params.dr_method_p2 == 3:
            # create figure instance
            fig = plt.figure()
            ax = fig.add_subplot(111, projection='3d')
            plot_3D_scatter(ax=ax, mds=result, params=self.params, data_sep=sep)
        else:
            fig = plt.figure()
            ax = fig.add_subplot(111)
            plot_2D_scatter(ax=ax, mds=result, params=self.params, data_sep=sep, labels=["REM", "NREM"])
        handles, labels = ax.get_legend_handles_labels()
        by_label = OrderedDict(zip(labels, handles))
        ax.legend(by_label.values(), by_label.keys())
        plt.show()

        comb_min = np.min(np.hstack((pre_prob_nrem_log_mean, pre_prob_rem_log_mean)))
        comb_max = np.max(np.hstack((pre_prob_nrem_log_mean, pre_prob_rem_log_mean)))
        plt.subplot(1,2,1)
        plt.imshow(np.expand_dims(pre_prob_nrem_log_mean, 1), vmin=comb_min, vmax=comb_max, interpolation='nearest',
                   aspect='auto')
        a=plt.colorbar()
        a.set_label("MEAN LOG-PROB.")
        plt.ylabel("MODE ID")
        plt.title("NREM")
        plt.gca().set_xticklabels([])
        plt.subplot(1,2,2)
        plt.imshow(np.expand_dims(pre_prob_rem_log_mean, 1), vmin=comb_min, vmax=comb_max, interpolation='nearest',
                   aspect='auto')
        a=plt.colorbar()
        a.set_label("MEAN LOG-PROB.")
        plt.title("REM")
        plt.gca().set_xticklabels([])
        plt.show()

        comb_min = np.min(np.hstack((pre_prob_nrem_mean, pre_prob_rem_mean)))
        comb_max = np.max(np.hstack((pre_prob_nrem_mean, pre_prob_rem_mean)))
        plt.subplot(1,2,1)
        plt.imshow(np.expand_dims(pre_prob_nrem_mean, 1), vmin=comb_min, vmax=comb_max, interpolation='nearest',
                   aspect='auto')
        a=plt.colorbar()
        a.set_label("MEAN PROB.")
        plt.ylabel("MODE ID")
        plt.title("NREM")
        plt.gca().set_xticklabels([])
        plt.subplot(1,2,2)
        plt.imshow(np.expand_dims(pre_prob_rem_mean, 1), vmin=comb_min, vmax=comb_max, interpolation='nearest',
                   aspect='auto')
        a=plt.colorbar()
        a.set_label("MEAN PROB.")
        plt.title("REM")
        plt.gca().set_xticklabels([])
        plt.show()

        plt.imshow(pre_prob_rem_log.T, interpolation='nearest', aspect='auto')
        a = plt.colorbar()
        a.set_label("LOG(PROB)")
        plt.xlabel("POP.VEC.ID")
        plt.ylabel("MODE ID")
        plt.title("REM")
        plt.show()

        plt.imshow(pre_prob_nrem_log.T, interpolation='nearest', aspect='auto')
        plt.xlabel("POP.VEC.ID")
        plt.ylabel("MODE ID")
        a = plt.colorbar()
        a.set_label("LOG(PROB)")
        plt.title("NREM")
        plt.show()

    def memory_drift_rem_nrem_decoding_temporal_visualization(self, template_type, pre_file_name=None,
                                                              post_file_name=None, only_stable_cells=False):

        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        pre_prob_list_rem, post_prob_list_rem, event_times_list_rem= \
            self.memory_drift_long_sleep_get_raw_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                         only_stable_cells=only_stable_cells)

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        pre_prob_list_nrem, post_prob_list_nrem, event_times_list_nrem= self.memory_drift_long_sleep_get_raw_results(
            template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="nrem",
                                                     only_stable_cells=only_stable_cells)


        time_per_rem_vec = []
        # assign temporal label to all vectors
        for prob_l, e_t in zip(pre_prob_list_rem, event_times_list_rem):
            time_per_rem_vec.append(np.linspace(e_t[0],e_t[1], prob_l.shape[0]))

        time_per_rem_vec = np.hstack(time_per_rem_vec)
        pre_prob_rem = np.vstack(pre_prob_list_rem)


        time_per_nrem_vec = []
        # assign temporal label to all vectors
        for prob_l, e_t in zip(pre_prob_list_nrem, event_times_list_nrem):
            time_per_nrem_vec.append(np.linspace(e_t[0],e_t[1], prob_l.shape[0]))

        time_per_nrem_vec = np.hstack(time_per_nrem_vec)
        pre_prob_nrem = np.vstack(pre_prob_list_nrem)

        sampling = 50

        pre_prob_rem_log = np.log(pre_prob_rem)
        pre_prob_nrem_log = np.log(pre_prob_nrem)

        pre_prob_rem_log = pre_prob_rem_log[0::sampling,:]
        time_per_rem_vec = time_per_rem_vec[0::sampling]
        pre_prob_nrem_log = pre_prob_nrem_log[0::sampling,:]
        time_per_nrem_vec = time_per_nrem_vec[0::sampling]

        comb = np.vstack((pre_prob_rem_log, pre_prob_nrem_log)).T
        # sep = np.array([pre_prob_rem_log.shape[0], comb.shape[1]])
        sep = pre_prob_rem_log.shape[0]

        result = multi_dim_scaling(act_mat=comb, param_dic=self.params)

        rem_res = result[:sep,:]
        nrem_res = result[sep:, :]

        fig = plt.figure()
        ax = fig.add_subplot(111, projection='3d')

        ax.scatter(rem_res[:,0], rem_res[:,1], time_per_rem_vec / 60, color="r", alpha=0.5)
        ax.scatter(nrem_res[:, 0], nrem_res[:, 1], time_per_nrem_vec / 60, color="b", alpha=0.5)
        for plot_c in range(rem_res.shape[0]-1):
            ax.plot(rem_res[plot_c:plot_c+2,0], rem_res[plot_c:plot_c+2,1], time_per_rem_vec[plot_c:plot_c+2] / 60,
                    c="lightcoral", alpha=0.5)

        for plot_c in range(nrem_res.shape[0]-1):
            ax.plot(nrem_res[plot_c:plot_c+2,0], nrem_res[plot_c:plot_c+2,1], time_per_nrem_vec[plot_c:plot_c+2] / 60,
                    c="royalblue", alpha=0.8)

        # hide labels
        ax.set_yticklabels([])
        ax.set_xticklabels([])
        ax.set_zlabel("Time / min")
        # set pane alpha value to zero --> transparent
        ax.w_xaxis.set_pane_color((0.8, 0.8, 0.8, 0.0))
        ax.w_yaxis.set_pane_color((0.8, 0.8, 0.8, 0.0))
        ax.w_zaxis.set_pane_color((0.8, 0.8, 0.8, 0.0))
        plt.show()

    def memory_drift_decoding_similarity_temporal(self, template_type, pre_file_name=None, samples_per_epoch = 40,
                                                  compare_with_previous=True, post_file_name=None,
                                                  only_stable_cells=False):

        # with open(self.params.pre_proc_dir+"temp_data/"+"test", 'rb') as f:
        #     per_event_max_corr = pickle.load(f)
        # start = 0
        # for i, event in enumerate(per_event_max_corr):
        #     length_event = event.shape[0]
        #     plt.plot(range(start, start+length_event),event)
        #     start += length_event
        #     plt.xlabel("Pop.Vec.ID (REM)")
        #     plt.ylabel("Max. corr. with previous NREM")
        #     plt.title("REM EPOCH "+str(i))
        #     plt.show()
        # exit()
        #
        #
        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        pre_prob_list_rem, post_prob_list_rem, event_times_list_rem= \
            self.memory_drift_long_sleep_get_raw_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                         pop_vec_threshold=10, only_stable_cells=only_stable_cells)

        # pre_prob_arr_rem = np.vstack(pre_prob_list_rem)
        length_per_event_rem = [x.shape[0] for x in pre_prob_list_rem]
        event_times_rem = np.vstack(event_times_list_rem)

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        pre_prob_list_nrem, post_prob_list_nrem, event_times_list_nrem= self.memory_drift_long_sleep_get_raw_results(
            template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="nrem",
            only_stable_cells=only_stable_cells)

        length_per_event_nrem = [x.shape[0] for x in pre_prob_list_nrem]
        event_times_nrem = np.vstack(event_times_list_nrem)

        # combine rem and nrem data to order it in the right way (chronologically)
        # --------------------------------------------------------------------------------------------------------------

        all_events_pre_prob = pre_prob_list_rem + pre_prob_list_nrem
        all_events_length = length_per_event_rem + length_per_event_nrem
        labels_events = np.zeros(len(pre_prob_list_rem) + len(pre_prob_list_nrem))
        labels_events[:len(pre_prob_list_rem)] = 1
        all_times = np.hstack((event_times_rem[:, 0], event_times_nrem[:, 0]))
        all_end_times = np.hstack((event_times_rem[:, 1], event_times_nrem[:, 2]))

        # sort events according to time
        sorted_events_pre_prob_list = [x for _, x in sorted(zip(all_times, all_events_pre_prob))]
        sorted_labels_events = [x for _, x in sorted(zip(all_times, labels_events))]
        sorted_lengths = [x for _, x in sorted(zip(all_times, all_events_length))]
        sorted_pre_prob = np.vstack(sorted_events_pre_prob_list)
        sorted_times = [x for x in sorted(all_times)]
        sorted_end_times = [x for _, x in sorted(zip(all_times, all_end_times))]

        # compute labels per population vector --> rem events = 1, nrem events = 0
        # --------------------------------------------------------------------------------------------------------------
        labels_per_pop_vec = []
        for len_event, label in zip(sorted_lengths, sorted_labels_events):
            if label:
                labels_per_pop_vec.extend(np.ones(len_event))
            elif label == 0:
                labels_per_pop_vec.extend(np.zeros(len_event))

        labels_per_pop_vec = np.array(labels_per_pop_vec)

        # concatenate events that have the same label
        # --------------------------------------------------------------------------------------------------------------
        sorted_labels_events = np.array(sorted_labels_events)
        label_change = np.diff(sorted_labels_events)
        merged_events_labels = []
        merged_events_times = []
        first = 0
        while True:
            dat = label_change[first:]
            if first > label_change.shape[0]:
                break
            if sorted_labels_events[first] == 0:
                # nrem event
                # check if this is the last event
                if np.where(dat == 1)[0].shape[0] == 0:
                    trans = label_change.shape[0] - first + 1
                else:
                    trans = np.where(dat == 1)[0][0] + 1
                merged_events_labels.append(np.unique(sorted_labels_events[first:first+trans]))
                merged_events_times.append([sorted_times[first], sorted_end_times[first+trans-1]])
            elif sorted_labels_events[first] == 1:
                # rem event
                # check if this is the last event
                if np.where(dat == -1)[0].shape[0] == 0:
                    trans = label_change.shape[0] - first + 1
                else:
                    trans = np.where(dat == -1)[0][0] + 1
                merged_events_labels.append(np.unique(sorted_labels_events[first:first+trans]))
                merged_events_times.append([sorted_times[first], sorted_end_times[first+trans-1]])
            first += trans

        merged_events_labels = np.squeeze(np.array(merged_events_labels))

        # concatenate population vectors that have the same label: TODO: there might be a better way to combine this
        # step and the previous step
        # --------------------------------------------------------------------------------------------------------------
        new_event_time_stamps = np.abs(np.diff(labels_per_pop_vec))

        # need to add one element at the beginning
        new_event_time_stamps = np.insert(new_event_time_stamps, 0, 1)
        start = np.nonzero(new_event_time_stamps == 1)[0]
        end = start[1:]
        end = np.append(end, new_event_time_stamps.shape[0])

        # only select data (pre_post ratio) from population vectors that belong to selected rem/nrem events
        # --> results per merged event
        # --------------------------------------------------------------------------------------------------------------
        pre_prob_per_merged_rem_event = []
        pre_prob_per_merged_nrem_event = []
        pre_prob_rem_nrem_events = []
        rem_nrem_events_label = []
        pre_prob_rem_nrem_pop_vec = []
        rem_nrem_pop_vec_label = []

        for start_event, end_event in zip(start, end):
            # rem event
            if labels_per_pop_vec[start_event + 1] == 1:
                pre_prob_per_merged_rem_event.append(sorted_pre_prob[start_event:end_event])
            # nrem event
            else:
                pre_prob_per_merged_nrem_event.append(sorted_pre_prob[start_event:end_event])

            pre_prob_rem_nrem_events.append(sorted_pre_prob[start_event:end_event])
            rem_nrem_events_label.extend([labels_per_pop_vec[start_event + 1]])

            pre_prob_rem_nrem_pop_vec.extend(sorted_pre_prob[start_event:end_event])
            rem_nrem_pop_vec_label.extend(labels_per_pop_vec[start_event:end_event])

        merged_events_times = np.vstack(merged_events_times)

        merged_rem_event_times = merged_events_times[merged_events_labels == 1]
        merged_nrem_event_times = merged_events_times[merged_events_labels == 0]

        # take samples from each epoch --> otherwise MDS won't work (too much data)
        pre_prob_per_merged_rem_event_samples = []
        for epoch in pre_prob_per_merged_rem_event:
            sample_ind = np.random.randint(0, epoch.shape[0], size=samples_per_epoch)
            pre_prob_per_merged_rem_event_samples.append(epoch[sample_ind, :])

        pre_prob_per_merged_nrem_event_samples = []
        for epoch in pre_prob_per_merged_nrem_event:
            sample_ind = np.random.randint(0, epoch.shape[0], size=samples_per_epoch)
            pre_prob_per_merged_nrem_event_samples.append(epoch[sample_ind, :])

        nr_rem_epochs = len(pre_prob_per_merged_rem_event_samples)
        nr_nrem_epochs = len(pre_prob_per_merged_nrem_event_samples)

        merged_data = np.vstack((np.vstack(pre_prob_per_merged_rem_event_samples),
                                 np.vstack(pre_prob_per_merged_nrem_event_samples)))

        merged_data = np.log(merged_data)
        # apply multidimensional scaling using correlations
        D = pairwise_distances(merged_data, metric="correlation")

        model = MDS(n_components=2, dissimilarity='precomputed', random_state=1)
        results = model.fit_transform(D)

        plt.scatter(results[:(samples_per_epoch*nr_rem_epochs),0], results[:(samples_per_epoch*nr_rem_epochs),1], color="r")
        plt.scatter(results[(samples_per_epoch * nr_rem_epochs):, 0], results[(samples_per_epoch * nr_rem_epochs):, 1], color="b")
        plt.show()

        # split again into REM and NREM
        split_results = np.split(results, nr_rem_epochs+nr_nrem_epochs)
        rem_results = split_results[:nr_rem_epochs]
        nrem_results = split_results[nr_rem_epochs:]

        # go through all epochs and compute spread
        rem_area = []
        rem_centers = []
        for rem_epoch in rem_results:
            # find mean
            m = np.mean(rem_epoch, axis=0)
            # apply pca to find ellipse that spans the data
            centered = rem_epoch - m
            # covariance
            c = centered.transpose() @ centered
            ev = np.linalg.eig(c)
            trans = centered @ ev[1]
            ellipse_wh = np.max(np.abs(trans), axis=0)
            area = np.round(np.pi*ellipse_wh[0]*ellipse_wh[1],3)
            # fig, ax = plt.subplots()
            # from matplotlib.patches import Ellipse
            # ax.scatter(trans[:,0], trans[:,1], color="red", label="LIKELIHOOD VECTORS, MDS")
            # e = Ellipse(xy=[0,0], width=2*ellipse_wh[0], height=2*ellipse_wh[1], facecolor="None", edgecolor="r")
            # ax.add_artist(e)
            # plt.xlim(-1.5, 1.5)
            # plt.ylim(-1.5, 1.5)
            # plt.xlabel("FIRST PC")
            # plt.ylabel("SECOND PC")
            # plt.legend()
            # plt.title("AREA ELLIPSE = "+str(area))
            # plt.show()

            rem_area.append(area)
            rem_centers.append(m)

        # go through all epochs and compute spread
        nrem_area = []
        nrem_centers = []
        for nrem_epoch in nrem_results:
            # find mean
            m = np.mean(nrem_epoch, axis=0)
            # apply pca to find ellipse that spans the data
            centered = nrem_epoch - m
            # covariance
            c = centered.transpose() @ centered
            ev = np.linalg.eig(c)
            trans = centered @ ev[1]
            ellipse_wh = np.max(np.abs(trans), axis=0)
            area = np.round(np.pi*ellipse_wh[0]*ellipse_wh[1],3)
            # fig, ax = plt.subplots()
            # from matplotlib.patches import Ellipse
            # ax.scatter(trans[:,0], trans[:,1], color="blue", label="LIKELIHOOD VECTORS, MDS")
            # e = Ellipse(xy=[0,0], width=2*ellipse_wh[0], height=2*ellipse_wh[1], facecolor="None", edgecolor="b")
            # ax.add_artist(e)
            # plt.xlim(-1.5, 1.5)
            # plt.ylim(-1.5, 1.5)
            # plt.xlabel("FIRST PC")
            # plt.ylabel("SECOND PC")
            # plt.legend()
            # plt.title("AREA ELLIPSE = "+str(area))
            # plt.show()
            nrem_area.append(area)
            nrem_centers.append(m)

        # Create figure and axes
        fig, ax = plt.subplots()
        prev_center = np.array([0,0])
        for i, (area, center, event_time) in enumerate(zip(nrem_area, nrem_centers, merged_nrem_event_times)):
            # center nrem around zero
            dist_from_prev = 0
            ax.hlines(dist_from_prev, event_time[0], event_time[1], colors="b", zorder=1000, label="CENTER NREM")
            rect = patches.Rectangle((event_time[0], dist_from_prev-0.5*area), event_time[1]-event_time[0], area, linewidth=1,
                                     edgecolor='None', facecolor='lightblue', zorder=800, label="SPREAD NREM")
            ax.add_patch(rect)

        nrem_centers = np.array(nrem_centers)
        rem_centers = np.array(rem_centers)
        # compute distance between rem and previous nrem
        if merged_events_labels[0] == 1:
            # first event is a REM event
            rem_centers = rem_centers[1:,:]
            rem_area = rem_area[1:]
            merged_rem_event_times = merged_rem_event_times[1:,:]
            nrem_centers = nrem_centers[:rem_centers.shape[0],:]
        elif merged_events_labels[0] == 0:
            nrem_centers = nrem_centers[:rem_centers.shape[0],:]

        dist = np.linalg.norm(nrem_centers-rem_centers, axis=1)

        for i, (area, d, event_time) in enumerate(zip(rem_area, dist, merged_rem_event_times)):
            # compute correlations within
            ax.hlines(d, event_time[0], event_time[1], colors="r", zorder=1000, label="REM DIST. TO PREV. NREM")
            rect = patches.Rectangle((event_time[0], d-0.5*area), event_time[1]-event_time[0], area, linewidth=1,
                                     edgecolor='None', facecolor='mistyrose', zorder=800, label="SPREAD REM")
            ax.add_patch(rect)

        handles, labels = plt.gca().get_legend_handles_labels()
        by_label = OrderedDict(zip(labels, handles))
        plt.legend(by_label.values(), by_label.keys())
        plt.xlabel("TIME / s")
        plt.ylabel("CENTER & SPREAD OF ELLIPSE")
        plt.show()

        per_event_max_corr = []

        exit()

        if part_to_analyze == "nrem":
            data = pre_prob_per_merged_nrem_event
            template = pre_prob_per_merged_rem_event
            data_identifier = 0
        elif part_to_analyze == "rem":
            data = pre_prob_per_merged_rem_event
            template = pre_prob_per_merged_nrem_event
            data_identifier = 1

        if compare_with_previous:
            if merged_events_labels[0] == data_identifier:
                # first event is NREM --> skip this one and compute for the next
                for i in np.arange(1, len(data)):
                    # get likelihoods from previous
                    prev_likeli = np.vstack(template[i-1])
                    # go through single likelhood vectors of current NREM and compare with previous REM
                    max_corr = []
                    for likeli_vec in data[i]:
                        # compute correlations
                        corr_mat = np.corrcoef(np.vstack((np.expand_dims(likeli_vec,0), prev_likeli)))
                        # want maximum correlation
                        max_corr.append(np.max(corr_mat[1:,0]))
                    per_event_max_corr.append(np.array(max_corr))
            else:
                # first event is REM
                for i in np.arange(0, len(data)):
                    # get likelihoods from previous REM
                    prev_likeli = np.vstack(template[i])
                    # go through single likelhood vectors of current NREM and compare with previous REM
                    max_corr = []
                    for likeli_vec in data[i]:
                        # compute correlations
                        corr_mat = np.corrcoef(np.log(np.vstack((np.expand_dims(likeli_vec, 0), prev_likeli))))
                        # want maximum correlation
                        max_corr.append(np.max(corr_mat[1:, 0]))
                    per_event_max_corr.append(np.array(max_corr))
        else:
            if merged_events_labels[0] == data_identifier:
                # first event is NREM
                for i in np.arange(0, len(data)):
                    # check if there is still a REM that follows
                    if i >= len(template):
                        break
                    # get likelihoods from next REM
                    next_likeli = np.vstack(template[i])
                    # go through single likelhood vectors of current NREM and compare with previous REM
                    max_corr = []
                    for likeli_vec in data[i]:
                        # compute correlations
                        corr_mat = np.corrcoef(np.vstack((np.expand_dims(likeli_vec,0), next_likeli)))
                        # want maximum correlation
                        max_corr.append(np.max(corr_mat[1:,0]))
                    per_event_max_corr.append(np.array(max_corr))
            else:
                # first event is REM
                for i in np.arange(0, len(data)):
                    # check if there is still a REM that follows
                    if (i+1) >= len(template):
                        break
                    # get likelihoods from previous REM
                    next_likeli = np.vstack(template[i+1])
                    # go through single likelhood vectors of current NREM and compare with previous REM
                    max_corr = []
                    for likeli_vec in data[i]:
                        # compute correlations
                        corr_mat = np.corrcoef(np.log(np.vstack((np.expand_dims(likeli_vec, 0), next_likeli))))
                        # want maximum correlation
                        max_corr.append(np.max(corr_mat[1:, 0]))
                    per_event_max_corr.append(np.array(max_corr))

        with open(self.params.pre_proc_dir+"temp_data/"+"test", "wb") as fp:  # Pickling
            pickle.dump(per_event_max_corr, fp)

        exit()


        time_per_rem_vec = []
        # assign temporal label to all vectors
        for prob_l, e_t in zip(pre_prob_list_rem, event_times_list_rem):
            time_per_rem_vec.append(np.linspace(e_t[0],e_t[1], prob_l.shape[0]))

        time_per_rem_vec = np.hstack(time_per_rem_vec)
        pre_prob_rem = np.vstack(pre_prob_list_rem)


        time_per_nrem_vec = []
        # assign temporal label to all vectors
        for prob_l, e_t in zip(pre_prob_list_nrem, event_times_list_nrem):
            time_per_nrem_vec.append(np.linspace(e_t[0],e_t[1], prob_l.shape[0]))

        time_per_nrem_vec = np.hstack(time_per_nrem_vec)
        pre_prob_nrem = np.vstack(pre_prob_list_nrem)

        sampling = 50

        pre_prob_rem_log = np.log(pre_prob_rem)
        pre_prob_nrem_log = np.log(pre_prob_nrem)

        pre_prob_rem_log = pre_prob_rem_log[0::sampling,:]
        time_per_rem_vec = time_per_rem_vec[0::sampling]
        pre_prob_nrem_log = pre_prob_nrem_log[0::sampling,:]
        time_per_nrem_vec = time_per_nrem_vec[0::sampling]

        comb = np.vstack((pre_prob_rem_log, pre_prob_nrem_log)).T
        # sep = np.array([pre_prob_rem_log.shape[0], comb.shape[1]])
        sep = pre_prob_rem_log.shape[0]

        result = multi_dim_scaling(act_mat=comb, param_dic=self.params)

        rem_res = result[:sep,:]
        nrem_res = result[sep:, :]

        fig = plt.figure()
        ax = fig.add_subplot(111, projection='3d')

        ax.scatter(rem_res[:,0], rem_res[:,1], time_per_rem_vec / 60, color="r", alpha=0.5)
        # ax.scatter(nrem_res[:, 0], nrem_res[:, 1], time_per_nrem_vec / 60, color="b", alpha=0.5)
        for plot_c in range(rem_res.shape[0]-1):
            ax.plot(rem_res[plot_c:plot_c+2,0], rem_res[plot_c:plot_c+2,1], time_per_rem_vec[plot_c:plot_c+2] / 60,
                    c="lightcoral", alpha=0.5)

        # for plot_c in range(nrem_res.shape[0]-1):
        #     ax.plot(nrem_res[plot_c:plot_c+2,0], nrem_res[plot_c:plot_c+2,1], time_per_nrem_vec[plot_c:plot_c+2] / 60,
        #             c="royalblue", alpha=0.8)

        # hide labels
        ax.set_yticklabels([])
        ax.set_xticklabels([])
        ax.set_zlabel("Time / min")
        # set pane alpha value to zero --> transparent
        ax.w_xaxis.set_pane_color((0.8, 0.8, 0.8, 0.0))
        ax.w_yaxis.set_pane_color((0.8, 0.8, 0.8, 0.0))
        ax.w_zaxis.set_pane_color((0.8, 0.8, 0.8, 0.0))
        plt.show()

    def memory_drift_rem_nrem_decoding_spatial(self, template_type, pre_file_name=None, post_file_name=None,
                                                  rem_pop_vec_threshold=100):

        # get spatial information of modes
        with open("temp_data/test1", "rb") as f:
            dic = pickle.load(f)

        std = dic["std_modes"]

        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_rem_event, event_times_rem, event_lengths_rem, event_duration_rem_in_s, pre_prob_rem, post_prob_rem= \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold)
        ratio_rem = np.hstack(ratio_per_rem_event)

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_nrem_event, event_times_nrem, event_lengths_nrem, event_duration_nrem_in_s, pre_prob_nrem, \
        post_prob_nrem= self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="nrem",
                                                     pop_vec_threshold=2)
        ratio_nrem = np.hstack(ratio_per_nrem_event)

        pre_prob_rem_log = np.log(pre_prob_rem)
        pre_prob_nrem_log = np.log(pre_prob_nrem)

        pre_prob_rem_log_mean = np.mean(pre_prob_rem_log, axis=0)
        pre_prob_nrem_log_mean = np.mean(pre_prob_nrem_log, axis=0)
        pre_prob_rem_mean = np.mean(pre_prob_rem, axis=0)
        pre_prob_nrem_mean = np.mean(pre_prob_nrem, axis=0)

        plt.scatter(std, pre_prob_nrem_log_mean)
        plt.xlabel("SPATIAL STD. OF MODE")
        plt.ylabel("MEAN(LOG(PROB)) OF MODE")
        plt.title("PROB. VS. SPATIAL CONSTRAINS OF MODE:\n R="+str(np.round(pearsonr(std, pre_prob_nrem_log_mean)[0],3)))
        plt.show()

    def memory_drift_rem_nrem_decoding_linear_separability(self, template_type, pre_file_name=None, post_file_name=None,
                                                  rem_pop_vec_threshold=100, log_transform=False):

        # get rem data first
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_rem_event, event_times_rem, event_lengths_rem, event_duration_rem_in_s, pre_prob_rem, post_prob_rem= \
            self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="rem",
                                                     pop_vec_threshold=rem_pop_vec_threshold)
        ratio_rem = np.hstack(ratio_per_rem_event)

        # get nrem data
        # --------------------------------------------------------------------------------------------------------------
        ratio_per_nrem_event, event_times_nrem, event_lengths_nrem, event_duration_nrem_in_s, pre_prob_nrem, \
        post_prob_nrem= self.memory_drift_long_sleep_get_results(template_type=template_type, pre_file_name=pre_file_name,
                                                     post_file_name=post_file_name, part_to_analyze="nrem",
                                                     pop_vec_threshold=2)
        ratio_nrem = np.hstack(ratio_per_nrem_event)

        # dat_1 = np.log(pre_prob_rem[0::50, :])
        # dat_2 = np.log(pre_prob_nrem[0::50, :])

        nr_samples = 2000

        ind_rem = np.random.choice(pre_prob_rem.shape[0], nr_samples, replace=False)
        ind_nrem = np.random.choice(pre_prob_nrem.shape[0], nr_samples, replace=False)

        dat_1 = pre_prob_rem[ind_rem, :]
        dat_2 = pre_prob_nrem[ind_nrem, :]

        if log_transform:
            dat_1 = np.log(dat_1)
            dat_2 = np.log(dat_2)

        # dat_1 = pre_prob_rem[0::20, :]
        # dat_2 = pre_prob_nrem[0::20, :]

        # linear separability of probability vectors
        data = np.vstack((dat_1, dat_2)).T
        labels = np.zeros(dat_1.shape[0]+dat_2.shape[0])
        # rem --> 1
        labels[:dat_1.shape[0]+1] = 1

        acc_rem_list = []
        acc_nrem_list = []
        acc_overal_list = []
        # try linear separability for different test/training set
        for iter in range(10):
            acc_rem, acc_nrem, acc = MlMethodsOnePopulation(params=self.params).linear_separability(
                input_data=data, input_labels=labels)
            acc_rem_list.append(acc_rem)
            acc_nrem_list.append(acc_nrem)
            acc_overal_list.append(acc)

        c = "white"

        acc_nrem = np.array(acc_nrem_list)
        acc_rem = np.array(acc_rem_list)
        acc_overal = np.array(acc_overal_list)
        res = np.vstack((acc_nrem, acc_rem, acc_overal)).T

        bplot=plt.boxplot(res, positions=[1, 2, 3], patch_artist=True,
                    labels=["NREM", "REM", "ALL"],
                    boxprops=dict(color=c),
                    capprops=dict(color=c),
                    whiskerprops=dict(color=c),
                    flierprops=dict(color=c, markeredgecolor=c),
                    medianprops=dict(color=c),
                    )
        colors = ['blue', 'red', "grey"]
        for patch, color in zip(bplot['boxes'], colors):
            patch.set_facecolor(color)
        if log_transform:
            plt.title("LINEAR SEPARABILITY OF LOG-LIKELIHOOD VECTORS")
        else:
            plt.title("LINEAR SEPARABILITY OF LIKELIHOOD VECTORS")
        plt.ylabel("ACCURACY (10 SPLITS)")
        plt.grid(color="grey", axis="y")
        plt.show()


"""#####################################################################################################################
#   CLASS FOR MULTIPLE PHASES AND MULTIPLE POPULATIONS
#####################################################################################################################"""

# TODO: make subclasses/ delete this one class below afterwards

class MultPhasesTwoPopulations:
    """Base class for sleep analysis"""

    def __init__(self, data_obj, params):
        self.params = params

        # initialize each phase
        for phase_id, phase in enumerate(data_obj.data_description):
            if phase == "SLEEP_FAMILIAR":
                self.sleep_fam = TwoPopSleep(data_obj.get_standard_data()[phase_id], data_obj.get_cell_type(), params)
            elif phase == "SLEEP_NOVEL":
                self.sleep_novel = TwoPopSleep(data_obj.get_standard_data()[phase_id], data_obj.get_cell_type(), params)
            elif phase == "EXPLORATION_FAMILIAR":
                self.exploration_fam = TwoPopExploration(data_obj.get_standard_data()[phase_id], data_obj.get_cell_type(), params)
            elif phase == "EXPLORATION_NOVEL":
                self.exploration_novel = TwoPopExploration(data_obj.get_standard_data()[phase_id], data_obj.get_cell_type(), params)

    def local_cca_place_fields(self, video_file_name="CCA_place_field.mp4", sel_range=range(200), co_firing_window_size=10):
        rate_maps = self.exploration_fam.get_rate_maps()
        x_maps = rate_maps[0]
        y_maps = rate_maps[1]

        x_maps = np.array(x_maps)
        y_maps = np.array(y_maps)
        # normalize maps
        x_empty_maps = []
        for i in range(x_maps.shape[0]):
            if np.count_nonzero(x_maps[i]):
                x_maps[i] /= np.max(x_maps[i])
            else:
                x_empty_maps.append(i)

        y_empty_maps = []
        for i in range(y_maps.shape[0]):
            # remove cells that do not fire during exploration
            if np.count_nonzero(y_maps[i]):
                y_maps[i] /= np.max(y_maps[i])
            else:
                y_empty_maps.append(i)

        x_maps = np.delete(x_maps, x_empty_maps, axis=0)
        y_maps = np.delete(y_maps, y_empty_maps, axis=0)

        empty_maps = [x_empty_maps, y_empty_maps]

        cell_type_array, map_list = self.sleep_fam.get_transformed_rasters()

        filtered_map_list = []
        for i, map in enumerate(map_list):
            filtered_map_list.append(np.delete(map, empty_maps[i], axis=0))

        new_ml_an = MlMethodsTwoPopulations(filtered_map_list, self.params, cell_type_array)
        x_loadings, y_loadings = new_ml_an.perform_local_cca(sel_range=sel_range, sliding_window=True, co_firing_window_size= \
            co_firing_window_size, plotting=False)

        fig = plt.figure(figsize=[10, 8])
        gs = fig.add_gridspec(20, 20)

        ax1 = fig.add_subplot(gs[:15, :9])
        ax2 = fig.add_subplot(gs[16:, :9])
        ax3 = fig.add_subplot(gs[:15, 11:])
        ax4 = fig.add_subplot(gs[16:, 11:])
        ims = []
        for time_step, (x_load, y_load) in enumerate(zip(x_loadings, y_loadings)):
            x_map_copy = x_maps.copy()
            y_map_copy = y_maps.copy()

            for i in range(x_map_copy.shape[0]):
                x_map_copy[i, :, :] *= x_load[i]/np.sum(x_load)
            for i in range(y_map_copy.shape[0]):
                y_map_copy[i, :, :] *= y_load[i]/np.sum(y_load)

            x_map_copy = np.sum(x_map_copy, axis=0)
            y_map_copy = np.sum(y_map_copy, axis=0)
            # normalize
            x_map_copy /= np.max(np.abs(x_map_copy))
            y_map_copy /= np.max(np.abs(y_map_copy))
            im1 = ax1.imshow(x_map_copy, vmin=-1, vmax=1)
            im2 = ax2.imshow(x_loadings.T, interpolation='nearest', aspect='auto',
                       extent=[0, self.params.time_bin_size * x_loadings.shape[0],
                               x_loadings.shape[1], 0])
            im3 = ax2.scatter(time_step * self.params.time_bin_size, 0.8, marker=".", color="r")
            im4 = ax3.imshow(y_map_copy, vmin=-1, vmax=1)
            im5 = ax4.imshow(y_loadings.T, interpolation='nearest', aspect='auto',
                       extent=[0, self.params.time_bin_size * y_loadings.shape[0],
                               y_loadings.shape[1], 0])
            im6 = ax4.scatter(time_step * self.params.time_bin_size, 0.8, marker=".", color="r")

            ims.append([im1, im2, im3, im4, im5, im6])
        import matplotlib.animation as animation
        # a = plt.colorbar()
        # a.set_label("PEARSON R")
        ax2.set_xlabel("START OF WINDOW (s)")
        ax2.set_ylim((x_loadings.shape[1], 0))
        ax2.set_xlim((0, self.params.time_bin_size * x_loadings.shape[0]))
        ax2.set_title("CCA LOADINGS CV1: " + str(self.params.time_bin_size) + "s TIME BIN, " + str(co_firing_window_size) + "s WINDOW")
        ax2.set_ylabel("CELL ID")
        ax4.set_title("CCA LOADINGS CV1: " + str(self.params.time_bin_size) + "s TIME BIN, " + str(co_firing_window_size) + "s WINDOW")
        ax4.set_ylabel("CELL ID")
        ax1.set_title("WEIGHTED RATE MAPS - POP1")
        ax1.set_xlabel("X")
        ax1.set_ylabel("Y")
        ax3.set_title("WEIGHTED RATE MAPS - POP2")
        ax3.set_xlabel("X")
        ax3.set_ylabel("Y")
        ax4.set_ylim((y_loadings.shape[1], 0))
        ax4.set_xlim((0, self.params.time_bin_size * y_loadings.shape[0]))

        ani = animation.ArtistAnimation(fig, ims, interval=100, blit=False,
                                        repeat_delay=1000)
        ani.save("movies/" + video_file_name)
        # plt.show()
        # print(x_loadings.shape)

        # plt.imshow(x_loadings.T, interpolation='nearest', aspect='auto',
        #            extent=[0, self.params.time_bin_size * x_loadings.shape[0], x_loadings.shape[1], 0])
        # plt.show()

    def global_cca_place_fields(self, sel_range=range(200), co_firing_window_size=10):
        rate_maps = self.exploration_fam.get_rate_maps()
        x_maps = rate_maps[0]
        y_maps = rate_maps[1]

        x_maps = np.array(x_maps)
        y_maps = np.array(y_maps)
        # normalize maps
        x_empty_maps = []
        for i in range(x_maps.shape[0]):
            if np.count_nonzero(x_maps[i]):
                x_maps[i] /= np.max(x_maps[i])
            else:
                x_empty_maps.append(i)

        y_empty_maps = []
        for i in range(y_maps.shape[0]):
            # remove cells that do not fire during exploration
            if np.count_nonzero(y_maps[i]):
                y_maps[i] /= np.max(y_maps[i])
            else:
                y_empty_maps.append(i)

        x_maps = np.delete(x_maps, x_empty_maps, axis=0)
        y_maps = np.delete(y_maps, y_empty_maps, axis=0)

        empty_maps = [x_empty_maps, y_empty_maps]

        cell_type_array, map_list = self.sleep_fam.get_transformed_rasters()

        filtered_map_list = []
        for i, map in enumerate(map_list):
            filtered_map_list.append(np.delete(map, empty_maps[i], axis=0))

        new_ml_an = MlMethodsTwoPopulations(filtered_map_list, self.params, cell_type_array)
        x_loadings, y_loadings = new_ml_an.perform_global_cca(sel_range=sel_range, sliding_window=True,
                                                              co_firing_window_size=co_firing_window_size)
        fig = plt.figure(figsize=[10, 8])
        gs = fig.add_gridspec(20, 20)

        ax1 = fig.add_subplot(gs[:15, :9])
        ax2 = fig.add_subplot(gs[16:, :9])
        ax3 = fig.add_subplot(gs[:15, 11:])
        ax4 = fig.add_subplot(gs[16:, 11:])
        ims = []

        x_map_copy = x_maps.copy()
        y_map_copy = y_maps.copy()

        for i in range(x_map_copy.shape[0]):
            x_map_copy[i, :, :] *= x_loadings[i]/np.sum(x_loadings)
        for i in range(y_map_copy.shape[0]):
            y_map_copy[i, :, :] *= y_loadings[i]/np.sum(y_loadings)

        x_map_copy = np.sum(x_map_copy, axis=0)
        y_map_copy = np.sum(y_map_copy, axis=0)
        # normalize
        x_map_copy /= np.max(np.abs(x_map_copy))
        y_map_copy /= np.max(np.abs(y_map_copy))
        im1 = ax1.imshow(x_map_copy, vmin=-1, vmax=1)
        im2 = ax2.scatter(range(x_loadings.shape[0]), x_loadings)
        im4 = ax3.imshow(y_map_copy, vmin=-1, vmax=1)
        im5 = ax4.scatter(range(y_loadings.shape[0]), y_loadings)

        ims.append([im1, im2, im4, im5])
        import matplotlib.animation as animation
        # a = plt.colorbar()
        # a.set_label("PEARSON R")
        ax2.set_xlabel("CELL ID")
        ax4.set_xlabel("CELL ID")

        ax2.set_title("CCA LOADINGS CV1: " + str(self.params.time_bin_size) + "s TIME BIN, " + str(co_firing_window_size) + "s WINDOW")
        ax2.set_ylabel("LOADING")
        ax4.set_title("CCA LOADINGS CV1: " + str(self.params.time_bin_size) + "s TIME BIN, " + str(co_firing_window_size) + "s WINDOW")
        ax4.set_ylabel("LOADING")
        ax1.set_title("WEIGHTED RATE MAPS - POP1")
        ax1.set_xlabel("X")
        ax1.set_ylabel("Y")
        ax3.set_title("WEIGHTED RATE MAPS - POP2")
        ax3.set_xlabel("X")
        ax3.set_ylabel("Y")

        plt.show()

    def max_correlated_cells_spatial_overlap(self, sel_range=None):
        # --------------------------------------------------------------------------------------------------------------
        # identified cells that are maximally correlated and weigh the place fields accordingly
        # --> e.g. do cells that are maximally correlated represent same spatial bins?
        #
        # args: - nr_clusters, int: #clusters to fit
        #
        # --------------------------------------------------------------------------------------------------------------

        co_fir_mat = self.sleep_fam.load_or_create_dynamic_co_firing(co_firing_window_size=1,sel_range=sel_range)
        nr_cells_pop_1 = self.sleep_fam.nr_cells_pop_1
        rate_maps_pop_1 = np.array(self.exploration_fam.get_rate_maps()[0])
        rate_maps_pop_2 = np.array(self.exploration_fam.get_rate_maps()[1])

        # TODO: only use across correlation values!!!

        mask_across = np.zeros((co_fir_mat[0].shape[0],co_fir_mat[0].shape[1]))
        mask_across[:(nr_cells_pop_1), (nr_cells_pop_1):] = 1

        map_sim_per_corr = []
        for mat in co_fir_mat:
            across_corr_val = mat * mask_across
            # find correlation values > threshold
            strong_corr_ind = np.argwhere(across_corr_val > 0.8)
            cells_pop_1 = np.unique(strong_corr_ind[:, 0])
            cells_pop_2 = np.unique(strong_corr_ind[:, 1])
            # need to offset by length of cell pop 1
            cells_pop_2 -= nr_cells_pop_1

            map_pop_1_avg = np.mean(rate_maps_pop_1[cells_pop_1,:,:], axis=0)
            # plt.imshow(map_pop_1_avg)
            # plt.show()

            map_pop_2_avg = np.mean(rate_maps_pop_2[cells_pop_2,:,:], axis=0)
            # plt.imshow(map_pop_2_avg)
            # plt.show()

            map_sim = pearsonr(map_pop_1_avg.flatten(), map_pop_2_avg.flatten())[0]
            map_sim_per_corr.append(map_sim)

        plt.plot(map_sim_per_corr)
        plt.xlabel("START POINT SLIDING WINDOW")
        plt.ylabel("CORRELATION AVG. MAP POP1 - AVG. MAP POP2")
        plt.show()

    def phmm_reactivations(self, file_name_pop_1, file_name_pop_2):
        pop_1 = self.exploration_fam.exploration_objects[0]
        pop_2 = self.exploration_fam.exploration_objects[1]

        means_l, std_modes_l, mode_freq_l, env_l, trans_mat_l, state_sequence_l, mode_lambda_l = \
            pop_1.load_fit_poisson_hmm(file_name=file_name_pop_1)
        means_r, std_modes_r, mode_freq_r, env_r, trans_mat_r, state_sequence_r, mode_lambda_r = \
            pop_2.load_fit_poisson_hmm(file_name=file_name_pop_2)

        nr_modes_l = means_l.shape[1]
        nr_modes_r = means_r.shape[1]

        D = np.zeros((nr_modes_l, nr_modes_r))

        # simultanuous activation of modes
        for i in np.arange(nr_modes_l):
            first = np.zeros(state_sequence_l.shape[0])
            first[state_sequence_l == i] = 1
            for j in np.arange(nr_modes_r):
                second = np.zeros(state_sequence_r.shape[0])
                second[state_sequence_r == j] = 1

                l_or = first + second
                nr_active = np.count_nonzero(l_or)

                l_and = first * second
                nr_active_together = np.count_nonzero(l_and)
                dat = nr_active_together / nr_active

                # shuffle data
                null_dat = []
                for shuffling in range(500):
                    np.random.shuffle(first)
                    np.random.shuffle(second)

                    l_or = first + second
                    nr_active = np.count_nonzero(l_or)

                    l_and = first * second
                    nr_active_together = np.count_nonzero(l_and)
                    null_dat.append(nr_active_together / nr_active)

                null_dat = np.array(null_dat)
                mean_shuffle = np.mean(null_dat)
                std_shuffle = np.std(null_dat)

                D[i,j] = (dat - mean_shuffle) / std_shuffle
                # D[i, j] = dat

        D = np.nan_to_num(D, posinf=100, neginf=-100)

        plt.title("CO-OCCURENCE OF MODES IN BOTH POPULATIONS")
        plt.imshow(D, interpolation='nearest', aspect='auto', cmap="jet", vmin=0, vmax=10)
        plt.ylabel("MODE ID LEFT")
        plt.xlabel("MODE ID RIGHT")
        a = plt.colorbar()
        a.set_label("CO-OCCURENCE (TOGETHER/TOTAL) - Z-SCORED")
        plt.show()

        plt.title("HISTOGRAM OF CO-OCCURENCE VALUES")
        plt.xlabel("CO-OCCURENCE (TOGETHER/TOTAL) - Z-SCORED")
        plt.ylabel("COUNTS")
        plt.hist(D.flatten(), bins=100, log=True)
        plt.show()

        # compute distance between means --> correlate with transition matrix

        dist_mat = np.zeros((nr_modes_l, nr_modes_r))

        for i, mean_1 in enumerate(means_l.T):
            for j, mean_2 in enumerate(means_r.T):
                dist_mat[i, j] = np.linalg.norm(mean_1 - mean_2)

        print(dist_mat.shape, D.shape)

        dist_flat = dist_mat.flatten()
        co_active_flat = D.flatten()

        plt.scatter(dist_flat, co_active_flat)
        plt.xlabel("DISTANCE BETWEEN MEANS / cm")
        plt.ylabel("CO-OCCURENCE (TOGETHER/TOTAL) - Z-SCORED")
        plt.title("DISTANCE & CO-OCCURENCE BETWEEN MODES")
        plt.show()

    def memory_drift_correlation_structure(self, correlation_window_size=2):
        corr_exp_fam = self.exploration_fam.correlation_matrix()
        corr_exp_nov = self.exploration_novel.correlation_matrix()
        nr_cells_x = self.exploration_fam.nr_cells_pop_1

        # # remove diagonal
        # corr_exp_fam = corr_exp_fam[~np.eye(corr_exp_fam.shape[0], dtype=bool)].reshape(corr_exp_fam.shape[0], -1)
        # corr_exp_nov = corr_exp_nov[~np.eye(corr_exp_nov.shape[0], dtype=bool)].reshape(corr_exp_nov.shape[0], -1)
        #
        # plt.imshow(corr_exp_fam, interpolation='nearest', aspect='auto')
        # plt.title("CORRELATIONS: EXPLORATION FAMILIAR")
        # plt.xlabel("CELL ID")
        # plt.ylabel("CELL ID")
        # a = plt.colorbar()
        # a.set_label("PEARSON CORR.")
        # plt.show()
        # plt.imshow(corr_exp_nov, interpolation='nearest', aspect='auto')
        # plt.title("CORRELATIONS: EXPLORATION NOVEL")
        # plt.xlabel("CELL ID")
        # plt.ylabel("CELL ID")
        # a = plt.colorbar()
        # a.set_label("PEARSON CORR.")
        # plt.show()

        time_bins_per_co_firing_matrix = int(correlation_window_size / self.params.time_bin_size)
        # corr_sleep_fam = self.sleep_fam.dynamic_co_firing(time_bins_per_co_firing_matrix=time_bins_per_co_firing_matrix,
        #                                                   sel_range=range(10000))

        sim_pop_1 = []
        sim_pop_2 = []
        sim_across = []
        # get correlation matrices for sliding window one after the other to not exhaust memory

        for corr in self.sleep_fam.dynamic_co_firing_generator(time_bins_per_co_firing_matrix=
                                                               time_bins_per_co_firing_matrix):

            sim_after = pearsonr(upper_tri_without_diag(corr[:nr_cells_x + 1, :nr_cells_x + 1]),
                                 upper_tri_without_diag(corr_exp_nov[:nr_cells_x + 1, :nr_cells_x + 1]))[0]
            sim_before = pearsonr(upper_tri_without_diag(corr[:nr_cells_x + 1, :nr_cells_x + 1]),
                                  upper_tri_without_diag(corr_exp_fam[:nr_cells_x + 1, :nr_cells_x + 1]))[0]
            sim_pop_1.append((sim_after - sim_before) / (sim_after + sim_before))

            sim_after = pearsonr(upper_tri_without_diag(corr[(nr_cells_x + 1):, (nr_cells_x + 1):]),
                                 upper_tri_without_diag(corr_exp_nov[(nr_cells_x + 1):, (nr_cells_x + 1):]))[0]
            sim_before = pearsonr(upper_tri_without_diag(corr[(nr_cells_x + 1):, (nr_cells_x + 1):]),
                                  upper_tri_without_diag(corr_exp_fam[(nr_cells_x + 1):, (nr_cells_x + 1):]))[0]
            sim_pop_2.append((sim_after - sim_before) / (sim_after + sim_before))

            sim_after = pearsonr(corr[:(nr_cells_x + 1), (nr_cells_x + 1):].flatten(),
                                 corr_exp_nov[:(nr_cells_x + 1), (nr_cells_x + 1):].flatten())[0]
            sim_before = pearsonr(corr[:(nr_cells_x + 1), (nr_cells_x + 1):].flatten(),
                                  corr_exp_fam[:(nr_cells_x + 1), (nr_cells_x + 1):].flatten())[0]
            sim_across.append((sim_after - sim_before) / (sim_after + sim_before))

            # across_dist[i] = np.linalg.norm(m[:(nr_cells_x+1), (nr_cells_x+1):].flatten())

        x = np.hstack((np.expand_dims(np.array(sim_pop_1), 1), np.expand_dims(np.array(sim_pop_2), 1)))

        stattools.grangercausalitytests(x, maxlag=10, verbose=True)

        fig = plt.figure()
        ax = fig.add_subplot()
        ax.plot(sim_pop_1, color="blue", label="WITHIN POP 1")
        ax.plot(sim_pop_2, color="red", label="WITHIN POP 2")
        ax.plot(sim_across, color="gray", label="ACROSS", alpha=0.5)
        plt.grid()
        plt.title(
            "CORRELATION STRUCTURE SIMILARITY: BEFORE - AFTER\n WINDOW SIZE: " + str(correlation_window_size) + "s")
        plt.xlabel("SLIDING WINDOW ID")
        plt.ylabel("SIMILARITY BEFORE - AFTER")
        plt.legend()
        # plt.ylim(-0.3, 0.3)
        plt.show()
